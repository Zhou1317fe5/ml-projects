{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1 模型过拟合与欠拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 基础代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入工具包，用于模型验证和数据处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # 可视化 用于创建各种类型的统计图形\n",
    "\n",
    "from scipy import stats # 用于统计分析\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.linear_model import LinearRegression  #线性回归\n",
    "from sklearn.neighbors import KNeighborsRegressor  #K近邻回归\n",
    "from sklearn.tree import DecisionTreeRegressor     #决策树回归\n",
    "from sklearn.ensemble import RandomForestRegressor #随机森林回归\n",
    "from sklearn.svm import SVR  #支持向量回归\n",
    "import lightgbm as lgb #lightGbm模型\n",
    "\n",
    "from sklearn.model_selection import train_test_split # 切分数据\n",
    "from sklearn.metrics import mean_squared_error #评价指标\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor # 随机梯度下降线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = \"./data/zhengqi_train.txt\"\n",
    "test_data_file =  \"./data/zhengqi_test.txt\"\n",
    "\n",
    "train_data = pd.read_csv(train_data_file, sep='\\t', encoding='utf-8')\n",
    "test_data = pd.read_csv(test_data_file, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V0</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "      <th>V31</th>\n",
       "      <th>V32</th>\n",
       "      <th>V33</th>\n",
       "      <th>V34</th>\n",
       "      <th>V35</th>\n",
       "      <th>V36</th>\n",
       "      <th>V37</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.566</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.452</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>-1.812</td>\n",
       "      <td>-2.360</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-2.114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-4.627</td>\n",
       "      <td>-4.789</td>\n",
       "      <td>-5.101</td>\n",
       "      <td>-2.608</td>\n",
       "      <td>-3.508</td>\n",
       "      <td>0.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.968</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-1.566</td>\n",
       "      <td>-2.360</td>\n",
       "      <td>0.332</td>\n",
       "      <td>-2.114</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.600</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>0.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.013</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.797</td>\n",
       "      <td>-1.367</td>\n",
       "      <td>-2.360</td>\n",
       "      <td>0.396</td>\n",
       "      <td>-2.114</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.277</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.765</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>0.633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.733</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.599</td>\n",
       "      <td>-0.679</td>\n",
       "      <td>-1.200</td>\n",
       "      <td>-2.086</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-2.114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.603</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>0.206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.684</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>-1.073</td>\n",
       "      <td>-2.086</td>\n",
       "      <td>0.314</td>\n",
       "      <td>-2.114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183</td>\n",
       "      <td>1.078</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.418</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-0.280</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      V0     V1     V2     V3     V4     V5     V6     V7     V8     V9  ...  \\\n",
       "0  0.566  0.016 -0.143  0.407  0.452 -0.901 -1.812 -2.360 -0.436 -2.114  ...   \n",
       "1  0.968  0.437  0.066  0.566  0.194 -0.893 -1.566 -2.360  0.332 -2.114  ...   \n",
       "2  1.013  0.568  0.235  0.370  0.112 -0.797 -1.367 -2.360  0.396 -2.114  ...   \n",
       "3  0.733  0.368  0.283  0.165  0.599 -0.679 -1.200 -2.086  0.403 -2.114  ...   \n",
       "4  0.684  0.638  0.260  0.209  0.337 -0.454 -1.073 -2.086  0.314 -2.114  ...   \n",
       "\n",
       "     V29    V30    V31    V32    V33    V34    V35    V36    V37  target  \n",
       "0  0.136  0.109 -0.615  0.327 -4.627 -4.789 -5.101 -2.608 -3.508   0.175  \n",
       "1 -0.128  0.124  0.032  0.600 -0.843  0.160  0.364 -0.335 -0.730   0.676  \n",
       "2 -0.009  0.361  0.277 -0.116 -0.843  0.160  0.364  0.765 -0.589   0.633  \n",
       "3  0.015  0.417  0.279  0.603 -0.843 -0.065  0.364  0.333 -0.112   0.206  \n",
       "4  0.183  1.078  0.328  0.418 -0.843 -0.215  0.364 -0.280 -0.028   0.384  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "# 1读取特征名\n",
    "features_columns = [col for col in train_data.columns if col not in ['target']]\n",
    "# 2实例化\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# 3训练集fit拟合\n",
    "min_max_scaler = min_max_scaler.fit(train_data[features_columns])\n",
    "# 4transform\n",
    "train_data_scaler = min_max_scaler.transform(train_data[features_columns])\n",
    "test_data_scaler = min_max_scaler.transform(test_data[features_columns])\n",
    "# 5转dataframe-设特征列名\n",
    "train_data_scaler = pd.DataFrame(train_data_scaler)\n",
    "train_data_scaler.columns = features_columns\n",
    "\n",
    "test_data_scaler = pd.DataFrame(test_data_scaler)\n",
    "test_data_scaler.columns = features_columns\n",
    "# 6添回target列\n",
    "train_data_scaler['target'] = train_data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA处理，特征降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA   #主成分分析法\n",
    "\n",
    "#PCA方法降维\n",
    "#保留16个主成分\n",
    "pca = PCA(n_components=16)\n",
    "new_train_pca_16 = pca.fit_transform(train_data_scaler.iloc[:,0:-1])\n",
    "new_test_pca_16 = pca.transform(test_data_scaler)\n",
    "new_train_pca_16 = pd.DataFrame(new_train_pca_16)\n",
    "new_test_pca_16 = pd.DataFrame(new_test_pca_16)\n",
    "new_train_pca_16['target'] = train_data_scaler['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#采用 pca 保留16维特征的数据\n",
    "new_train_pca_16 = new_train_pca_16.fillna(0)\n",
    "train = new_train_pca_16[new_test_pca_16.columns]\n",
    "target = new_train_pca_16['target']\n",
    "\n",
    "# 切分数据 训练数据80% 验证数据20%\n",
    "train_data,test_data,train_target,test_target=train_test_split(train,target,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 欠拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor train MSE:    0.1512027000204227\n",
      "SGDRegressor test MSE:    0.15563803697708092\n"
     ]
    }
   ],
   "source": [
    "clf = SGDRegressor(max_iter=500, tol=1e-2) \n",
    "clf.fit(train_data, train_target)\n",
    "score_train = mean_squared_error(train_target, clf.predict(train_data))\n",
    "score_test = mean_squared_error(test_target, clf.predict(test_data))\n",
    "print(\"SGDRegressor train MSE:   \", score_train)\n",
    "print(\"SGDRegressor test MSE:   \", score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码解释\n",
    "SGDRegressor 是一种基于梯度下降的线性回归模型，使用随机梯度下降算法进行参数估计适用于大规模数据集和高维特征。\n",
    "与传统的批量梯度下降不同，随机梯度下降每次迭代只使用一个样本或一小批样本来更新模型参数，从而减少了内存消耗和计算复杂度。\n",
    "\n",
    "主要参数：\n",
    "- loss: 损失函数的类型。可选参数有 'squared_loss'（平方损失，默认）、'huber'（Huber 损失）、'epsilon_insensitive'（ϵ-insensitive 损失）等。\n",
    "- penalty: 正则化项的类型。可选参数有 'l2'（L2 正则化，默认）、'l1'（L1 正则化）、'elasticnet'（弹性网正则化）等。\n",
    "- alpha: 正则化项的惩罚力度。默认为0.0001。\n",
    "- max_iter: 最大迭代次数。默认为1000。\n",
    "- learning_rate: 学习率的类型或大小。可选参数有 'constant'（恒定学习率）、'optimal'（最优学习率）、'invscaling'（逆标度学习率）等。\n",
    "- eta0: 初始学习率。默认为0.01。\n",
    "\n",
    "主要方法：\n",
    "- fit(X, y): 使用训练数据训练模型。\n",
    "- predict(X): 对新的输入数据进行预测。\n",
    "- score(X, y): 返回模型在给定测试数据上的 R^2 分数。\n",
    "\n",
    "\n",
    "在 SGDRegressor 中，`tol` 是用来控制迭代的停止条件的参数。tol（tolerance）表示容忍度，即当损失函数的变化小于 tol 时，算法会停止迭代。`tol=1e-2` 表示容忍度为 0.01。也就是说，当连续两次迭代的损失函数值之差小于 0.01 时，算法会认为模型已经收敛，并且提前结束迭代，不再继续优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor train MSE:    0.13231376777560042\n",
      "SGDRegressor test MSE:    0.14449345818114218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures # 用于进行多项式特征转换。\n",
    "poly = PolynomialFeatures(5)\n",
    "train_data_poly = poly.fit_transform(train_data)\n",
    "test_data_poly = poly.transform(test_data)\n",
    "clf = SGDRegressor(max_iter=1000, tol=1e-3) \n",
    "clf.fit(train_data_poly, train_target)\n",
    "score_train = mean_squared_error(train_target, clf.predict(train_data_poly))\n",
    "score_test = mean_squared_error(test_target, clf.predict(test_data_poly))\n",
    "print(\"SGDRegressor train MSE:   \", score_train)\n",
    "print(\"SGDRegressor test MSE:   \", score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4正常拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor train MSE:    0.13446303779746302\n",
      "SGDRegressor test MSE:    0.14249559510800386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(3)\n",
    "train_data_poly = poly.fit_transform(train_data)\n",
    "test_data_poly = poly.transform(test_data)\n",
    "clf = SGDRegressor(max_iter=1000, tol=1e-3) \n",
    "clf.fit(train_data_poly, train_target)\n",
    "score_train = mean_squared_error(train_target, clf.predict(train_data_poly))\n",
    "score_test = mean_squared_error(test_target, clf.predict(test_data_poly))\n",
    "print(\"SGDRegressor train MSE:   \", score_train)\n",
    "print(\"SGDRegressor test MSE:   \", score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码解释\n",
    "`PolynomialFeatures` 是 scikit-learn（sklearn）库中的一个预处理类，用于生成多项式特征。它可以将原始特征转换为高阶多项式特征，从而扩展特征空间，使模型能够更好地拟合非线性关系。\n",
    "\n",
    "`PolynomialFeatures` 的主要作用是通过对原始特征进行多项式扩展，引入多项式交互项，从而增加模型的表示能力。对于给定的一组原始特征 x1, x2, ..., xn，`PolynomialFeatures` 将创建由这些特征的所有可能的多项式组合组成的新特征矩阵。\n",
    "\n",
    "`PolynomialFeatures` 可以生成包括以下几种特征的多项式：\n",
    "\n",
    "- 指数项：x^d （d 为指定的度数）\n",
    "- 交叉项：x1^i * x2^j * ... * xn^k （i + j + ... + k 不大于指定的度数）\n",
    "\n",
    "使用 `PolynomialFeatures` 的步骤如下：\n",
    "\n",
    "1. 创建 `PolynomialFeatures` 实例，并指定所需的度数。\n",
    "2. 使用 `fit_transform` 方法将原始特征数据集转换为多项式特征数据集。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 模型正则化\n",
    "\n",
    "正则化(Regularization)是给需要训练的目标函数加上一些规则（限制），目的是为了防止过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 L2范数正则化\n",
    "$$\n",
    "\\parallel x\\parallel_2=\\left(\\sum_{i=1}^n\\mid x_i\\mid^2\\right)^{\\frac{1}{2}}\n",
    "$$\n",
    "又叫欧几里得(Euclid)范数，即向量元素绝对值平方和再进行开方\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor train MSE:    0.13411403126239324\n",
      "SGDRegressor test MSE:    0.14231299221464913\n"
     ]
    }
   ],
   "source": [
    "poly = PolynomialFeatures(3)\n",
    "train_data_poly = poly.fit_transform(train_data)\n",
    "test_data_poly = poly.transform(test_data)\n",
    "clf = SGDRegressor(max_iter=1000, tol=1e-3, penalty= 'L2', alpha=0.0001) \n",
    "clf.fit(train_data_poly, train_target)\n",
    "score_train = mean_squared_error(train_target, clf.predict(train_data_poly))\n",
    "score_test = mean_squared_error(test_target, clf.predict(test_data_poly))\n",
    "print(\"SGDRegressor train MSE:   \", score_train)\n",
    "print(\"SGDRegressor test MSE:   \", score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 L1范数正则化\n",
    "$$\n",
    "\\parallel x\\parallel_1=\\sum_{i=1}^N\\mid x_i\\mid \n",
    "$$\n",
    "即向量元素绝对值之和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor train MSE:    0.13398000147745295\n",
      "SGDRegressor test MSE:    0.14251532036918307\n"
     ]
    }
   ],
   "source": [
    "poly = PolynomialFeatures(3)\n",
    "train_data_poly = poly.fit_transform(train_data)\n",
    "test_data_poly = poly.transform(test_data)\n",
    "clf = SGDRegressor(max_iter=1000, tol=1e-3, penalty= 'L1', alpha=0.00001) \n",
    "clf.fit(train_data_poly, train_target)\n",
    "score_train = mean_squared_error(train_target, clf.predict(train_data_poly))\n",
    "score_test = mean_squared_error(test_target, clf.predict(test_data_poly))\n",
    "print(\"SGDRegressor train MSE:   \", score_train)\n",
    "print(\"SGDRegressor test MSE:   \", score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 ElasticNet 联合 L1和L2范数加权正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor train MSE:    0.1340561345351531\n",
      "SGDRegressor test MSE:    0.14222684948804637\n"
     ]
    }
   ],
   "source": [
    "poly = PolynomialFeatures(3)\n",
    "train_data_poly = poly.fit_transform(train_data)\n",
    "test_data_poly = poly.transform(test_data)\n",
    "clf = SGDRegressor(max_iter=1000, tol=1e-3, penalty= 'elasticnet', l1_ratio=0.9, alpha=0.00001) \n",
    "clf.fit(train_data_poly, train_target)\n",
    "score_train = mean_squared_error(train_target, clf.predict(train_data_poly))\n",
    "score_test = mean_squared_error(test_target, clf.predict(test_data_poly))\n",
    "print(\"SGDRegressor train MSE:   \", score_train)\n",
    "print(\"SGDRegressor test MSE:   \", score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码解释\n",
    "\n",
    "1. `elasticnet`：是 `SGDRegressor` 的正则化方法之一。Elastic Net 是一种结合了 L1 正则化（Lasso）和 L2 正则化（Ridge）的线性回归模型正则化方法。通过引入两种正则化项，Elastic Net 可以在处理高维数据时具有特征选择的能力，并且可以克服 Lasso 存在的某些限制。默认情况下，`penalty` 参数被设置为 `'l2'`，即使用 L2 正则化；而设置为 `'elasticnet'` 则表示同时使用 L1 和 L2 正则化。\n",
    "\n",
    "2. `l1_ratio`：这是 Elastic Net 的混合参数，取值范围为 0 到 1 之间。它控制着 L1 正则化在 Elastic Net 中的比例。当 `l1_ratio` 为 0 时，相当于只使用 L2 正则化，而当 `l1_ratio` 为 1 时，相当于只使用 L1 正则化。在 0 和 1 之间的值表示混合使用两种正则化方法。在给定的示例中，`l1_ratio=0.9` 表示 Elastic Net 正则化主要使用 L1 正则化，较少使用 L2 正则化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 模型交叉验证\n",
    "\n",
    "## 3.1 简单交叉验证 Hold-out-menthod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor train MSE:    0.14147987854211916\n",
      "SGDRegressor test MSE:    0.14691226675167257\n"
     ]
    }
   ],
   "source": [
    "# 简单交叉验证\n",
    "from sklearn.model_selection import train_test_split # 切分数据\n",
    "# 切分数据 训练数据80% 验证数据20%\n",
    "train_data,test_data,train_target,test_target=train_test_split(train,target,test_size=0.2,random_state=0)\n",
    "\n",
    "clf = SGDRegressor(max_iter=1000, tol=1e-3) \n",
    "clf.fit(train_data, train_target)\n",
    "score_train = mean_squared_error(train_target, clf.predict(train_data))\n",
    "score_test = mean_squared_error(test_target, clf.predict(test_data))\n",
    "print(\"SGDRegressor train MSE:   \", score_train)\n",
    "print(\"SGDRegressor test MSE:   \", score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 K折交叉验证 K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  折 SGDRegressor train MSE:    0.1499970632107563\n",
      "0  折 SGDRegressor test MSE:    0.1063390268659094 \n",
      "\n",
      "1  折 SGDRegressor train MSE:    0.13356087422474547\n",
      "1  折 SGDRegressor test MSE:    0.18230158494740145 \n",
      "\n",
      "2  折 SGDRegressor train MSE:    0.14645508337571578\n",
      "2  折 SGDRegressor test MSE:    0.13285611993663504 \n",
      "\n",
      "3  折 SGDRegressor train MSE:    0.14071174844695053\n",
      "3  折 SGDRegressor test MSE:    0.16282750562016682 \n",
      "\n",
      "4  折 SGDRegressor train MSE:    0.13816484490648068\n",
      "4  折 SGDRegressor test MSE:    0.16468360066162507 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5折交叉验证\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "for k, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "    train_data,test_data,train_target,test_target = train.values[train_index],train.values[test_index],target[train_index],target[test_index]\n",
    "    clf = SGDRegressor(max_iter=1000, tol=1e-3) \n",
    "    clf.fit(train_data, train_target)\n",
    "    score_train = mean_squared_error(train_target, clf.predict(train_data))\n",
    "    score_test = mean_squared_error(test_target, clf.predict(test_data))\n",
    "    print(k, \" 折\", \"SGDRegressor train MSE:   \", score_train)\n",
    "    print(k, \" 折\", \"SGDRegressor test MSE:   \", score_test, '\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码详解 -`KFold`函数\n",
    "`KFold` 是 scikit-learn 库中的一个交叉验证方法，用于划分数据集为 k 折，并生成相应的训练集和测试集索引。\n",
    "\n",
    "语法如下：\n",
    "\n",
    "```python\n",
    "sklearn.model_selection.KFold(n_splits, shuffle=False, random_state=None)\n",
    "```\n",
    "\n",
    "参数说明：\n",
    "- `n_splits`：表示将数据集划分为几个折（即 k 值），默认为 5。\n",
    "- `shuffle`（可选）：表示是否在划分之前对数据进行洗牌，默认为 False。如果设置为 True，则会在划分之前对数据进行洗牌以打乱顺序。\n",
    "- `random_state`（可选）：表示随机数种子，用于指定洗牌时的随机性。设置相同的随机数种子可以保证每次划分的结果一致。\n",
    "\n",
    "常用方法和属性：\n",
    "\n",
    "- `split(X[, y, groups])`：返回一个生成器对象，用于生成每个折的训练集和测试集索引。\n",
    "- `get_n_splits([X, y, groups])`：返回划分的折数（即 k 值）。\n",
    "\n",
    "例子：\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 创建一个 KFold 对象\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# 模拟一个数据集\n",
    "data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# 使用 KFold 进行划分\n",
    "for train_index, test_index in kf.split(data):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    \n",
    "for train_i,test_i in kf.split(data):\n",
    "    print(data[train_i],data[test_i])\n",
    "    print(\"---------\")\n",
    "```\n",
    "\n",
    "输出结果：\n",
    "```\n",
    "Train: [2 3 4 5 6 7 8 9] Test: [0 1]\n",
    "Train: [0 1 4 5 6 7 8 9] Test: [2 3]\n",
    "Train: [0 1 2 3 6 7 8 9] Test: [4 5]\n",
    "Train: [0 1 2 3 4 5 8 9] Test: [6 7]\n",
    "Train: [0 1 2 3 4 5 6 7] Test: [8 9]\n",
    "\n",
    "[3  4  5  6  7  8  9 10] [1 2]\n",
    "---------\n",
    "[1  2  5  6  7  8  9 10] [3 4]\n",
    "---------\n",
    "[1  2  3  4  7  8  9 10] [5 6]\n",
    "---------\n",
    "[1  2  3  4  5  6  9 10] [7 8]\n",
    "---------\n",
    "[1  2  3  4  5  6  7  8] [9 10]\n",
    "---------\n",
    "```\n",
    "\n",
    "在这个例子中，我们先创建了一个 `KFold` 对象 `kf`，将数据集 `data` 划分为 5 折交叉验证。然后，在循环中，我们使用 `kf.split(data)` 生成了每个折的训练集索引 `train_index` 和测试集索引 `test_index`。通过打印这些索引，我们可以看到每个折的训练集和测试集索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码详解 -`enumerate`函数\n",
    "用于将一个可迭代对象转换为一个枚举对象。它返回一个包含索引和元素的元组的迭代器。\n",
    "\n",
    "语法如下：\n",
    "\n",
    "```python\n",
    "enumerate(iterable, start=0)\n",
    "```\n",
    "\n",
    "- `iterable`：表示要进行枚举的可迭代对象，可以是列表、元组、字符串、集合等。\n",
    "- `start`（可选）：表示索引的起始值，默认为 0。\n",
    "\n",
    "当对一个可迭代对象使用 `enumerate` 函数时，它会返回一个生成器对象，每次迭代都会产生一个元组 `(index, element)`，其中 `index` 是当前元素的索引，从 `start` 开始递增，`element` 是对应的元素。\n",
    "\n",
    "例子：\n",
    "\n",
    "```python\n",
    "fruits = ['apple', 'banana', 'orange']\n",
    "\n",
    "for index, fruit in enumerate(fruits):\n",
    "    print(index, fruit)\n",
    "```\n",
    "\n",
    "输出结果：\n",
    "```\n",
    "0 apple\n",
    "1 banana\n",
    "2 orange\n",
    "```\n",
    "\n",
    "在例子中，我们使用 `enumerate` 对列表 `fruits` 进行枚举。在每次迭代中，`index` 表示元素的索引，`fruit` 表示对应的水果名称。通过打印 `index` 和 `fruit`，我们可以看到每个元素的索引和对应的水果名称。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 留一法 LOO CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  个 SGDRegressor train MSE:    0.1410185842070489\n",
      "0  个 SGDRegressor test MSE:    0.010827929853781873 \n",
      "\n",
      "1  个 SGDRegressor train MSE:    0.141590758615128\n",
      "1  个 SGDRegressor test MSE:    0.12270162748400217 \n",
      "\n",
      "2  个 SGDRegressor train MSE:    0.1416252019921188\n",
      "2  个 SGDRegressor test MSE:    0.039192123489995435 \n",
      "\n",
      "3  个 SGDRegressor train MSE:    0.14165796049407978\n",
      "3  个 SGDRegressor test MSE:    0.0033427293223949133 \n",
      "\n",
      "4  个 SGDRegressor train MSE:    0.14178306922694353\n",
      "4  个 SGDRegressor test MSE:    0.014331018726507659 \n",
      "\n",
      "5  个 SGDRegressor train MSE:    0.1416194008941752\n",
      "5  个 SGDRegressor test MSE:    0.1395785088902323 \n",
      "\n",
      "6  个 SGDRegressor train MSE:    0.14155706330988685\n",
      "6  个 SGDRegressor test MSE:    0.02450958112859262 \n",
      "\n",
      "7  个 SGDRegressor train MSE:    0.14170072946953924\n",
      "7  个 SGDRegressor test MSE:    0.0005502535574655062 \n",
      "\n",
      "8  个 SGDRegressor train MSE:    0.14161675737940554\n",
      "8  个 SGDRegressor test MSE:    0.09323279818275634 \n",
      "\n",
      "9  个 SGDRegressor train MSE:    0.1416201187298724\n",
      "9  个 SGDRegressor test MSE:    0.05099617936227031 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "# num = 100\n",
    "for k, (train_index, test_index) in enumerate(loo.split(train)):\n",
    "    train_data,test_data,train_target,test_target = train.values[train_index],train.values[test_index],target[train_index],target[test_index]\n",
    "    clf = SGDRegressor(max_iter=1000, tol=1e-3) \n",
    "    clf.fit(train_data, train_target)\n",
    "    score_train = mean_squared_error(train_target, clf.predict(train_data))\n",
    "    score_test = mean_squared_error(test_target, clf.predict(test_data))\n",
    "    print(k, \" 个\", \"SGDRegressor train MSE:   \", score_train)\n",
    "    print(k, \" 个\", \"SGDRegressor test MSE:   \", score_test, '\\n') \n",
    "    if k >= 9: # k 大于等于 9时停止迭代\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 留P法 LPO CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  10个 SGDRegressor train MSE:    0.14200927354868126\n",
      "0  10个 SGDRegressor test MSE:    0.049055757037640826 \n",
      "\n",
      "1  10个 SGDRegressor train MSE:    0.14192871166773074\n",
      "1  10个 SGDRegressor test MSE:    0.0448838237516389 \n",
      "\n",
      "2  10个 SGDRegressor train MSE:    0.14204144385030168\n",
      "2  10个 SGDRegressor test MSE:    0.04598243659109025 \n",
      "\n",
      "3  10个 SGDRegressor train MSE:    0.14189127176134397\n",
      "3  10个 SGDRegressor test MSE:    0.054390058108866525 \n",
      "\n",
      "4  10个 SGDRegressor train MSE:    0.1419105219186622\n",
      "4  10个 SGDRegressor test MSE:    0.06974169710474927 \n",
      "\n",
      "5  10个 SGDRegressor train MSE:    0.14189622202861754\n",
      "5  10个 SGDRegressor test MSE:    0.04543581249452035 \n",
      "\n",
      "6  10个 SGDRegressor train MSE:    0.1419924494916834\n",
      "6  10个 SGDRegressor test MSE:    0.0491792400501897 \n",
      "\n",
      "7  10个 SGDRegressor train MSE:    0.14201889146876026\n",
      "7  10个 SGDRegressor test MSE:    0.05379754575378942 \n",
      "\n",
      "8  10个 SGDRegressor train MSE:    0.1418060540575406\n",
      "8  10个 SGDRegressor test MSE:    0.04699852587289613 \n",
      "\n",
      "9  10个 SGDRegressor train MSE:    0.1419593024137016\n",
      "9  10个 SGDRegressor test MSE:    0.045204896844313294 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeavePOut\n",
    "lpo = LeavePOut(p=10)\n",
    "# num = 100\n",
    "for k, (train_index, test_index) in enumerate(lpo.split(train)):\n",
    "    train_data,test_data,train_target,test_target = train.values[train_index],train.values[test_index],target[train_index],target[test_index]\n",
    "    clf = SGDRegressor(max_iter=1000, tol=1e-3) \n",
    "    clf.fit(train_data, train_target)\n",
    "    score_train = mean_squared_error(train_target, clf.predict(train_data))\n",
    "    score_test = mean_squared_error(test_target, clf.predict(test_data))\n",
    "    print(k, \" 10个\", \"SGDRegressor train MSE:   \", score_train)\n",
    "    print(k, \" 10个\", \"SGDRegressor test MSE:   \", score_test, '\\n') \n",
    "    if k >= 9: # k 大于等于 9时停止迭代。\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 留P交叉验证 和 K折交叉验证\n",
    "区别和特点如下：\n",
    "\n",
    "- 样本划分：留 P 交叉验证按照固定数量 P 的样本划分为测试集，剩余的样本为训练集；K 折交叉验证按照 K 个折的划分将数据集划分为测试集和训练集。\n",
    "- 迭代次数：留 P 交叉验证的迭代次数取决于样本组合的可能性，通常较大；K 折交叉验证的迭代次数为 K，通常较小。\n",
    "- 样本重复：留 P 交叉验证每个样本只出现一次作为测试集，可能会有样本重复出现在训练集中；K 折交叉验证每个样本会被分到不同的训练集和测试集中，避免了样本的重复。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4模型超参空间及调参\n",
    "\n",
    "## 4.1穷举网格搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor GridSearchCV test MSE:    0.25872053984728777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mean_fit_time',\n",
       " 'mean_score_time',\n",
       " 'mean_test_score',\n",
       " 'param_max_depth',\n",
       " 'param_n_estimators',\n",
       " 'params',\n",
       " 'rank_test_score',\n",
       " 'split0_test_score',\n",
       " 'split1_test_score',\n",
       " 'split2_test_score',\n",
       " 'split3_test_score',\n",
       " 'split4_test_score',\n",
       " 'std_fit_time',\n",
       " 'std_score_time',\n",
       " 'std_test_score']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用数据训练随机森林模型，采用网格搜索方法调参\n",
    "from sklearn.model_selection import GridSearchCV # 网格搜索\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split # 切分数据\n",
    "# 切分数据 训练数据80% 验证数据20%\n",
    "train_data,test_data,train_target,test_target=train_test_split(train,target,test_size=0.2,random_state=0)\n",
    "\n",
    "randomForestRegressor = RandomForestRegressor()\n",
    "parameters = {\n",
    "              'n_estimators':[50, 100, 200],\n",
    "              'max_depth':[1, 2, 3]\n",
    "        }\n",
    "\n",
    "clf = GridSearchCV(randomForestRegressor, parameters, cv=5) # cv对train_data使用 5 折交叉验证\n",
    "clf.fit(train_data, train_target)\n",
    "\n",
    "score_test = mean_squared_error(test_target, clf.predict(test_data))\n",
    "\n",
    "print(\"RandomForestRegressor GridSearchCV test MSE:   \", score_test)\n",
    "sorted(clf.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码解释\n",
    "1. GridSearchCV()\n",
    "\n",
    "`GridSearchCV` 是一个用于进行网格搜索的类，可以用于参数调优和模型选择。\n",
    "\n",
    "网格搜索是指将所有可能的参数组合都尝试一遍，并对每组参数进行评估。通过设置参数范围和步长等信息，我们可以控制参数搜索的规模。\n",
    "\n",
    "下面是使用 `GridSearchCV` 的一般流程：\n",
    "\n",
    "1. 导入 `GridSearchCV` 类和要使用的模型类。\n",
    "2. 定义要搜索的参数空间（通常是一个字典）。\n",
    "3. 创建模型对象。\n",
    "4. 创建 `GridSearchCV` 对象，传入模型对象、参数空间和其他参数（例如交叉验证的折数）。\n",
    "5. 使用 `fit` 方法拟合数据，并进行网格搜索和交叉验证。\n",
    "6. 可以通过 `best_params_` 属性查看最佳参数组合。\n",
    "7. 可以通过 `best_score_` 属性查看最佳参数组合的得分。\n",
    "8. 可以通过 `cv_results_` 属性查看所有参数组合的详细结果。\n",
    "-----\n",
    "2. `clf.cv_results_.keys()` \n",
    "\n",
    "是一个列表，该列表返回字典 `clf.cv_results_` 中所有的键。`clf.cv_results_` 包含了网格搜索过程中的详细结果，包括每组参数组合的得分、训练时间等信息。\n",
    "\n",
    "通过执行 `clf.cv_results_.keys()`，你将会得到一个列表，其中包含了所有的键。这个列表提供了一个快速查看可用键的方式。\n",
    "\n",
    "常见的一些键可能包括：\n",
    "\n",
    "- `'mean_fit_time'`：每个参数组合的平均训练时间。\n",
    "- `'mean_score_time'`：每个参数组合的平均预测时间。\n",
    "- `'mean_test_score'`：每个参数组合在交叉验证的测试集上的平均得分。\n",
    "- `'param_XXX'`：参数 `XXX` 的值（例如，如果有参数 `'n_estimators'`，则会出现 `'param_n_estimators'`）。\n",
    "- `'rank_test_score'`：每个参数组合在交叉验证中的得分排名。\n",
    "- `'splitX_test_score'`：第 X 折交叉验证中每个参数组合的得分。\n",
    "\n",
    "这些指标在模型分析中十分重要，可以查找到整个建模过程中可能的异常\n",
    "\n",
    "`sorted(clf.cv_results_.keys())` 按字母顺序排序并打印出 clf.cv_results_ 字典中的键."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 随机参数优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor RandomizedSearchCV test MSE:    0.1957208280769947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mean_fit_time',\n",
       " 'mean_score_time',\n",
       " 'mean_test_score',\n",
       " 'param_max_depth',\n",
       " 'param_n_estimators',\n",
       " 'params',\n",
       " 'rank_test_score',\n",
       " 'split0_test_score',\n",
       " 'split1_test_score',\n",
       " 'split2_test_score',\n",
       " 'split3_test_score',\n",
       " 'split4_test_score',\n",
       " 'std_fit_time',\n",
       " 'std_score_time',\n",
       " 'std_test_score']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用数据训练随机森林模型，采用随机参数优化方法调参\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split # 切分数据\n",
    "# 切分数据 训练数据80% 验证数据20%\n",
    "train_data,test_data,train_target,test_target=train_test_split(train,target,test_size=0.2,random_state=0)\n",
    "\n",
    "randomForestRegressor = RandomForestRegressor()\n",
    "parameters = {\n",
    "              'n_estimators':[50, 100, 200, 300],\n",
    "              'max_depth':[1, 2, 3, 4, 5]\n",
    "        }\n",
    "\n",
    "clf = RandomizedSearchCV(randomForestRegressor, parameters, cv=5)\n",
    "clf.fit(train_data, train_target)\n",
    "\n",
    "score_test = mean_squared_error(test_target, clf.predict(test_data))\n",
    "\n",
    "print(\"RandomForestRegressor RandomizedSearchCV test MSE:   \", score_test)\n",
    "sorted(clf.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码解释\n",
    "`RandomizedSearchCV` 是一个用于进行随机搜索的类，与 `GridSearchCV` 类似，它也可以用于参数调优和模型选择。\n",
    "\n",
    "与网格搜索不同的是，随机搜索并不尝试所有可能的参数组合。相反，它从参数空间中随机抽样一组参数，并对每组参数进行评估。通过设置抽样的次数，我们可以控制随机搜索的规模。\n",
    "\n",
    "下面是使用 `RandomizedSearchCV` 的一般流程：\n",
    "\n",
    "1. 导入 `RandomizedSearchCV` 类和要使用的模型类。\n",
    "2. 定义要搜索的参数空间。可以使用分布函数（如均匀分布、正态分布），也可以使用指定的参数列表。\n",
    "3. 创建模型对象。\n",
    "4. 创建 `RandomizedSearchCV` 对象，传入模型对象、参数空间和其他参数（例如交叉验证的折数）。\n",
    "5. 使用 `fit` 方法拟合数据，并进行随机搜索和交叉验证。\n",
    "6. 可以通过 `best_params_` 属性查看最佳参数组合。\n",
    "7. 可以通过 `best_score_` 属性查看最佳参数组合的得分。\n",
    "8. 可以通过 `cv_results_` 属性查看所有参数组合的详细结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Lgb 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000201 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.113883\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.124781\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.129659\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.128611\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000148 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.134065\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.113883\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.124781\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.129659\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.128611\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000121 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.134065\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000176 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.113883\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.124781\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.129659\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000125 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.128611\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.134065\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.113883\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.124781\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.129659\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.128611\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000167 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.134065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.113883\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.124781\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.129659\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.128611\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.134065\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.113883\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.124781\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.129659\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.128611\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 1848, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.134065\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4080\n",
      "[LightGBM] [Info] Number of data points in the train set: 2310, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 0.126200\n",
      "Best parameters found by grid search are: {'learning_rate': 0.1, 'log_evaluation': 10000, 'n_estimators': 40}\n",
      "[LightGBM] [Warning] Unknown parameter: log_evaluation\n",
      "LGBMRegressor RandomizedSearchCV test MSE:    0.1488757209972878\n"
     ]
    }
   ],
   "source": [
    "# 使用数据训练LGB模型，采用网格搜索方法调参\n",
    "\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "callbacks = [log_evaluation(period=1000)]\n",
    "\n",
    "clf = lgb.LGBMRegressor(num_leaves=31)\n",
    "\n",
    "parameters = {\n",
    "  \n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [20, 40]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(clf, parameters, cv=5)\n",
    "clf.fit(train_data, train_target)\n",
    "\n",
    "print('Best parameters found by grid search are:', clf.best_params_)\n",
    "score_test = mean_squared_error(test_target, clf.predict(test_data))\n",
    "print(\"LGBMRegressor RandomizedSearchCV test MSE:   \", score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Lgb 线下验证\n",
    "下面进行数据建模、5折交叉验证、划分数据、对LGB模型进行训练、计算MSE评价性能等流程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data2 = pd.read_csv('./data/zhengqi_train.txt',sep='\\t')\n",
    "test_data2 = pd.read_csv('./data/zhengqi_test.txt',sep='\\t')\n",
    "\n",
    "train_data2_f = train_data2[test_data2.columns].values\n",
    "train_data2_target = train_data2['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000679 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8853\n",
      "[LightGBM] [Info] Number of data points in the train set: 2310, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.121817\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tTrain's l2: 0.223959\tTest's l2: 0.247217\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tTrain's l2: 0.223959\tTest's l2: 0.247217\n",
      "第0折 训练和预测 训练MSE 预测MSE\n",
      "------\n",
      " 训练MSE\n",
      " 0.22395910176815867 \n",
      "------\n",
      "------\n",
      " 预测MSE\n",
      " 0.24721747302378572 \n",
      "------\n",
      "\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8853\n",
      "[LightGBM] [Info] Number of data points in the train set: 2310, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.113222\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tTrain's l2: 0.221409\tTest's l2: 0.25435\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tTrain's l2: 0.221409\tTest's l2: 0.25435\n",
      "第1折 训练和预测 训练MSE 预测MSE\n",
      "------\n",
      " 训练MSE\n",
      " 0.22140920255015417 \n",
      "------\n",
      "------\n",
      " 预测MSE\n",
      " 0.2543498234383045 \n",
      "------\n",
      "\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000840 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8838\n",
      "[LightGBM] [Info] Number of data points in the train set: 2310, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.132497\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tTrain's l2: 0.221398\tTest's l2: 0.256187\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tTrain's l2: 0.221398\tTest's l2: 0.256187\n",
      "第2折 训练和预测 训练MSE 预测MSE\n",
      "------\n",
      " 训练MSE\n",
      " 0.22139783747290231 \n",
      "------\n",
      "------\n",
      " 预测MSE\n",
      " 0.25618731408519657 \n",
      "------\n",
      "\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000649 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8849\n",
      "[LightGBM] [Info] Number of data points in the train set: 2311, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.125889\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tTrain's l2: 0.225142\tTest's l2: 0.250629\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tTrain's l2: 0.225142\tTest's l2: 0.250629\n",
      "第3折 训练和预测 训练MSE 预测MSE\n",
      "------\n",
      " 训练MSE\n",
      " 0.22514211678664525 \n",
      "------\n",
      "------\n",
      " 预测MSE\n",
      " 0.2506288004181115 \n",
      "------\n",
      "\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8837\n",
      "[LightGBM] [Info] Number of data points in the train set: 2311, number of used features: 38\n",
      "[LightGBM] [Info] Start training from score 0.138334\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tTrain's l2: 0.2212\tTest's l2: 0.256939\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tTrain's l2: 0.2212\tTest's l2: 0.256939\n",
      "第4折 训练和预测 训练MSE 预测MSE\n",
      "------\n",
      " 训练MSE\n",
      " 0.22120001873881776 \n",
      "------\n",
      "------\n",
      " 预测MSE\n",
      " 0.2569386630797994 \n",
      "------\n",
      "\n",
      "------\n",
      " 训练MSE\n",
      " [0.22395910176815867, 0.22140920255015417, 0.22139783747290231, 0.22514211678664525, 0.22120001873881776] \n",
      " 0.22262165546333562 \n",
      "------\n",
      "------\n",
      " 预测MSE\n",
      " [0.24721747302378572, 0.2543498234383045, 0.25618731408519657, 0.2506288004181115, 0.2569386630797994] \n",
      " 0.2530644148090395 \n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# lgb 模型\n",
    "from sklearn.model_selection  import KFold\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "###\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "callbacks = [log_evaluation(period=100),early_stopping(stopping_rounds=100)]\n",
    "###\n",
    "\n",
    "# 5折交叉验证\n",
    "Folds=5\n",
    "kf = KFold( n_splits=Folds, random_state=100, shuffle=True)\n",
    "# 记录训练和预测MSE\n",
    "MSE_DICT = {\n",
    "    'train_mse':[],\n",
    "    'test_mse':[]\n",
    "}\n",
    "\n",
    "# 线下训练预测\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_data2_f)):\n",
    "    # lgb树模型\n",
    "    lgb_reg = lgb.LGBMRegressor(\n",
    "        learning_rate=0.01,\n",
    "        max_depth=-1,\n",
    "        n_estimators=100,\n",
    "        boosting_type='gbdt',\n",
    "        random_state=100,\n",
    "        objective='regression',\n",
    "    )\n",
    "   \n",
    "    # 切分训练集和预测集\n",
    "    X_train_KFold, X_test_KFold = train_data2_f[train_index], train_data2_f[test_index]\n",
    "    y_train_KFold, y_test_KFold = train_data2_target[train_index], train_data2_target[test_index]\n",
    "    \n",
    "    # 训练模型\n",
    "#     reg.fit(X_train_KFold, y_train_KFold)\n",
    "    lgb_reg.fit(\n",
    "            X=X_train_KFold,y=y_train_KFold,\n",
    "            eval_set=[(X_train_KFold, y_train_KFold),(X_test_KFold, y_test_KFold)],\n",
    "            eval_names=['Train','Test'],\n",
    "            #early_stopping_rounds=100,#该参数现已弃用\n",
    "            eval_metric='MSE',\n",
    "            #verbose=50，#该参数现已弃用\n",
    "            ###\n",
    "            callbacks=callbacks\n",
    "            ###\n",
    "        )\n",
    "\n",
    "\n",
    "    # 训练集预测 测试集预测\n",
    "    y_train_KFold_predict = lgb_reg.predict(X_train_KFold,num_iteration=lgb_reg.best_iteration_)\n",
    "    y_test_KFold_predict = lgb_reg.predict(X_test_KFold,num_iteration=lgb_reg.best_iteration_) \n",
    "    \n",
    "    print('第{}折 训练和预测 训练MSE 预测MSE'.format(i))\n",
    "    train_mse = mean_squared_error(y_train_KFold_predict, y_train_KFold)\n",
    "    print('------\\n', '训练MSE\\n', train_mse, '\\n------')\n",
    "    test_mse = mean_squared_error(y_test_KFold_predict, y_test_KFold)\n",
    "    print('------\\n', '预测MSE\\n', test_mse, '\\n------\\n')\n",
    "    \n",
    "    MSE_DICT['train_mse'].append(train_mse)\n",
    "    MSE_DICT['test_mse'].append(test_mse)\n",
    "print('------\\n', '训练MSE\\n', MSE_DICT['train_mse'], '\\n', np.mean(MSE_DICT['train_mse']), '\\n------')\n",
    "print('------\\n', '预测MSE\\n', MSE_DICT['test_mse'], '\\n', np.mean(MSE_DICT['test_mse']), '\\n------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 学习曲线和验证曲线\n",
    "\n",
    "## 5.1 学习曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'n_splits'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11956\\713786604.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# score curves, each time with 20% data randomly selected as a validation set.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m cv = model_selection.ShuffleSplit(X.shape[0], n_splits=100,\n\u001b[1;32m---> 48\u001b[1;33m                                    test_size=0.2, random_state=0)\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGDRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got multiple values for argument 'n_splits'"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection \n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "X = train_data2[test_data2.columns].values\n",
    "y = train_data2['target'].values\n",
    "\n",
    "\n",
    "title = \"LinearRegression\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = model_selection.ShuffleSplit(X.shape[0], n_splits=100,\n",
    "                                   test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = SGDRegressor()\n",
    "plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 验证曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "X = train_data2[test_data2.columns].values\n",
    "y = train_data2['target'].values\n",
    "# max_iter=1000, tol=1e-3, penalty= 'L1', alpha=0.00001\n",
    "\n",
    "param_range = [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    SGDRegressor(max_iter=1000, tol=1e-3, penalty= 'L1'), X, y, param_name=\"alpha\", param_range=param_range,\n",
    "    cv=10, scoring='r2', n_jobs=1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with SGDRegressor\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"g\")\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
