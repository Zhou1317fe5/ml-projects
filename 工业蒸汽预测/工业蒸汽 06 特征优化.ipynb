{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 赛题特征优化\n",
    "\n",
    "# 1 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data_file = \"./data/zhengqi_train.txt\"\n",
    "test_data_file =  \"./data/zhengqi_test.txt\"\n",
    "\n",
    "train_data = pd.read_csv(train_data_file, sep='\\t', encoding='utf-8')\n",
    "test_data = pd.read_csv(test_data_file, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 定义特征构造方法\n",
    "定义特征构造方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1e-5\n",
    "\n",
    "#组交叉特征，可以自行定义，如增加： x*x/y, log(x)/y 等等。\n",
    "func_dict = {\n",
    "            'add': lambda x,y: x+y,\n",
    "            'mins': lambda x,y: x-y,\n",
    "            'div': lambda x,y: x/(y+epsilon),\n",
    "            'multi': lambda x,y: x*y\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码解释\n",
    "`epsilon` 经常用作一个非常小的正数，用于避免在计算中出现除以零的情况。\n",
    "\n",
    "该代码中的 `epsilon` 被用作分母在除法运算中的修正因子。当分母接近零时，加上一个非常小的正数 `epsilon` 可以确保避免出现除以零的错误，同时保持数值计算的准确性。\n",
    "\n",
    "`epsilon` 被定义为 `1e-5`，即 0.00001。在执行诸如 `div` 函数的除法运算时，分母会加上 `epsilon`，从而避免除以零的情况。这样可以确保在进行组交叉特征计算时，不会出现除以零的错误，并保持计算的稳定性和准确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 定义特征构造函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_features_make(train_data,test_data,func_dict,col_list):\n",
    "    train_data, test_data = train_data.copy(), test_data.copy()\n",
    "    \n",
    "    for col_i in col_list:\n",
    "        for col_j in col_list:\n",
    "            for func_name, func in func_dict.items():\n",
    "                for data in [train_data,test_data]:\n",
    "                    func_features = func(data[col_i],data[col_j])\n",
    "                    col_func_features = '-'.join([col_i,func_name,col_j])\n",
    "                    data[col_func_features] = func_features\n",
    "                    \n",
    "    return train_data,test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码解释\n",
    "函数的作用是在给定的数据集 `train_data` 和 `test_data` 上生成自动特征。\n",
    "\n",
    "1. 拷贝 `train_data` 和 `test_data` 数据集。\n",
    "\n",
    "\n",
    "2. 使用嵌套的循环`    for col_i in col_list:` 和 `for col_j in col_list:` 遍历 `col_list` 中的每一对列名 `col_i` 和 `col_j`,例如`[V1 V0\n",
    "]`，`[V1 V1]`。\n",
    "    \n",
    "\n",
    "3. 在内部的循环中，遍历 `func_dict` 中的每个函数，这些函数以函数名 `func_name` 和对应的函数对象 `func` 表示。\n",
    "\n",
    "\n",
    "4. 对于每个数据集（`train_data` 和 `test_data`），将 `col_i` 和 `col_j` 列的值作为参数传递给当前迭代的函数 `func`，并计算得到新的特征值 `func_features`。\n",
    "\n",
    "\n",
    "5. 创建一个新的特征列名 `col_func_features`，使用 `-` 连接 `col_i`、`func_name` 和 `col_j`。\n",
    "\n",
    "\n",
    "6. 将计算得到的新特征值 `func_features` 存储在对应的特征列 `col_func_features` 中。\n",
    "\n",
    "\n",
    "7. 重复步骤 4-6，直到遍历完所有的列和函数组合。\n",
    "\n",
    "\n",
    "8. 最后，返回生成新特征后的 `train_data` 和 `test_data`。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对训练集和测试集数据进行特征构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\anaconda3\\envs\\ml\\lib\\site-packages\\ipykernel_launcher.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "train_data2, test_data2 = auto_features_make(train_data,test_data,func_dict,col_list=test_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码解释\n",
    "警告提示数据集可能存在严重的碎片化问题，即内存中的 DataFrame 对象非常分散。这通常是由于多次调用 DataFrame.insert 方法导致的，而 insert 方法的性能较差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 特征降维处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA   #主成分分析法\n",
    "\n",
    "#PCA方法降维\n",
    "pca = PCA(n_components=500)\n",
    "# train_data2_pca = pca.fit_transform(train_data2.iloc[:,0:-1]) # 有误 target列并不在最后一列\n",
    "train_data2_pca = pca.fit_transform(train_data2.drop(['target'], axis=1)) # 提取特征列\n",
    "test_data2_pca = pca.transform(test_data2)\n",
    "\n",
    "train_data2_pca = pd.DataFrame(train_data2_pca)\n",
    "test_data2_pca = pd.DataFrame(test_data2_pca)\n",
    "\n",
    "train_data2_pca['target'] = train_data2['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = train_data2[test_data2.columns].values\n",
    "y_train = train_data2['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 模型训练和评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用lightgbm模型对新构造的特征进行模型训练和评估。注意运行时间长 二十多分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.257947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1467773\n",
      "[LightGBM] [Info] Number of data points in the train set: 2310, number of used features: 5776\n",
      "[LightGBM] [Info] Start training from score 0.125850\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tTrain's l2: 0.198054\tTest's l2: 0.246445\n",
      "[200]\tTrain's l2: 0.068339\tTest's l2: 0.132002\n",
      "[300]\tTrain's l2: 0.0357877\tTest's l2: 0.111144\n",
      "[400]\tTrain's l2: 0.0224493\tTest's l2: 0.105219\n",
      "[500]\tTrain's l2: 0.0150641\tTest's l2: 0.102189\n",
      "[600]\tTrain's l2: 0.0105281\tTest's l2: 0.10117\n",
      "[700]\tTrain's l2: 0.0075079\tTest's l2: 0.100629\n",
      "[800]\tTrain's l2: 0.00546952\tTest's l2: 0.100569\n",
      "[900]\tTrain's l2: 0.00404382\tTest's l2: 0.100378\n",
      "[1000]\tTrain's l2: 0.00302518\tTest's l2: 0.100273\n",
      "[1100]\tTrain's l2: 0.00230305\tTest's l2: 0.100035\n",
      "[1200]\tTrain's l2: 0.00177343\tTest's l2: 0.0998876\n",
      "[1300]\tTrain's l2: 0.00137755\tTest's l2: 0.0997785\n",
      "[1400]\tTrain's l2: 0.00108744\tTest's l2: 0.0996757\n",
      "[1500]\tTrain's l2: 0.000869836\tTest's l2: 0.0996087\n",
      "[1600]\tTrain's l2: 0.000705997\tTest's l2: 0.0995319\n",
      "[1700]\tTrain's l2: 0.000579663\tTest's l2: 0.0994892\n",
      "[1800]\tTrain's l2: 0.000481583\tTest's l2: 0.0994377\n",
      "[1900]\tTrain's l2: 0.000404522\tTest's l2: 0.0994234\n",
      "[2000]\tTrain's l2: 0.000343499\tTest's l2: 0.0993884\n",
      "[2100]\tTrain's l2: 0.000294169\tTest's l2: 0.0993409\n",
      "[2200]\tTrain's l2: 0.000254251\tTest's l2: 0.0993272\n",
      "[2300]\tTrain's l2: 0.00022147\tTest's l2: 0.0993203\n",
      "[2400]\tTrain's l2: 0.000194188\tTest's l2: 0.0993115\n",
      "[2500]\tTrain's l2: 0.000171526\tTest's l2: 0.099301\n",
      "[2600]\tTrain's l2: 0.00015223\tTest's l2: 0.0993005\n",
      "Early stopping, best iteration is:\n",
      "[2507]\tTrain's l2: 0.000170121\tTest's l2: 0.0992975\n",
      "第0折 训练和预测 训练MSE 预测MSE\n",
      "------\n",
      " 训练MSE\n",
      " 0.00017012122282987903 \n",
      "------\n",
      "------\n",
      " 预测MSE\n",
      " 0.09929753449545065 \n",
      "------\n",
      "\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.241817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1467677\n",
      "[LightGBM] [Info] Number of data points in the train set: 2310, number of used features: 5776\n",
      "[LightGBM] [Info] Start training from score 0.111800\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tTrain's l2: 0.198813\tTest's l2: 0.250634\n",
      "[200]\tTrain's l2: 0.0661544\tTest's l2: 0.152931\n",
      "[300]\tTrain's l2: 0.033771\tTest's l2: 0.133815\n",
      "[400]\tTrain's l2: 0.0208654\tTest's l2: 0.128809\n",
      "[500]\tTrain's l2: 0.0137505\tTest's l2: 0.126561\n",
      "[600]\tTrain's l2: 0.00936581\tTest's l2: 0.126071\n",
      "[700]\tTrain's l2: 0.00651927\tTest's l2: 0.125727\n",
      "[800]\tTrain's l2: 0.00457076\tTest's l2: 0.125187\n",
      "[900]\tTrain's l2: 0.00325252\tTest's l2: 0.125089\n",
      "Early stopping, best iteration is:\n",
      "[840]\tTrain's l2: 0.00398611\tTest's l2: 0.12501\n",
      "第1折 训练和预测 训练MSE 预测MSE\n",
      "------\n",
      " 训练MSE\n",
      " 0.003986113221006774 \n",
      "------\n",
      "------\n",
      " 预测MSE\n",
      " 0.1250100344303853 \n",
      "------\n",
      "\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264048 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1467765\n",
      "[LightGBM] [Info] Number of data points in the train set: 2310, number of used features: 5776\n",
      "[LightGBM] [Info] Start training from score 0.131779\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tTrain's l2: 0.201107\tTest's l2: 0.226116\n",
      "[200]\tTrain's l2: 0.069817\tTest's l2: 0.120116\n",
      "[300]\tTrain's l2: 0.0365592\tTest's l2: 0.100598\n",
      "[400]\tTrain's l2: 0.0228129\tTest's l2: 0.0955853\n",
      "[500]\tTrain's l2: 0.0152273\tTest's l2: 0.0942304\n",
      "[600]\tTrain's l2: 0.0105302\tTest's l2: 0.0934844\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1912\\2094296319.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MSE'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;31m# verbose=50 # 已弃用\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperiod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         )\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Development\\anaconda3\\envs\\ml\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1060\u001b[0m             \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1062\u001b[1;33m             \u001b[0minit_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1063\u001b[0m         )\n\u001b[0;32m   1064\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Development\\anaconda3\\envs\\ml\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    849\u001b[0m             \u001b[0minit_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m             \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 851\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    852\u001b[0m         )\n\u001b[0;32m    853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Development\\anaconda3\\envs\\ml\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    264\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_LGBM_BoosterEvalMethodResultType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Development\\anaconda3\\envs\\ml\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   3557\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   3558\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3559\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   3560\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3561\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ls_validation i\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# 5折交叉验证\n",
    "Folds=5\n",
    "kf = KFold(n_splits=Folds, random_state=2019, shuffle=True)\n",
    "# 记录训练和预测MSE\n",
    "MSE_DICT = {\n",
    "    'train_mse':[],\n",
    "    'test_mse':[]\n",
    "}\n",
    "\n",
    "# 线下训练预测\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X_train2)):\n",
    "    # lgb树模型\n",
    "    lgb_reg = lgb.LGBMRegressor(\n",
    "        learning_rate=0.01,\n",
    "        max_depth=-1,\n",
    "        n_estimators=5000,\n",
    "        boosting_type='gbdt',\n",
    "        random_state=2019,\n",
    "        objective='regression',\n",
    "    )\n",
    "   \n",
    "    # 切分训练集和预测集\n",
    "    X_train_KFold, X_test_KFold = X_train2[train_index], X_train2[test_index]\n",
    "    y_train_KFold, y_test_KFold = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    # 训练模型\n",
    "    lgb_reg.fit(\n",
    "            X=X_train_KFold,y=y_train_KFold,\n",
    "            eval_set=[(X_train_KFold, y_train_KFold),(X_test_KFold, y_test_KFold)],\n",
    "            eval_names=['Train','Test'],\n",
    "            # early_stopping_rounds=100,# 已弃用\n",
    "            eval_metric='MSE',\n",
    "            # verbose=50 # 已弃用\n",
    "        callbacks=[lgb.log_evaluation(period=100), lgb.early_stopping(stopping_rounds=100)]\n",
    "        )\n",
    "\n",
    "\n",
    "    # 训练集预测 测试集预测\n",
    "    y_train_KFold_predict = lgb_reg.predict(X_train_KFold,num_iteration=lgb_reg.best_iteration_)\n",
    "    y_test_KFold_predict = lgb_reg.predict(X_test_KFold,num_iteration=lgb_reg.best_iteration_) \n",
    "    \n",
    "    print('第{}折 训练和预测 训练MSE 预测MSE'.format(i))\n",
    "    train_mse = mean_squared_error(y_train_KFold_predict, y_train_KFold)\n",
    "    print('------\\n', '训练MSE\\n', train_mse, '\\n------')\n",
    "    test_mse = mean_squared_error(y_test_KFold_predict, y_test_KFold)\n",
    "    print('------\\n', '预测MSE\\n', test_mse, '\\n------\\n')\n",
    "    \n",
    "    MSE_DICT['train_mse'].append(train_mse)\n",
    "    MSE_DICT['test_mse'].append(test_mse)\n",
    "print('------\\n', '训练MSE\\n', MSE_DICT['train_mse'], '\\n', np.mean(MSE_DICT['train_mse']), '\\n------')\n",
    "print('------\\n', '预测MSE\\n', MSE_DICT['test_mse'], '\\n', np.mean(MSE_DICT['test_mse']), '\\n------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
