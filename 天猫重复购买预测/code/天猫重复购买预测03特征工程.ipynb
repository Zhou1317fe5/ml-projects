{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 工具导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import gc\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    " \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据集\n",
    "test_data = pd.read_csv('./data/data_format1/test_format1.csv')\n",
    "train_data = pd.read_csv('./data/data_format1/train_format1.csv')\n",
    "user_info = pd.read_csv('./data/data_format1/user_info_format1.csv')\n",
    "user_log = pd.read_csv('./data/data_format1/user_log_format1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据资源查看**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260864 entries, 0 to 260863\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype\n",
      "---  ------       --------------   -----\n",
      " 0   user_id      260864 non-null  int64\n",
      " 1   merchant_id  260864 non-null  int64\n",
      " 2   label        260864 non-null  int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 6.0 MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 261477 entries, 0 to 261476\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   user_id      261477 non-null  int64  \n",
      " 1   merchant_id  261477 non-null  int64  \n",
      " 2   prob         0 non-null       float64\n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 6.0 MB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 424170 entries, 0 to 424169\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   user_id    424170 non-null  int64  \n",
      " 1   age_range  421953 non-null  float64\n",
      " 2   gender     417734 non-null  float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 9.7 MB\n"
     ]
    }
   ],
   "source": [
    "user_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54925330 entries, 0 to 54925329\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   user_id      int64  \n",
      " 1   item_id      int64  \n",
      " 2   cat_id       int64  \n",
      " 3   seller_id    int64  \n",
      " 4   brand_id     float64\n",
      " 5   time_stamp   int64  \n",
      " 6   action_type  int64  \n",
      "dtypes: float64(1), int64(6)\n",
      "memory usage: 2.9 GB\n"
     ]
    }
   ],
   "source": [
    "user_log.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据资源非常大，甚至达到2.9GB，需要进行数据压缩 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 对数据进行内存压缩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）定义内存压缩方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    # 开始时的内存使用\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    \n",
    "    # 遍历每一列\n",
    "    for col in df.columns:\n",
    "        # 获取列的数据类型\n",
    "        col_type = df[col].dtypes\n",
    "        # 如果列的数据类型在数值类型中\n",
    "        if col_type in numerics:\n",
    "            # 获取列的最小值和最大值\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            # 如果数据类型为int\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                # 如果最小值大于int8的最小值，最大值小于int8的最大值\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    # 将列的数据类型转换为int8\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                # 如果最小值大于int16的最小值，最大值小于int16的最大值\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    # 将列的数据类型转换为int16\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                # 如果最小值大于int32的最小值，最大值小于int32的最大值\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    # 将列的数据类型转换为int32\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                # 如果最小值大于int64的最小值，最大值小于int64的最大值\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    # 将列的数据类型转换为int64\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            # 如果数据类型为float\n",
    "            else:\n",
    "                # 如果最小值大于float16的最小值，最大值小于float16的最大值\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    # 将列的数据类型转换为float16\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                # 如果最小值大于float32的最小值，最大值小于float32的最大值\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    # 将列的数据类型转换为float32\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                # 如果最小值大于float64的最小值，最大值小于float64的最大值\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    # 结束时的内存使用\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    # 打印优化后的内存使用\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    # 打印优化率\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    # 返回优化后的DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码解释\n",
    "用于优化数据帧（dataframe）的内存使用，主要目的是将数据帧中的整数和浮点数类型转换为更小的数据类型，从而减少内存占用。\n",
    "\n",
    "以下是代码的详细解释：\n",
    "\n",
    "1. 定义一个名为`reduce_mem_usage`的函数，接受一个数据帧`df`和一个布尔值`verbose`作为参数。\n",
    "\n",
    "2. 计算原始数据帧的内存使用情况，并将结果除以1024^2以转换为MB。\n",
    "\n",
    "3. 定义一个包含整数和浮点数类型的列表`numerics`。\n",
    "\n",
    "4. 遍历数据帧的列（`for col in df.columns:`）。\n",
    "\n",
    "5. 获取当前列的数据类型（`col_type = df[col].dtypes`）。\n",
    "\n",
    "6. 如果当前列的数据类型在`numerics`列表中（`if col_type in numerics:`），则进行以下操作：\n",
    "\n",
    "    a. 计算当前列的最小值和最大值（`c_min = df[col].min()`和`c_max = df[col].max()`）。\n",
    "\n",
    "    b. 如果当前列的数据类型是整数类型（`if str(col_type)[:3] == 'int'`），则检查当前列的最小值和最大值是否在整数类型的范围中。如果是，则将当前列转换为更小的整数类型（`int8`、`int16`、`int32`或`int64`）。\n",
    "\n",
    "    c. 如果当前列的数据类型是浮点数类型，则检查当前列的最小值和最大值是否在浮点数类型的范围中。如果是，则将当前列转换为更小的浮点数类型（`float16`、`float32`或`64`）。\n",
    "\n",
    "7. 计算优化后的数据帧的内存使用情况，并将结果除以1024^2以转换为MB。\n",
    "\n",
    "8. 打印优化后的数据帧的内存使用情况，以及与原始数据帧的内存使用情况的百分比差异。\n",
    "\n",
    "9. 返回优化后的数据帧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）对数据进行内存压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取函数\n",
    "def read_csv(file_name, num_rows):\n",
    "    return pd.read_csv(file_name, nrows=num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 1.74 MB\n",
      "Decreased by 70.8%\n",
      "Memory usage after optimization is: 3.49 MB\n",
      "Decreased by 41.7%\n",
      "Memory usage after optimization is: 3.24 MB\n",
      "Decreased by 66.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 32.43 MB\n",
      "Decreased by 69.6%\n"
     ]
    }
   ],
   "source": [
    "num_rows = None\n",
    "num_rows = 200 * 10000 \n",
    "\n",
    "train_file = './data/data_format1/train_format1.csv'\n",
    "test_file = './data/data_format1/test_format1.csv'\n",
    "\n",
    "user_info_file = './data/data_format1/user_info_format1.csv'\n",
    "user_log_file = './data/data_format1/user_log_format1.csv'\n",
    "\n",
    "train_data = reduce_mem_usage(read_csv(train_file, num_rows))\n",
    "test_data = reduce_mem_usage(read_csv(test_file, num_rows))\n",
    "\n",
    "user_info = reduce_mem_usage(read_csv(user_info_file, num_rows))\n",
    "user_log = reduce_mem_usage(read_csv(user_log_file, num_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）查看压缩后的数据信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260864 entries, 0 to 260863\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype\n",
      "---  ------       --------------   -----\n",
      " 0   user_id      260864 non-null  int32\n",
      " 1   merchant_id  260864 non-null  int16\n",
      " 2   label        260864 non-null  int8 \n",
      "dtypes: int16(1), int32(1), int8(1)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 261477 entries, 0 to 261476\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   user_id      261477 non-null  int32  \n",
      " 1   merchant_id  261477 non-null  int16  \n",
      " 2   prob         0 non-null       float64\n",
      "dtypes: float64(1), int16(1), int32(1)\n",
      "memory usage: 3.5 MB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 424170 entries, 0 to 424169\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   user_id    424170 non-null  int32  \n",
      " 1   age_range  421953 non-null  float16\n",
      " 2   gender     417734 non-null  float16\n",
      "dtypes: float16(2), int32(1)\n",
      "memory usage: 3.2 MB\n"
     ]
    }
   ],
   "source": [
    "user_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000000 entries, 0 to 1999999\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   user_id      int32  \n",
      " 1   item_id      int32  \n",
      " 2   cat_id       int16  \n",
      " 3   seller_id    int16  \n",
      " 4   brand_id     float16\n",
      " 5   time_stamp   int16  \n",
      " 6   action_type  int8   \n",
      "dtypes: float16(1), int16(3), int32(2), int8(1)\n",
      "memory usage: 32.4 MB\n"
     ]
    }
   ],
   "source": [
    "user_log.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 数据处理\n",
    "\n",
    "### 4.1 合并用户信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 合并训练集、测试集、用户信息表\n",
    "del test_data['prob']\n",
    "all_data = train_data.append(test_data)\n",
    "all_data = all_data.merge(user_info,on=['user_id'],how='left')\n",
    "del train_data, test_data, user_info\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34176</td>\n",
       "      <td>3906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34176</td>\n",
       "      <td>121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34176</td>\n",
       "      <td>4356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34176</td>\n",
       "      <td>2217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230784</td>\n",
       "      <td>4818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  age_range  gender\n",
       "0    34176         3906    0.0        6.0     0.0\n",
       "1    34176          121    0.0        6.0     0.0\n",
       "2    34176         4356    1.0        6.0     0.0\n",
       "3    34176         2217    0.0        6.0     0.0\n",
       "4   230784         4818    0.0        0.0     0.0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码解释\n",
    "这段代码的主要目的是合并训练集、测试集和用户信息表，并将它们合并为一个新的数据集all_data。\n",
    "\n",
    "1. 首先，代码删除了测试集中名为'prob'的列，因为它在训练集中不存在。\n",
    "2. 然后，将训练集和测试集合并为一个新的数据集all_data。这里使用了`append()`方法将测试集添加到训练集的末尾。。\n",
    "3. 接着，使用`merge()`方法将用户信息表和合并后的数据集all_data合并，使用'on'参数指定连接的列，使用'how'参数指定连接方式为左连接（即保留左表中的所有行，即使右表中没有匹配的行）。\n",
    "4. 最后，删除了训练集、测试集和用户信息表，使用`del`关键字。同时，调用`gc.collect()`函数释放内存。\n",
    "\n",
    "- `del`关键字用于删除变量或对象。当在Python中使用`del`关键字时，它会从内存中删除变量或对象的引用，从而释放该对象所占用的内存空间。\n",
    "\n",
    "\n",
    "- `gc.collect()`函数是Python的垃圾回收器（Garbage Collector）的接口，用于手动触发垃圾回收操作。在某些情况下，Python可能会无法及时回收垃圾，这时可以使用`gc.collect()`函数手动触发垃圾回收，以释放被占用的内存空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 用户行为日志信息按时间进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "按时间排序\n",
    "\"\"\"\n",
    "user_log = user_log.sort_values(['user_id','time_stamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>action_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61975</th>\n",
       "      <td>16</td>\n",
       "      <td>980982</td>\n",
       "      <td>437</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61976</th>\n",
       "      <td>16</td>\n",
       "      <td>980982</td>\n",
       "      <td>437</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61977</th>\n",
       "      <td>16</td>\n",
       "      <td>980982</td>\n",
       "      <td>437</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61978</th>\n",
       "      <td>16</td>\n",
       "      <td>962763</td>\n",
       "      <td>19</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61979</th>\n",
       "      <td>16</td>\n",
       "      <td>391126</td>\n",
       "      <td>437</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  cat_id  seller_id  brand_id  time_stamp  action_type\n",
       "61975       16   980982     437        650    4276.0         914            0\n",
       "61976       16   980982     437        650    4276.0         914            0\n",
       "61977       16   980982     437        650    4276.0         914            0\n",
       "61978       16   962763      19        650    4276.0         914            0\n",
       "61979       16   391126     437        650    4276.0         914            0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码解释\n",
    "对user_id列和time_stamp列进行排序的具体步骤如下：\n",
    "\n",
    "1. 首先，使用`sort_values()`方法对user_id列进行升序排序。升序排序意味着从小到大排列，即按user_id的顺序排列。\n",
    "2. 然后，使用`sort_values()`方法对time_stamp列进行升序排序。升序排序意味着从小到大排列，即按时间戳的顺序排列。\n",
    "\n",
    "因此，经过这两步排序后，user_log数据框中的数据将按照用户ID和时间戳的顺序进行排列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 对每个用户逐个合并所有的字段\n",
    "合并字段为item_id, cat_id,seller_id,brand_id,time_stamp, action_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "合并数据\n",
    "\"\"\"\n",
    "list_join_func = lambda x: \" \".join([str(i) for i in x])\n",
    "\n",
    "\n",
    "agg_dict = {\n",
    "            'item_id' : list_join_func,\t\n",
    "            'cat_id' : list_join_func,\n",
    "            'seller_id' : list_join_func,\n",
    "            'brand_id' : list_join_func,\n",
    "            'time_stamp' : list_join_func,\n",
    "            'action_type' : list_join_func\n",
    "        }\n",
    "\n",
    "rename_dict = {\n",
    "            'item_id' : 'item_path',\n",
    "            'cat_id' : 'cat_path',\n",
    "            'seller_id' : 'seller_path',\n",
    "            'brand_id' : 'brand_path',\n",
    "            'time_stamp' : 'time_stamp_path',\n",
    "            'action_type' : 'action_type_path'\n",
    "        }\n",
    "\n",
    "# def merge_list(df_ID, join_columns, df_data, agg_dict, rename_dict):\n",
    "#     # 对df_data按照join_columns进行分组，并使用agg_dict进行聚合操作，然后重命名列\n",
    "#     df_data = df_data.\\\n",
    "#                 groupby(join_columns).\\\n",
    "#                 agg(agg_dict).\\\n",
    "#                 reset_index().\\\n",
    "#                 rename(columns=rename_dict)\n",
    "    \n",
    "#     # 将df_ID和df_data按照join_columns进行左连接\n",
    "#     df_ID = df_ID.merge(df_data, on=join_columns, how=\"left\") \n",
    "#     return df_data,df_ID\n",
    "# all_data = merge_list(all_data, 'user_id', user_log, agg_dict, rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码解释\n",
    "这段代码的主要目的是合并数据，将多个列组合成一个字符串，并将结果合并到现有的数据框中。实现原理如下：\n",
    "\n",
    "1. 使用lambda函数定义一个名为`list_join_func`的函数，该函数接受一个列表参数，并将列表中的元素以空格分隔拼接成一个字符串。\n",
    "\n",
    "2. 定义一个名为`agg_dict`的字典，该字典包含要进行聚合操作的列及其对应的聚合函数。例如，`'item_id' : list_join_func`表示要将`item_id`列的值拼接成一个字符串。\n",
    "\n",
    "3. 定义一个名为`rename_dict`的字典，该字典包含要重命名的列。例如，`'item_id' : 'item_path'`表示要将`item_id`列重命名为`item_path`。\n",
    "\n",
    "4. 定义一个名为`merge_list`的函数，该函数接受四个参数：`df_ID`、`join_columns`、`df_data`和`agg_dict`。函数首先对`df_data`按照`join_columns`进行分组，并使用`agg_dict`进行聚合操作。然后，将结果重命名列，并返回重命名后的数据框。\n",
    "\n",
    "5. 在主程序中，首先对`all_data`和`user_log`按照`user_id`进行左连接。然后，将合并后的数据帧传递给`merge_list`函数，并将结果更新到`all_data`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>action_type_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>980982 980982 980982 962763 391126 827174 6731...</td>\n",
       "      <td>437 437 437 19 437 437 437 437 895 19 437 437 ...</td>\n",
       "      <td>650 650 650 650 650 650 650 650 3948 650 650 6...</td>\n",
       "      <td>4276.0 4276.0 4276.0 4276.0 4276.0 4276.0 4276...</td>\n",
       "      <td>914 914 914 914 914 914 914 914 914 914 914 91...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>388018 388018 88673 88673 88673 88673 846066 5...</td>\n",
       "      <td>949 949 614 614 614 614 420 1401 948 948 513 1...</td>\n",
       "      <td>2772 2772 4066 4066 4066 4066 4951 4951 2872 2...</td>\n",
       "      <td>2112.0 2112.0 1552.0 1552.0 1552.0 1552.0 5200...</td>\n",
       "      <td>710 710 711 711 711 711 908 908 1105 1105 1105...</td>\n",
       "      <td>0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>60215 1004605 60215 60215 60215 60215 628525 5...</td>\n",
       "      <td>1308 1308 1308 1308 1308 1308 1271 656 656 656...</td>\n",
       "      <td>2128 3207 2128 2128 2128 2128 3142 4618 4618 4...</td>\n",
       "      <td>3848.0 3848.0 3848.0 3848.0 3848.0 3848.0 1014...</td>\n",
       "      <td>521 521 521 521 521 522 529 828 828 828 828 82...</td>\n",
       "      <td>0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>889499 528459 765746 553259 889499 22435 40047...</td>\n",
       "      <td>662 1075 662 1577 662 11 184 1604 11 11 177 11...</td>\n",
       "      <td>4048 601 3104 3828 4048 4766 2419 2768 2565 26...</td>\n",
       "      <td>5360.0 1040.0 8240.0 1446.0 5360.0 4360.0 3428...</td>\n",
       "      <td>517 520 525 528 602 602 610 610 610 610 610 61...</td>\n",
       "      <td>3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155</td>\n",
       "      <td>979639 890128 981780 211366 211366 797946 4567...</td>\n",
       "      <td>267 1271 1505 267 267 1075 1075 407 407 1075 4...</td>\n",
       "      <td>2429 4785 3784 800 800 1595 1418 2662 2662 315...</td>\n",
       "      <td>2276.0 1422.0 5692.0 6328.0 6328.0 5800.0 7140...</td>\n",
       "      <td>529 529 602 604 604 607 607 607 607 607 607 60...</td>\n",
       "      <td>0 0 0 2 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 2 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                          item_path  \\\n",
       "0       16  980982 980982 980982 962763 391126 827174 6731...   \n",
       "1       19  388018 388018 88673 88673 88673 88673 846066 5...   \n",
       "2       41  60215 1004605 60215 60215 60215 60215 628525 5...   \n",
       "3       56  889499 528459 765746 553259 889499 22435 40047...   \n",
       "4      155  979639 890128 981780 211366 211366 797946 4567...   \n",
       "\n",
       "                                            cat_path  \\\n",
       "0  437 437 437 19 437 437 437 437 895 19 437 437 ...   \n",
       "1  949 949 614 614 614 614 420 1401 948 948 513 1...   \n",
       "2  1308 1308 1308 1308 1308 1308 1271 656 656 656...   \n",
       "3  662 1075 662 1577 662 11 184 1604 11 11 177 11...   \n",
       "4  267 1271 1505 267 267 1075 1075 407 407 1075 4...   \n",
       "\n",
       "                                         seller_path  \\\n",
       "0  650 650 650 650 650 650 650 650 3948 650 650 6...   \n",
       "1  2772 2772 4066 4066 4066 4066 4951 4951 2872 2...   \n",
       "2  2128 3207 2128 2128 2128 2128 3142 4618 4618 4...   \n",
       "3  4048 601 3104 3828 4048 4766 2419 2768 2565 26...   \n",
       "4  2429 4785 3784 800 800 1595 1418 2662 2662 315...   \n",
       "\n",
       "                                          brand_path  \\\n",
       "0  4276.0 4276.0 4276.0 4276.0 4276.0 4276.0 4276...   \n",
       "1  2112.0 2112.0 1552.0 1552.0 1552.0 1552.0 5200...   \n",
       "2  3848.0 3848.0 3848.0 3848.0 3848.0 3848.0 1014...   \n",
       "3  5360.0 1040.0 8240.0 1446.0 5360.0 4360.0 3428...   \n",
       "4  2276.0 1422.0 5692.0 6328.0 6328.0 5800.0 7140...   \n",
       "\n",
       "                                     time_stamp_path  \\\n",
       "0  914 914 914 914 914 914 914 914 914 914 914 91...   \n",
       "1  710 710 711 711 711 711 908 908 1105 1105 1105...   \n",
       "2  521 521 521 521 521 522 529 828 828 828 828 82...   \n",
       "3  517 520 525 528 602 602 610 610 610 610 610 61...   \n",
       "4  529 529 602 604 604 607 607 607 607 607 607 60...   \n",
       "\n",
       "                                    action_type_path  \n",
       "0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 0 ...  \n",
       "1  0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "2  0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ...  \n",
       "3  3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ...  \n",
       "4  0 0 0 2 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 2 ...  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log_path = user_log.groupby('user_id').agg(agg_dict).reset_index().rename(columns=rename_dict)\n",
    "user_log_path.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>action_type_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>986160 681407 681407 910680 681407 592698 3693...</td>\n",
       "      <td>35 1554 1554 119 1554 662 1095 662 35 833 833 ...</td>\n",
       "      <td>4811 4811 4811 1897 4811 3315 2925 1340 1875 4...</td>\n",
       "      <td>127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...</td>\n",
       "      <td>518 518 518 520 520 524 524 524 525 525 525 52...</td>\n",
       "      <td>2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396970 961553 627712 926681 1012423 825576 149...</td>\n",
       "      <td>1023 420 407 1505 962 602 184 1606 351 1505 11...</td>\n",
       "      <td>1435 1648 223 3178 2418 1614 3004 2511 2285 78...</td>\n",
       "      <td>5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...</td>\n",
       "      <td>517 520 522 522 527 530 530 530 601 601 602 60...</td>\n",
       "      <td>2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400</td>\n",
       "      <td>302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256546 202393 927572 2587 10956 549283 270303 ...</td>\n",
       "      <td>1188 646 1175 1188 1414 681 1175 681 681 115 1...</td>\n",
       "      <td>805 390 4252 3979 1228 2029 2029 2029 4252 923...</td>\n",
       "      <td>1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...</td>\n",
       "      <td>517 604 604 604 607 609 609 609 609 615 621 62...</td>\n",
       "      <td>2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  age_range  gender  \\\n",
       "0   105600         1487    0.0        6.0     1.0   \n",
       "1   110976          159    0.0        5.0     0.0   \n",
       "2   374400          302    0.0        5.0     1.0   \n",
       "3   189312         1760    0.0        4.0     0.0   \n",
       "4   189312         2511    0.0        4.0     0.0   \n",
       "\n",
       "                                           item_path  \\\n",
       "0  986160 681407 681407 910680 681407 592698 3693...   \n",
       "1  396970 961553 627712 926681 1012423 825576 149...   \n",
       "2  256546 202393 927572 2587 10956 549283 270303 ...   \n",
       "3  290583 166235 556025 217894 166235 556025 5589...   \n",
       "4  290583 166235 556025 217894 166235 556025 5589...   \n",
       "\n",
       "                                            cat_path  \\\n",
       "0  35 1554 1554 119 1554 662 1095 662 35 833 833 ...   \n",
       "1  1023 420 407 1505 962 602 184 1606 351 1505 11...   \n",
       "2  1188 646 1175 1188 1414 681 1175 681 681 115 1...   \n",
       "3  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "4  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "\n",
       "                                         seller_path  \\\n",
       "0  4811 4811 4811 1897 4811 3315 2925 1340 1875 4...   \n",
       "1  1435 1648 223 3178 2418 1614 3004 2511 2285 78...   \n",
       "2  805 390 4252 3979 1228 2029 2029 2029 4252 923...   \n",
       "3  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "4  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "\n",
       "                                          brand_path  \\\n",
       "0  127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...   \n",
       "1  5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...   \n",
       "2  1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...   \n",
       "3  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "4  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "\n",
       "                                     time_stamp_path  \\\n",
       "0  518 518 518 520 520 524 524 524 525 525 525 52...   \n",
       "1  517 520 522 522 527 530 530 530 601 601 602 60...   \n",
       "2  517 604 604 604 607 609 609 609 609 615 621 62...   \n",
       "3  924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "4  924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "\n",
       "                                    action_type_path  \n",
       "0  2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "1  2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...  \n",
       "2  2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "3  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "4  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_path = all_data.merge(user_log_path,on='user_id')\n",
    "all_data_path.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 删除数据并回收内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "删除不需要的数据\n",
    "\"\"\"\n",
    "del user_log\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 定义数据统计函数\n",
    "\n",
    "### 5.1 定义统计函数\n",
    "**(1)定义统计数据总数的函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnt_(x):\n",
    "    try:\n",
    "        return len(x.split(' '))\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)定义统计数据唯一值总数的函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nunique_(x):\n",
    "    try:\n",
    "        return len(set(x.split(' ')))\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)定义统计数据最大值的函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_(x):\n",
    "    try:\n",
    "        return np.max([int(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)定义统计数据最小值的函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_(x):\n",
    "    try:\n",
    "        return np.min([int(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)定义统计数据标准差的函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_(x):\n",
    "    try:\n",
    "        return np.std([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(6)定义统计数据中topN数据的函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_n(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][0]\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(7)定义统计数据中topN数据总数的函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_n_cnt(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][1]\n",
    "    except:\n",
    "        return -1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 调用定义的统计函数\n",
    "调用数据集的特征统计函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def user_cnt(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(cnt_)\n",
    "    return df_data\n",
    "\n",
    "def user_nunique(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(nunique_)\n",
    "    return df_data\n",
    "    \n",
    "def user_max(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(max_)\n",
    "    return df_data\n",
    "\n",
    "def user_min(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(min_)\n",
    "    return df_data\n",
    "    \n",
    "def user_std(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(std_)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n_cnt(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n_cnt(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 提取统计特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 特征统计\n",
    "#### (1)店铺特征统计\n",
    "统计与店铺特点有关的特征，如店铺、商品、品牌等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    提取基本统计特征\n",
    "\"\"\"\n",
    "#all_data_test = all_data\n",
    "all_data_test = all_data_path\n",
    "# 统计用户 点击、浏览、加购、购买行为\n",
    "# 总次数\n",
    "all_data_test = user_cnt(all_data_test,  'seller_path', 'user_cnt')\n",
    "# 不同店铺个数\n",
    "all_data_test = user_nunique(all_data_test,  'seller_path', 'seller_nunique')\n",
    "# 不同品类个数\n",
    "all_data_test = user_nunique(all_data_test,  'cat_path', 'cat_nunique')\n",
    "# 不同品牌个数\n",
    "all_data_test = user_nunique(all_data_test,  'brand_path', 'brand_nunique')\n",
    "# 不同商品个数\n",
    "all_data_test = user_nunique(all_data_test,  'item_path', 'item_nunique')\n",
    "# 活跃天数\n",
    "all_data_test = user_nunique(all_data_test,  'time_stamp_path', 'time_stamp_nunique')\n",
    "# 不用行为种数\n",
    "all_data_test = user_nunique(all_data_test,  'action_type_path', 'action_type_nunique')\n",
    "# ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>action_type_path</th>\n",
       "      <th>user_cnt</th>\n",
       "      <th>seller_nunique</th>\n",
       "      <th>cat_nunique</th>\n",
       "      <th>brand_nunique</th>\n",
       "      <th>item_nunique</th>\n",
       "      <th>time_stamp_nunique</th>\n",
       "      <th>action_type_nunique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>986160 681407 681407 910680 681407 592698 3693...</td>\n",
       "      <td>35 1554 1554 119 1554 662 1095 662 35 833 833 ...</td>\n",
       "      <td>4811 4811 4811 1897 4811 3315 2925 1340 1875 4...</td>\n",
       "      <td>127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...</td>\n",
       "      <td>518 518 518 520 520 524 524 524 525 525 525 52...</td>\n",
       "      <td>2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>310</td>\n",
       "      <td>96</td>\n",
       "      <td>37</td>\n",
       "      <td>88</td>\n",
       "      <td>217</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396970 961553 627712 926681 1012423 825576 149...</td>\n",
       "      <td>1023 420 407 1505 962 602 184 1606 351 1505 11...</td>\n",
       "      <td>1435 1648 223 3178 2418 1614 3004 2511 2285 78...</td>\n",
       "      <td>5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...</td>\n",
       "      <td>517 520 522 522 527 530 530 530 601 601 602 60...</td>\n",
       "      <td>2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...</td>\n",
       "      <td>274</td>\n",
       "      <td>181</td>\n",
       "      <td>70</td>\n",
       "      <td>159</td>\n",
       "      <td>233</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400</td>\n",
       "      <td>302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256546 202393 927572 2587 10956 549283 270303 ...</td>\n",
       "      <td>1188 646 1175 1188 1414 681 1175 681 681 115 1...</td>\n",
       "      <td>805 390 4252 3979 1228 2029 2029 2029 4252 923...</td>\n",
       "      <td>1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...</td>\n",
       "      <td>517 604 604 604 607 609 609 609 609 615 621 62...</td>\n",
       "      <td>2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>278</td>\n",
       "      <td>57</td>\n",
       "      <td>59</td>\n",
       "      <td>62</td>\n",
       "      <td>148</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>237</td>\n",
       "      <td>49</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>170</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>237</td>\n",
       "      <td>49</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>170</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  age_range  gender  \\\n",
       "0   105600         1487    0.0        6.0     1.0   \n",
       "1   110976          159    0.0        5.0     0.0   \n",
       "2   374400          302    0.0        5.0     1.0   \n",
       "3   189312         1760    0.0        4.0     0.0   \n",
       "4   189312         2511    0.0        4.0     0.0   \n",
       "\n",
       "                                           item_path  \\\n",
       "0  986160 681407 681407 910680 681407 592698 3693...   \n",
       "1  396970 961553 627712 926681 1012423 825576 149...   \n",
       "2  256546 202393 927572 2587 10956 549283 270303 ...   \n",
       "3  290583 166235 556025 217894 166235 556025 5589...   \n",
       "4  290583 166235 556025 217894 166235 556025 5589...   \n",
       "\n",
       "                                            cat_path  \\\n",
       "0  35 1554 1554 119 1554 662 1095 662 35 833 833 ...   \n",
       "1  1023 420 407 1505 962 602 184 1606 351 1505 11...   \n",
       "2  1188 646 1175 1188 1414 681 1175 681 681 115 1...   \n",
       "3  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "4  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "\n",
       "                                         seller_path  \\\n",
       "0  4811 4811 4811 1897 4811 3315 2925 1340 1875 4...   \n",
       "1  1435 1648 223 3178 2418 1614 3004 2511 2285 78...   \n",
       "2  805 390 4252 3979 1228 2029 2029 2029 4252 923...   \n",
       "3  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "4  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "\n",
       "                                          brand_path  \\\n",
       "0  127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...   \n",
       "1  5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...   \n",
       "2  1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...   \n",
       "3  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "4  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "\n",
       "                                     time_stamp_path  \\\n",
       "0  518 518 518 520 520 524 524 524 525 525 525 52...   \n",
       "1  517 520 522 522 527 530 530 530 601 601 602 60...   \n",
       "2  517 604 604 604 607 609 609 609 609 615 621 62...   \n",
       "3  924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "4  924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "\n",
       "                                    action_type_path  user_cnt  \\\n",
       "0  2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...       310   \n",
       "1  2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...       274   \n",
       "2  2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...       278   \n",
       "3  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...       237   \n",
       "4  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...       237   \n",
       "\n",
       "   seller_nunique  cat_nunique  brand_nunique  item_nunique  \\\n",
       "0              96           37             88           217   \n",
       "1             181           70            159           233   \n",
       "2              57           59             62           148   \n",
       "3              49           35             45           170   \n",
       "4              49           35             45           170   \n",
       "\n",
       "   time_stamp_nunique  action_type_nunique  \n",
       "0                  29                    2  \n",
       "1                  52                    3  \n",
       "2                  35                    3  \n",
       "3                   9                    2  \n",
       "4                   9                    2  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最晚时间\n",
    "all_data_test = user_max(all_data_test,  'action_type_path', 'time_stamp_max')\n",
    "# 最早时间\n",
    "all_data_test = user_min(all_data_test,  'action_type_path', 'time_stamp_min')\n",
    "# 活跃天数方差\n",
    "all_data_test = user_std(all_data_test,  'action_type_path', 'time_stamp_std')\n",
    "# 最早和最晚相差天数\n",
    "all_data_test['time_stamp_range'] = all_data_test['time_stamp_max'] - all_data_test['time_stamp_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户最喜欢的店铺\n",
    "all_data_test = user_most_n(all_data_test, 'seller_path', 'seller_most_1', n=1)\n",
    "# 最喜欢的类目\n",
    "all_data_test = user_most_n(all_data_test, 'cat_path', 'cat_most_1', n=1)\n",
    "# 最喜欢的品牌\n",
    "all_data_test = user_most_n(all_data_test, 'brand_path', 'brand_most_1', n=1)\n",
    "# 最常见的行为动作\n",
    "all_data_test = user_most_n(all_data_test, 'action_type_path', 'action_type_1', n=1)\n",
    "# ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户最喜欢的店铺 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'seller_path', 'seller_most_1_cnt', n=1)\n",
    "# 最喜欢的类目 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'cat_path', 'cat_most_1_cnt', n=1)\n",
    "# 最喜欢的品牌 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'brand_path', 'brand_most_1_cnt', n=1)\n",
    "# 最常见的行为动作 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'action_type_path', 'action_type_1_cnt', n=1)\n",
    "# ....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2)用户特征统计\n",
    "对用户的点击、加购、购买、收藏等特征进行统计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对点击、加购、购买、收藏 分开统计\n",
    "\"\"\"\n",
    "统计基本特征函数  \n",
    "-- 知识点二\n",
    "-- 根据不同行为的业务函数\n",
    "-- 提取不同特征\n",
    "\"\"\"\n",
    "def col_cnt_(df_data, columns_list, action_type):\n",
    "    # 定义一个名为col_cnt_的函数，参数分别为df_data（数据框），columns_list（列名列表），action_type（行为类型）\n",
    "    try:\n",
    "        # 定义一个名为data_dict的字典\n",
    "        data_dict = {}\n",
    "\n",
    "        # 将columns_list复制一份，命名为col_list\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        # 如果action_type不为空，将action_type_path添加到col_list中\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "\n",
    "        # 将df_data中的每一列按照空格分割，存入data_dict中\n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        # 获取data_dict中每一列的长度\n",
    "        path_len = len(data_dict[col])\n",
    "\n",
    "        # 定义一个名为data_out的空列表\n",
    "        data_out = []\n",
    "        # 遍历data_dict中每一列的长度\n",
    "        for i_ in range(path_len):\n",
    "            # 定义一个名为data_txt的空字符串\n",
    "            data_txt = ''\n",
    "            # 遍历columns_list中的每一列\n",
    "            for col_ in columns_list:\n",
    "                # 如果action_type_path中当前行的值为action_type，将data_dict中当前列的值添加到data_txt中\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            # 将data_txt添加到data_out中\n",
    "            data_out.append(data_txt)\n",
    "\n",
    "        # 返回data_out的长度\n",
    "        return len(data_out)  \n",
    "    except:\n",
    "        # 如果发生异常，返回-1\n",
    "        return -1\n",
    "\n",
    "def col_nuique_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "\n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col])\n",
    "\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "\n",
    "        return len(set(data_out))\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "\n",
    "def user_col_cnt(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_cnt_(x, columns_list, action_type), axis=1)\n",
    "    return df_data\n",
    "\n",
    "def user_col_nunique(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_nuique_(x, columns_list, action_type), axis=1)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 代码解释\n",
    "这段代码用于对用户在购物网站上的点击、加购、购买和收藏行为进行统计。实现原理是使用DataFrame的apply方法对数据进行处理，根据不同的行为类型和业务逻辑对数据进行提取和统计。\n",
    "\n",
    "功能：\n",
    "\n",
    "1. `col_cnt_`函数：统计指定列（columns_list）中每个路径的重复次数。\n",
    "2. `col_nuique_`函数：统计指定列（columns_list）中每个路径去重后的次数。\n",
    "3. `user_col_cnt`函数：将统计结果存储在DataFrame的指定列（name）中。\n",
    "4. `user_col_nunique`函数：将统计结果存储在DataFrame的指定列（name）中。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3)统计用户和店铺的关系\n",
    "统计店铺被用户点击次数，加购次数，购买次数，收藏次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '0', 'user_cnt_0')\n",
    "# 加购次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '1', 'user_cnt_1')\n",
    "# 购买次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '2', 'user_cnt_2')\n",
    "# 收藏次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '3', 'user_cnt_3')\n",
    "\n",
    "\n",
    "# 不同店铺个数\n",
    "all_data_test = user_col_nunique(all_data_test,  ['seller_path'], '0', 'seller_nunique_0')\n",
    "# ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>...</th>\n",
       "      <th>action_type_1</th>\n",
       "      <th>seller_most_1_cnt</th>\n",
       "      <th>cat_most_1_cnt</th>\n",
       "      <th>brand_most_1_cnt</th>\n",
       "      <th>action_type_1_cnt</th>\n",
       "      <th>user_cnt_0</th>\n",
       "      <th>user_cnt_1</th>\n",
       "      <th>user_cnt_2</th>\n",
       "      <th>user_cnt_3</th>\n",
       "      <th>seller_nunique_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>986160 681407 681407 910680 681407 592698 3693...</td>\n",
       "      <td>35 1554 1554 119 1554 662 1095 662 35 833 833 ...</td>\n",
       "      <td>4811 4811 4811 1897 4811 3315 2925 1340 1875 4...</td>\n",
       "      <td>127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...</td>\n",
       "      <td>518 518 518 520 520 524 524 524 525 525 525 52...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>43</td>\n",
       "      <td>35</td>\n",
       "      <td>299</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396970 961553 627712 926681 1012423 825576 149...</td>\n",
       "      <td>1023 420 407 1505 962 602 184 1606 351 1505 11...</td>\n",
       "      <td>1435 1648 223 3178 2418 1614 3004 2511 2285 78...</td>\n",
       "      <td>5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...</td>\n",
       "      <td>517 520 522 522 527 530 530 530 601 601 602 60...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>259</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400</td>\n",
       "      <td>302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256546 202393 927572 2587 10956 549283 270303 ...</td>\n",
       "      <td>1188 646 1175 1188 1414 681 1175 681 681 115 1...</td>\n",
       "      <td>805 390 4252 3979 1228 2029 2029 2029 4252 923...</td>\n",
       "      <td>1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...</td>\n",
       "      <td>517 604 604 604 607 609 609 609 609 615 621 62...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>29</td>\n",
       "      <td>48</td>\n",
       "      <td>241</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>228</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>228</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16859</th>\n",
       "      <td>120191</td>\n",
       "      <td>1899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>793882 288225 288225 288225 195714 195714 1957...</td>\n",
       "      <td>387 35 35 35 1213 1213 1213 1213 1075 447 1213...</td>\n",
       "      <td>1146 696 696 696 1200 1200 1200 1200 2702 4279...</td>\n",
       "      <td>8064.0 3600.0 3600.0 3600.0 2276.0 2276.0 2276...</td>\n",
       "      <td>512 516 516 516 606 606 606 606 606 606 606 60...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>69</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16860</th>\n",
       "      <td>121727</td>\n",
       "      <td>4044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>544029 562170 544029 562170 544029 544029 5621...</td>\n",
       "      <td>1505 662 1505 662 1505 1505 662 1505 1505 1505...</td>\n",
       "      <td>795 1910 795 1910 795 795 1910 795 795 795 411...</td>\n",
       "      <td>3608.0 950.0 3608.0 950.0 3608.0 3608.0 950.0 ...</td>\n",
       "      <td>628 628 628 628 628 628 628 628 628 628 710 71...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16861</th>\n",
       "      <td>385919</td>\n",
       "      <td>3912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187936 187936 657875 969054 462255 985073 1602...</td>\n",
       "      <td>602 602 1389 1505 1228 1604 1228 662 1228 662 ...</td>\n",
       "      <td>661 661 643 643 4738 643 3740 643 4738 643 643...</td>\n",
       "      <td>1484.0 1484.0 968.0 968.0 6220.0 968.0 4072.0 ...</td>\n",
       "      <td>512 512 513 513 524 524 524 524 524 524 526 60...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>44</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16862</th>\n",
       "      <td>215423</td>\n",
       "      <td>4356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>885364 938282 966141 174392 885364 821661 3473...</td>\n",
       "      <td>1389 662 1095 1095 1389 662 1577 821 662 1389 ...</td>\n",
       "      <td>2602 2602 2602 2602 2602 2602 3128 2602 2602 2...</td>\n",
       "      <td>1900.0 1900.0 1900.0 1900.0 1900.0 1900.0 8392...</td>\n",
       "      <td>521 521 521 521 521 521 521 521 521 521 521 52...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>58</td>\n",
       "      <td>63</td>\n",
       "      <td>118</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16863</th>\n",
       "      <td>215423</td>\n",
       "      <td>1840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>885364 938282 966141 174392 885364 821661 3473...</td>\n",
       "      <td>1389 662 1095 1095 1389 662 1577 821 662 1389 ...</td>\n",
       "      <td>2602 2602 2602 2602 2602 2602 3128 2602 2602 2...</td>\n",
       "      <td>1900.0 1900.0 1900.0 1900.0 1900.0 1900.0 8392...</td>\n",
       "      <td>521 521 521 521 521 521 521 521 521 521 521 52...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>58</td>\n",
       "      <td>63</td>\n",
       "      <td>118</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16864 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  merchant_id  label  age_range  gender  \\\n",
       "0       105600         1487    0.0        6.0     1.0   \n",
       "1       110976          159    0.0        5.0     0.0   \n",
       "2       374400          302    0.0        5.0     1.0   \n",
       "3       189312         1760    0.0        4.0     0.0   \n",
       "4       189312         2511    0.0        4.0     0.0   \n",
       "...        ...          ...    ...        ...     ...   \n",
       "16859   120191         1899    NaN        4.0     0.0   \n",
       "16860   121727         4044    NaN        4.0     0.0   \n",
       "16861   385919         3912    NaN        0.0     0.0   \n",
       "16862   215423         4356    NaN        5.0     0.0   \n",
       "16863   215423         1840    NaN        5.0     0.0   \n",
       "\n",
       "                                               item_path  \\\n",
       "0      986160 681407 681407 910680 681407 592698 3693...   \n",
       "1      396970 961553 627712 926681 1012423 825576 149...   \n",
       "2      256546 202393 927572 2587 10956 549283 270303 ...   \n",
       "3      290583 166235 556025 217894 166235 556025 5589...   \n",
       "4      290583 166235 556025 217894 166235 556025 5589...   \n",
       "...                                                  ...   \n",
       "16859  793882 288225 288225 288225 195714 195714 1957...   \n",
       "16860  544029 562170 544029 562170 544029 544029 5621...   \n",
       "16861  187936 187936 657875 969054 462255 985073 1602...   \n",
       "16862  885364 938282 966141 174392 885364 821661 3473...   \n",
       "16863  885364 938282 966141 174392 885364 821661 3473...   \n",
       "\n",
       "                                                cat_path  \\\n",
       "0      35 1554 1554 119 1554 662 1095 662 35 833 833 ...   \n",
       "1      1023 420 407 1505 962 602 184 1606 351 1505 11...   \n",
       "2      1188 646 1175 1188 1414 681 1175 681 681 115 1...   \n",
       "3      601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "4      601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "...                                                  ...   \n",
       "16859  387 35 35 35 1213 1213 1213 1213 1075 447 1213...   \n",
       "16860  1505 662 1505 662 1505 1505 662 1505 1505 1505...   \n",
       "16861  602 602 1389 1505 1228 1604 1228 662 1228 662 ...   \n",
       "16862  1389 662 1095 1095 1389 662 1577 821 662 1389 ...   \n",
       "16863  1389 662 1095 1095 1389 662 1577 821 662 1389 ...   \n",
       "\n",
       "                                             seller_path  \\\n",
       "0      4811 4811 4811 1897 4811 3315 2925 1340 1875 4...   \n",
       "1      1435 1648 223 3178 2418 1614 3004 2511 2285 78...   \n",
       "2      805 390 4252 3979 1228 2029 2029 2029 4252 923...   \n",
       "3      3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "4      3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "...                                                  ...   \n",
       "16859  1146 696 696 696 1200 1200 1200 1200 2702 4279...   \n",
       "16860  795 1910 795 1910 795 795 1910 795 795 795 411...   \n",
       "16861  661 661 643 643 4738 643 3740 643 4738 643 643...   \n",
       "16862  2602 2602 2602 2602 2602 2602 3128 2602 2602 2...   \n",
       "16863  2602 2602 2602 2602 2602 2602 3128 2602 2602 2...   \n",
       "\n",
       "                                              brand_path  \\\n",
       "0      127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...   \n",
       "1      5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...   \n",
       "2      1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...   \n",
       "3      549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "4      549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "...                                                  ...   \n",
       "16859  8064.0 3600.0 3600.0 3600.0 2276.0 2276.0 2276...   \n",
       "16860  3608.0 950.0 3608.0 950.0 3608.0 3608.0 950.0 ...   \n",
       "16861  1484.0 1484.0 968.0 968.0 6220.0 968.0 4072.0 ...   \n",
       "16862  1900.0 1900.0 1900.0 1900.0 1900.0 1900.0 8392...   \n",
       "16863  1900.0 1900.0 1900.0 1900.0 1900.0 1900.0 8392...   \n",
       "\n",
       "                                         time_stamp_path  ... action_type_1  \\\n",
       "0      518 518 518 520 520 524 524 524 525 525 525 52...  ...             0   \n",
       "1      517 520 522 522 527 530 530 530 601 601 602 60...  ...             0   \n",
       "2      517 604 604 604 607 609 609 609 609 615 621 62...  ...             0   \n",
       "3      924 924 924 924 924 924 924 924 924 924 924 92...  ...             0   \n",
       "4      924 924 924 924 924 924 924 924 924 924 924 92...  ...             0   \n",
       "...                                                  ...  ...           ...   \n",
       "16859  512 516 516 516 606 606 606 606 606 606 606 60...  ...             0   \n",
       "16860  628 628 628 628 628 628 628 628 628 628 710 71...  ...             0   \n",
       "16861  512 512 513 513 524 524 524 524 524 524 526 60...  ...             0   \n",
       "16862  521 521 521 521 521 521 521 521 521 521 521 52...  ...             0   \n",
       "16863  521 521 521 521 521 521 521 521 521 521 521 52...  ...             0   \n",
       "\n",
       "       seller_most_1_cnt  cat_most_1_cnt  brand_most_1_cnt  action_type_1_cnt  \\\n",
       "0                     35              43                35                299   \n",
       "1                      9              56                11                259   \n",
       "2                     93              29                48                241   \n",
       "3                     45              68                45                228   \n",
       "4                     45              68                45                228   \n",
       "...                  ...             ...               ...                ...   \n",
       "16859                 11              14                11                 69   \n",
       "16860                 12              12                12                 43   \n",
       "16861                 33              19                33                 44   \n",
       "16862                 63              58                63                118   \n",
       "16863                 63              58                63                118   \n",
       "\n",
       "       user_cnt_0  user_cnt_1  user_cnt_2  user_cnt_3  seller_nunique_0  \n",
       "0             310         310         310         310                97  \n",
       "1             274         274         274         274               181  \n",
       "2             278         278         278         278                56  \n",
       "3             237         237         237         237                50  \n",
       "4             237         237         237         237                50  \n",
       "...           ...         ...         ...         ...               ...  \n",
       "16859          96          96          96          96                29  \n",
       "16860          49          49          49          49                15  \n",
       "16861          54          54          54          54                12  \n",
       "16862         152         152         152         152                25  \n",
       "16863         152         152         152         152                25  \n",
       "\n",
       "[16864 rows x 35 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 特征组合\n",
    "\n",
    "#### （1）特征组合进行业务特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path', 'item_path'], '0', 'user_cnt_0')\n",
    "\n",
    "# 不同店铺个数\n",
    "all_data_test = user_col_nunique(all_data_test,  ['seller_path', 'item_path'], '0', 'seller_nunique_0')\n",
    "# ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （2）查看提取的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'merchant_id', 'label', 'age_range', 'gender', 'item_path',\n",
       "       'cat_path', 'seller_path', 'brand_path', 'time_stamp_path',\n",
       "       'action_type_path', 'user_cnt', 'seller_nunique', 'cat_nunique',\n",
       "       'brand_nunique', 'item_nunique', 'time_stamp_nunique',\n",
       "       'action_type_nunique', 'time_stamp_max', 'time_stamp_min',\n",
       "       'time_stamp_std', 'time_stamp_range', 'seller_most_1', 'cat_most_1',\n",
       "       'brand_most_1', 'action_type_1', 'seller_most_1_cnt', 'cat_most_1_cnt',\n",
       "       'brand_most_1_cnt', 'action_type_1_cnt', 'user_cnt_0', 'user_cnt_1',\n",
       "       'user_cnt_2', 'user_cnt_3', 'seller_nunique_0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'merchant_id',\n",
       " 'label',\n",
       " 'age_range',\n",
       " 'gender',\n",
       " 'item_path',\n",
       " 'cat_path',\n",
       " 'seller_path',\n",
       " 'brand_path',\n",
       " 'time_stamp_path',\n",
       " 'action_type_path',\n",
       " 'user_cnt',\n",
       " 'seller_nunique',\n",
       " 'cat_nunique',\n",
       " 'brand_nunique',\n",
       " 'item_nunique',\n",
       " 'time_stamp_nunique',\n",
       " 'action_type_nunique',\n",
       " 'time_stamp_max',\n",
       " 'time_stamp_min',\n",
       " 'time_stamp_std',\n",
       " 'time_stamp_range',\n",
       " 'seller_most_1',\n",
       " 'cat_most_1',\n",
       " 'brand_most_1',\n",
       " 'action_type_1',\n",
       " 'seller_most_1_cnt',\n",
       " 'cat_most_1_cnt',\n",
       " 'brand_most_1_cnt',\n",
       " 'action_type_1_cnt',\n",
       " 'user_cnt_0',\n",
       " 'user_cnt_1',\n",
       " 'user_cnt_2',\n",
       " 'user_cnt_3',\n",
       " 'seller_nunique_0']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(all_data_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 利用Countvector，TF-IDF提取特征\n",
    "(1)利用Countvector和TF-IDF提取特征，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 知识点四\n",
    "-- 利用countvector，tfidf提取特征\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from scipy import sparse\n",
    "# cntVec = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 1), max_features=100)\n",
    "tfidfVec = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 1), max_features=100)\n",
    "\n",
    "\n",
    "# columns_list = ['seller_path', 'cat_path', 'brand_path', 'action_type_path', 'item_path', 'time_stamp_path']\n",
    "columns_list = ['seller_path']\n",
    "for i, col in enumerate(columns_list):\n",
    "    all_data_test[col] = all_data_test[col].astype(str)\n",
    "    tfidfVec.fit(all_data_test[col])\n",
    "    data_ = tfidfVec.transform(all_data_test[col])\n",
    "    if i == 0:\n",
    "        data_cat = data_\n",
    "    else:\n",
    "        data_cat = sparse.hstack((data_cat, data_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码解释\n",
    "这段代码是针对文本数据的向量化处理。代码主要使用了`TfidfVectorizer`进行文本特征的提取。以下是对代码逐行的解释：\n",
    "\n",
    "1. 首先，导入必要的库和方法。`CountVectorizer`和`TfidfVectorizer`用于将文本数据转换为向量形式。`ENGLISH_STOP_WORDS`提供英文停用词列表，这些词通常在文本处理中被过滤掉，因为它们通常不含有相关信息。`sparse`来自`scipy`库，用于处理稀疏矩阵。\n",
    "\n",
    "2. 注释掉了一个创建`CountVectorizer`的实例的行，这意味着作者可能原本计划使用它，但最后选择了`TfidfVectorizer`。停用词被设为英语停用词，`ngram_range`为(1, 1)，表示只考虑单个词语（一元模型），`max_features=100`表示仅保留词频最高的100个词。\n",
    "\n",
    "3. `TfidfVectorizer`同样创建了一个实例，配置和`CountVectorizer`几乎相同，但是`TfidfVectorizer`计算的是词的TF-IDF值，这是词频（TF）和逆文档频率（IDF）的乘积，能够更好地表示词的重要性。\n",
    "\n",
    "4. `columns_list`定义了需要进行特征提取的列名称列表，在这段代码中，作者只选择了`'seller_path'`一列进行特征提取。\n",
    "\n",
    "5. 通过遍历`columns_list`，对每个列采取如下操作：\n",
    "   - 首先将列的数据类型转换为字符串类型。\n",
    "   - 使用`tfidfVec`对象的`fit`方法来“学习”这一列的词汇和IDF值。\n",
    "   - 使用`transform`方法将这列的文本转换为TF-IDF的稀疏矩阵形式。\n",
    "   \n",
    "6. 如果这是第一个列（由`i == 0`检查），则将转换后得到的稀疏矩阵赋值给`data_cat`。如果不是第一个列，则使用`scipy`库中的`sparse.hstack`方法将新的稀疏矩阵与前面的矩阵横向拼接。\n",
    "\n",
    "7. 结果是`data_cat`变量现在包含了`'seller_path'`列经过TF-IDF转换后的特征矩阵。若`columns_list`中有更多列，则`data_cat`会依据上述逻辑横向拼接它们的TF-IDF矩阵。\n",
    "\n",
    "这段代码的目的是生成用于后续机器学习模型训练的特征，这种特征提取方法在文本分类、情感分析等NLP任务中非常常见。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)特征重命名 特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(data_cat.toarray())\n",
    "df_tfidf.columns = ['tfidf_' + str(i) for i in df_tfidf.columns]\n",
    "all_data_test = pd.concat([all_data_test, df_tfidf],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 嵌入(embeeding)特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Train Word2Vec model\n",
    "\n",
    "model = gensim.models.Word2Vec(all_data_test['seller_path'].apply(lambda x: x.split(' ')), vector_size=100, window=5, min_count=5, workers=4)\n",
    "# model.save(\"product2vec.model\")\n",
    "# model = gensim.models.Word2Vec.load(\"product2vec.model\")\n",
    "\n",
    "def mean_w2v_(x, model, vector_size=100):\n",
    "    try:\n",
    "        i = 0\n",
    "        for word in x.split(' '):\n",
    "            if word in model.wv.vocab:\n",
    "                i += 1\n",
    "                if i == 1:\n",
    "                    vec = np.zeros(vector_size)\n",
    "                vec += model.wv[word]\n",
    "        return vec / i \n",
    "    except:\n",
    "        return  np.zeros(vector_size)\n",
    "\n",
    "\n",
    "def get_mean_w2v(df_data, columns, model, vector_size):\n",
    "    data_array = []\n",
    "    for index, row in df_data.iterrows():\n",
    "        w2v = mean_w2v_(row[columns], model, vector_size)\n",
    "        data_array.append(w2v)\n",
    "    return pd.DataFrame(data_array)\n",
    "\n",
    "df_embeeding = get_mean_w2v(all_data_test, 'seller_path', model, 100)\n",
    "df_embeeding.columns = ['embeeding_' + str(i) for i in df_embeeding.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**embeeding特征和原始特征合并**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_test = pd.concat([all_data_test, df_embeeding],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 stacking特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 知识点六\n",
    "-- stacking特征\n",
    "\"\"\"\n",
    "# from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss,mean_absolute_error,mean_squared_error\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) stacking 回归特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 回归\n",
    "-- stacking 回归特征\n",
    "\"\"\"\n",
    "def stacking_reg(clf,train_x,train_y,test_x,clf_name,kf,label_split=None):\n",
    "    train=np.zeros((train_x.shape[0],1))\n",
    "    test=np.zeros((test_x.shape[0],1))\n",
    "    test_pre=np.empty((folds,test_x.shape[0],1))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf.split(train_x,label_split)):       \n",
    "        tr_x=train_x[train_index]\n",
    "        tr_y=train_y[train_index]\n",
    "        te_x=train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "        if clf_name in [\"rf\",\"ada\",\"gb\",\"et\",\"lr\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre=clf.predict(te_x).reshape(-1,1)\n",
    "            train[test_index]=pre\n",
    "            test_pre[i,:]=clf.predict(test_x).reshape(-1,1)\n",
    "            cv_scores.append(mean_squared_error(te_y, pre))\n",
    "        elif clf_name in [\"xgb\"]:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x, label=te_y, missing=-1)\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'eval_metric': 'rmse',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 12\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'),\n",
    "                         (test_matrix, 'eval')\n",
    "                         ]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_boost_round=num_round,evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(test_matrix,ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(z, ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "                cv_scores.append(mean_squared_error(te_y, pre))\n",
    "\n",
    "        elif clf_name in [\"lgb\"]:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      'objective': 'regression_l2',\n",
    "                      'metric': 'mse',\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'num_leaves': 2**5,\n",
    "                      'lambda_l2': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'learning_rate': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 12,\n",
    "                      'silent': True,\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            #early_stopping_rounds = 100\n",
    "            #callbacks=[lightgbm.log_evaluation(period=100), lightgbm.early_stopping(stopping_rounds=100)]\n",
    "            callbacks=[lightgbm.early_stopping(stopping_rounds=100)]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                                  #early_stopping_rounds=early_stopping_rounds\n",
    "                                  callbacks=callbacks\n",
    "                                  )\n",
    "                pre= model.predict(te_x,num_iteration=model.best_iteration).reshape(-1,1)\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(test_x, num_iteration=model.best_iteration).reshape(-1,1)\n",
    "                cv_scores.append(mean_squared_error(te_y, pre))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "    return train.reshape(-1,1),test.reshape(-1,1)\n",
    "\n",
    "def rf_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestRegressor(n_estimators=600, max_depth=20, n_jobs=-1, random_state=2017, max_features=\"auto\",verbose=1)\n",
    "    rf_train, rf_test = stacking_reg(randomforest, x_train, y_train, x_valid, \"rf\", kf, label_split=label_split)\n",
    "    return rf_train, rf_test,\"rf_reg\"\n",
    "\n",
    "def ada_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostRegressor(n_estimators=30, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_reg(adaboost, x_train, y_train, x_valid, \"ada\", kf, label_split=label_split)\n",
    "    return ada_train, ada_test,\"ada_reg\"\n",
    "\n",
    "def gb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingRegressor(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking_reg(gbdt, x_train, y_train, x_valid, \"gb\", kf, label_split=label_split)\n",
    "    return gbdt_train, gbdt_test,\"gb_reg\"\n",
    "\n",
    "def et_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesRegressor(n_estimators=600, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking_reg(extratree, x_train, y_train, x_valid, \"et\", kf, label_split=label_split)\n",
    "    return et_train, et_test,\"et_reg\"\n",
    "\n",
    "def lr_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lr_reg=LinearRegression(n_jobs=-1)\n",
    "    lr_train, lr_test = stacking_reg(lr_reg, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return lr_train, lr_test, \"lr_reg\"\n",
    "\n",
    "def xgb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_reg(xgboost, x_train, y_train, x_valid, \"xgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"xgb_reg\"\n",
    "\n",
    "def lgb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lgb_train, lgb_test = stacking_reg(lightgbm, x_train, y_train, x_valid, \"lgb\", kf, label_split=label_split)\n",
    "    return lgb_train, lgb_test,\"lgb_reg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) stacking 分类特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 分类\n",
    "-- stacking 分类特征\n",
    "\"\"\"\n",
    "def stacking_clf(clf,train_x,train_y,test_x,clf_name,kf,label_split=None):\n",
    "    train=np.zeros((train_x.shape[0],1))\n",
    "    test=np.zeros((test_x.shape[0],1))\n",
    "    test_pre=np.empty((folds,test_x.shape[0],1))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf.split(train_x,label_split)):       \n",
    "        tr_x=train_x[train_index]\n",
    "        tr_y=train_y[train_index]\n",
    "        te_x=train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "\n",
    "        if clf_name in [\"rf\",\"ada\",\"gb\",\"et\",\"lr\",\"knn\",\"gnb\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre=clf.predict_proba(te_x)\n",
    "            \n",
    "            train[test_index]=pre[:,0].reshape(-1,1)\n",
    "            test_pre[i,:]=clf.predict_proba(test_x)[:,0].reshape(-1,1)\n",
    "            \n",
    "            cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        elif clf_name in [\"xgb\"]:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x)\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'objective': 'multi:softprob',\n",
    "                      'eval_metric': 'mlogloss',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      \"num_class\": 2\n",
    "                      }\n",
    "\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'),\n",
    "                         (test_matrix, 'eval')\n",
    "                         ]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_boost_round=num_round,evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(test_matrix,ntree_limit=model.best_ntree_limit)\n",
    "                train[test_index]=pre[:,0].reshape(-1,1)\n",
    "                test_pre[i, :]= model.predict(z, ntree_limit=model.best_ntree_limit)[:,0].reshape(-1,1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        elif clf_name in [\"lgb\"]:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      #'boosting_type': 'dart',\n",
    "                      'objective': 'multiclass',\n",
    "                      'metric': 'multi_logloss',\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'num_leaves': 2**5,\n",
    "                      'lambda_l2': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'learning_rate': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      \"num_class\": 2,\n",
    "                      'silent': True,\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            #early_stopping_rounds = 100\n",
    "            #callbacks=[lightgbm.log_evaluation(period=100), lightgbm.early_stopping(stopping_rounds=100)]\n",
    "            callbacks=[lightgbm.early_stopping(stopping_rounds=100)]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                                  #early_stopping_rounds=early_stopping_rounds\n",
    "                                  callbacks=callbacks\n",
    "                                  )\n",
    "                pre= model.predict(te_x,num_iteration=model.best_iteration)\n",
    "                train[test_index]=pre[:,0].reshape(-1,1)\n",
    "                test_pre[i, :]= model.predict(test_x, num_iteration=model.best_iteration)[:,0].reshape(-1,1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "    return train.reshape(-1,1),test.reshape(-1,1)\n",
    "\n",
    "def rf_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestClassifier(n_estimators=1200, max_depth=20, n_jobs=-1, random_state=2017, max_features=\"auto\",verbose=1)\n",
    "    rf_train, rf_test = stacking_clf(randomforest, x_train, y_train, x_valid, \"rf\", kf, label_split=label_split)\n",
    "    return rf_train, rf_test,\"rf\"\n",
    "\n",
    "def ada_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostClassifier(n_estimators=50, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_clf(adaboost, x_train, y_train, x_valid, \"ada\", kf, label_split=label_split)\n",
    "    return ada_train, ada_test,\"ada\"\n",
    "\n",
    "def gb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingClassifier(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking_clf(gbdt, x_train, y_train, x_valid, \"gb\", kf, label_split=label_split)\n",
    "    return gbdt_train, gbdt_test,\"gb\"\n",
    "\n",
    "def et_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesClassifier(n_estimators=1200, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking_clf(extratree, x_train, y_train, x_valid, \"et\", kf, label_split=label_split)\n",
    "    return et_train, et_test,\"et\"\n",
    "\n",
    "def xgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(xgboost, x_train, y_train, x_valid, \"xgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"xgb\"\n",
    "\n",
    "def lgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(lightgbm, x_train, y_train, x_valid, \"lgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"lgb\"\n",
    "\n",
    "def gnb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gnb=GaussianNB()\n",
    "    gnb_train, gnb_test = stacking_clf(gnb, x_train, y_train, x_valid, \"gnb\", kf, label_split=label_split)\n",
    "    return gnb_train, gnb_test,\"gnb\"\n",
    "\n",
    "def lr_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    logisticregression=LogisticRegression(n_jobs=-1,random_state=2017,C=0.1,max_iter=200)\n",
    "    lr_train, lr_test = stacking_clf(logisticregression, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return lr_train, lr_test, \"lr\"\n",
    "\n",
    "def knn_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    kneighbors=KNeighborsClassifier(n_neighbors=200,n_jobs=-1)\n",
    "    knn_train, knn_test = stacking_clf(kneighbors, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return knn_train, knn_test, \"knn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 获取训练和验证数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_columns = [c for c in all_data_test.columns if c not in ['label', 'prob', 'seller_path', 'cat_path', 'brand_path', 'action_type_path', 'item_path', 'time_stamp_path']]\n",
    "x_train = all_data_test[~all_data_test['label'].isna()][features_columns].values\n",
    "y_train = all_data_test[~all_data_test['label'].isna()]['label'].values\n",
    "x_valid = all_data_test[all_data_test['label'].isna()][features_columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理函数值inf以及nan，为特征工程做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix(data):\n",
    "    where_are_nan = np.isnan(data)\n",
    "    where_are_inf = np.isinf(data)\n",
    "    data[where_are_nan] = 0\n",
    "    data[where_are_inf] = 0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.float_(get_matrix(np.float_(x_train)))\n",
    "y_train = np.int_(y_train)\n",
    "x_valid = x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 使用lgb和xgb分类模型构造stacking特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1）使用5折交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "folds = 5\n",
    "seed = 1\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2）选择1gb和xgb分类模型作为基模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_list = [lgb_clf, xgb_clf, lgb_reg, xgb_reg]\n",
    "# clf_list_col = ['lgb_clf', 'xgb_clf', 'lgb_reg', 'xgb_reg']\n",
    "\n",
    "clf_list = [lgb_clf, xgb_clf]\n",
    "clf_list_col = ['lgb_clf', 'xgb_clf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3）训练模型，获取stacking特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 17890\n",
      "[LightGBM] [Info] Number of data points in the train set: 6764, number of used features: 127\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.072177\n",
      "[LightGBM] [Info] Start training from score -2.664512\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's multi_logloss: 0.226094\n",
      "lgb now score is: [2.5655945498739623]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002083 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 17829\n",
      "[LightGBM] [Info] Number of data points in the train set: 6764, number of used features: 127\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.070430\n",
      "[LightGBM] [Info] Start training from score -2.688143\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's multi_logloss: 0.245323\n",
      "lgb now score is: [2.5655945498739623, 2.5150791385183453]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002523 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 17813\n",
      "[LightGBM] [Info] Number of data points in the train set: 6764, number of used features: 127\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.071065\n",
      "[LightGBM] [Info] Start training from score -2.679485\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's multi_logloss: 0.23763\n",
      "lgb now score is: [2.5655945498739623, 2.5150791385183453, 2.532372049599257]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17998\n",
      "[LightGBM] [Info] Number of data points in the train set: 6764, number of used features: 127\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.067579\n",
      "[LightGBM] [Info] Start training from score -2.728060\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's multi_logloss: 0.272271\n",
      "lgb now score is: [2.5655945498739623, 2.5150791385183453, 2.532372049599257, 2.559832207248309]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 17857\n",
      "[LightGBM] [Info] Number of data points in the train set: 6764, number of used features: 127\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.069637\n",
      "[LightGBM] [Info] Start training from score -2.699072\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's multi_logloss: 0.251718\n",
      "lgb now score is: [2.5655945498739623, 2.5150791385183453, 2.532372049599257, 2.559832207248309, 2.532995395195058]\n",
      "lgb_score_list: [2.5655945498739623, 2.5150791385183453, 2.532372049599257, 2.559832207248309, 2.532995395195058]\n",
      "lgb_score_mean: 2.5411746680869864\n",
      "[0]\ttrain-mlogloss:0.67118\teval-mlogloss:0.67077\n",
      "[1]\ttrain-mlogloss:0.65052\teval-mlogloss:0.64969\n",
      "[2]\ttrain-mlogloss:0.63116\teval-mlogloss:0.62995\n",
      "[3]\ttrain-mlogloss:0.61284\teval-mlogloss:0.61129\n",
      "[4]\ttrain-mlogloss:0.59555\teval-mlogloss:0.59356\n",
      "[5]\ttrain-mlogloss:0.57923\teval-mlogloss:0.57690\n",
      "[6]\ttrain-mlogloss:0.56351\teval-mlogloss:0.56084\n",
      "[7]\ttrain-mlogloss:0.54870\teval-mlogloss:0.54577\n",
      "[8]\ttrain-mlogloss:0.53467\teval-mlogloss:0.53140\n",
      "[9]\ttrain-mlogloss:0.52148\teval-mlogloss:0.51793\n",
      "[10]\ttrain-mlogloss:0.50887\teval-mlogloss:0.50499\n",
      "[11]\ttrain-mlogloss:0.49695\teval-mlogloss:0.49281\n",
      "[12]\ttrain-mlogloss:0.48556\teval-mlogloss:0.48118\n",
      "[13]\ttrain-mlogloss:0.47469\teval-mlogloss:0.47010\n",
      "[14]\ttrain-mlogloss:0.46438\teval-mlogloss:0.45957\n",
      "[15]\ttrain-mlogloss:0.45447\teval-mlogloss:0.44940\n",
      "[16]\ttrain-mlogloss:0.44513\teval-mlogloss:0.43980\n",
      "[17]\ttrain-mlogloss:0.43618\teval-mlogloss:0.43058\n",
      "[18]\ttrain-mlogloss:0.42765\teval-mlogloss:0.42183\n",
      "[19]\ttrain-mlogloss:0.41941\teval-mlogloss:0.41338\n",
      "[20]\ttrain-mlogloss:0.41174\teval-mlogloss:0.40550\n",
      "[21]\ttrain-mlogloss:0.40432\teval-mlogloss:0.39786\n",
      "[22]\ttrain-mlogloss:0.39727\teval-mlogloss:0.39062\n",
      "[23]\ttrain-mlogloss:0.39047\teval-mlogloss:0.38367\n",
      "[24]\ttrain-mlogloss:0.38399\teval-mlogloss:0.37707\n",
      "[25]\ttrain-mlogloss:0.37772\teval-mlogloss:0.37056\n",
      "[26]\ttrain-mlogloss:0.37180\teval-mlogloss:0.36453\n",
      "[27]\ttrain-mlogloss:0.36613\teval-mlogloss:0.35874\n",
      "[28]\ttrain-mlogloss:0.36071\teval-mlogloss:0.35320\n",
      "[29]\ttrain-mlogloss:0.35550\teval-mlogloss:0.34789\n",
      "[30]\ttrain-mlogloss:0.35053\teval-mlogloss:0.34280\n",
      "[31]\ttrain-mlogloss:0.34567\teval-mlogloss:0.33781\n",
      "[32]\ttrain-mlogloss:0.34108\teval-mlogloss:0.33304\n",
      "[33]\ttrain-mlogloss:0.33669\teval-mlogloss:0.32852\n",
      "[34]\ttrain-mlogloss:0.33248\teval-mlogloss:0.32418\n",
      "[35]\ttrain-mlogloss:0.32846\teval-mlogloss:0.32002\n",
      "[36]\ttrain-mlogloss:0.32460\teval-mlogloss:0.31608\n",
      "[37]\ttrain-mlogloss:0.32091\teval-mlogloss:0.31224\n",
      "[38]\ttrain-mlogloss:0.31744\teval-mlogloss:0.30865\n",
      "[39]\ttrain-mlogloss:0.31414\teval-mlogloss:0.30521\n",
      "[40]\ttrain-mlogloss:0.31094\teval-mlogloss:0.30195\n",
      "[41]\ttrain-mlogloss:0.30785\teval-mlogloss:0.29878\n",
      "[42]\ttrain-mlogloss:0.30481\teval-mlogloss:0.29570\n",
      "[43]\ttrain-mlogloss:0.30198\teval-mlogloss:0.29282\n",
      "[44]\ttrain-mlogloss:0.29922\teval-mlogloss:0.28996\n",
      "[45]\ttrain-mlogloss:0.29664\teval-mlogloss:0.28727\n",
      "[46]\ttrain-mlogloss:0.29412\teval-mlogloss:0.28471\n",
      "[47]\ttrain-mlogloss:0.29175\teval-mlogloss:0.28227\n",
      "[48]\ttrain-mlogloss:0.28941\teval-mlogloss:0.27997\n",
      "[49]\ttrain-mlogloss:0.28724\teval-mlogloss:0.27779\n",
      "[50]\ttrain-mlogloss:0.28506\teval-mlogloss:0.27554\n",
      "[51]\ttrain-mlogloss:0.28300\teval-mlogloss:0.27347\n",
      "[52]\ttrain-mlogloss:0.28108\teval-mlogloss:0.27150\n",
      "[53]\ttrain-mlogloss:0.27918\teval-mlogloss:0.26965\n",
      "[54]\ttrain-mlogloss:0.27739\teval-mlogloss:0.26789\n",
      "[55]\ttrain-mlogloss:0.27563\teval-mlogloss:0.26619\n",
      "[56]\ttrain-mlogloss:0.27394\teval-mlogloss:0.26454\n",
      "[57]\ttrain-mlogloss:0.27238\teval-mlogloss:0.26297\n",
      "[58]\ttrain-mlogloss:0.27087\teval-mlogloss:0.26148\n",
      "[59]\ttrain-mlogloss:0.26936\teval-mlogloss:0.26000\n",
      "[60]\ttrain-mlogloss:0.26793\teval-mlogloss:0.25865\n",
      "[61]\ttrain-mlogloss:0.26660\teval-mlogloss:0.25740\n",
      "[62]\ttrain-mlogloss:0.26525\teval-mlogloss:0.25601\n",
      "[63]\ttrain-mlogloss:0.26401\teval-mlogloss:0.25475\n",
      "[64]\ttrain-mlogloss:0.26283\teval-mlogloss:0.25356\n",
      "[65]\ttrain-mlogloss:0.26169\teval-mlogloss:0.25245\n",
      "[66]\ttrain-mlogloss:0.26058\teval-mlogloss:0.25138\n",
      "[67]\ttrain-mlogloss:0.25946\teval-mlogloss:0.25030\n",
      "[68]\ttrain-mlogloss:0.25838\teval-mlogloss:0.24934\n",
      "[69]\ttrain-mlogloss:0.25727\teval-mlogloss:0.24835\n",
      "[70]\ttrain-mlogloss:0.25632\teval-mlogloss:0.24738\n",
      "[71]\ttrain-mlogloss:0.25542\teval-mlogloss:0.24651\n",
      "[72]\ttrain-mlogloss:0.25452\teval-mlogloss:0.24566\n",
      "[73]\ttrain-mlogloss:0.25365\teval-mlogloss:0.24487\n",
      "[74]\ttrain-mlogloss:0.25284\teval-mlogloss:0.24409\n",
      "[75]\ttrain-mlogloss:0.25200\teval-mlogloss:0.24334\n",
      "[76]\ttrain-mlogloss:0.25117\teval-mlogloss:0.24261\n",
      "[77]\ttrain-mlogloss:0.25047\teval-mlogloss:0.24198\n",
      "[78]\ttrain-mlogloss:0.24974\teval-mlogloss:0.24131\n",
      "[79]\ttrain-mlogloss:0.24908\teval-mlogloss:0.24071\n",
      "[80]\ttrain-mlogloss:0.24836\teval-mlogloss:0.24017\n",
      "[81]\ttrain-mlogloss:0.24768\teval-mlogloss:0.23953\n",
      "[82]\ttrain-mlogloss:0.24701\teval-mlogloss:0.23900\n",
      "[83]\ttrain-mlogloss:0.24635\teval-mlogloss:0.23852\n",
      "[84]\ttrain-mlogloss:0.24583\teval-mlogloss:0.23802\n",
      "[85]\ttrain-mlogloss:0.24519\teval-mlogloss:0.23755\n",
      "[86]\ttrain-mlogloss:0.24453\teval-mlogloss:0.23701\n",
      "[87]\ttrain-mlogloss:0.24393\teval-mlogloss:0.23659\n",
      "[88]\ttrain-mlogloss:0.24342\teval-mlogloss:0.23620\n",
      "[89]\ttrain-mlogloss:0.24282\teval-mlogloss:0.23574\n",
      "[90]\ttrain-mlogloss:0.24229\teval-mlogloss:0.23530\n",
      "[91]\ttrain-mlogloss:0.24180\teval-mlogloss:0.23491\n",
      "[92]\ttrain-mlogloss:0.24131\teval-mlogloss:0.23454\n",
      "[93]\ttrain-mlogloss:0.24081\teval-mlogloss:0.23421\n",
      "[94]\ttrain-mlogloss:0.24028\teval-mlogloss:0.23386\n",
      "[95]\ttrain-mlogloss:0.23987\teval-mlogloss:0.23353\n",
      "[96]\ttrain-mlogloss:0.23950\teval-mlogloss:0.23327\n",
      "[97]\ttrain-mlogloss:0.23905\teval-mlogloss:0.23297\n",
      "[98]\ttrain-mlogloss:0.23863\teval-mlogloss:0.23260\n",
      "[99]\ttrain-mlogloss:0.23818\teval-mlogloss:0.23237\n",
      "[100]\ttrain-mlogloss:0.23775\teval-mlogloss:0.23217\n",
      "[101]\ttrain-mlogloss:0.23734\teval-mlogloss:0.23194\n",
      "[102]\ttrain-mlogloss:0.23690\teval-mlogloss:0.23167\n",
      "[103]\ttrain-mlogloss:0.23654\teval-mlogloss:0.23145\n",
      "[104]\ttrain-mlogloss:0.23620\teval-mlogloss:0.23120\n",
      "[105]\ttrain-mlogloss:0.23585\teval-mlogloss:0.23103\n",
      "[106]\ttrain-mlogloss:0.23562\teval-mlogloss:0.23085\n",
      "[107]\ttrain-mlogloss:0.23528\teval-mlogloss:0.23071\n",
      "[108]\ttrain-mlogloss:0.23500\teval-mlogloss:0.23052\n",
      "[109]\ttrain-mlogloss:0.23457\teval-mlogloss:0.23028\n",
      "[110]\ttrain-mlogloss:0.23426\teval-mlogloss:0.23013\n",
      "[111]\ttrain-mlogloss:0.23402\teval-mlogloss:0.23001\n",
      "[112]\ttrain-mlogloss:0.23376\teval-mlogloss:0.22992\n",
      "[113]\ttrain-mlogloss:0.23347\teval-mlogloss:0.22974\n",
      "[114]\ttrain-mlogloss:0.23314\teval-mlogloss:0.22955\n",
      "[115]\ttrain-mlogloss:0.23285\teval-mlogloss:0.22940\n",
      "[116]\ttrain-mlogloss:0.23250\teval-mlogloss:0.22933\n",
      "[117]\ttrain-mlogloss:0.23218\teval-mlogloss:0.22918\n",
      "[118]\ttrain-mlogloss:0.23182\teval-mlogloss:0.22909\n",
      "[119]\ttrain-mlogloss:0.23151\teval-mlogloss:0.22892\n",
      "[120]\ttrain-mlogloss:0.23117\teval-mlogloss:0.22888\n",
      "[121]\ttrain-mlogloss:0.23092\teval-mlogloss:0.22877\n",
      "[122]\ttrain-mlogloss:0.23067\teval-mlogloss:0.22868\n",
      "[123]\ttrain-mlogloss:0.23035\teval-mlogloss:0.22857\n",
      "[124]\ttrain-mlogloss:0.23007\teval-mlogloss:0.22842\n",
      "[125]\ttrain-mlogloss:0.22985\teval-mlogloss:0.22834\n",
      "[126]\ttrain-mlogloss:0.22953\teval-mlogloss:0.22831\n",
      "[127]\ttrain-mlogloss:0.22934\teval-mlogloss:0.22828\n",
      "[128]\ttrain-mlogloss:0.22917\teval-mlogloss:0.22817\n",
      "[129]\ttrain-mlogloss:0.22890\teval-mlogloss:0.22812\n",
      "[130]\ttrain-mlogloss:0.22848\teval-mlogloss:0.22797\n",
      "[131]\ttrain-mlogloss:0.22831\teval-mlogloss:0.22786\n",
      "[132]\ttrain-mlogloss:0.22817\teval-mlogloss:0.22783\n",
      "[133]\ttrain-mlogloss:0.22787\teval-mlogloss:0.22771\n",
      "[134]\ttrain-mlogloss:0.22763\teval-mlogloss:0.22768\n",
      "[135]\ttrain-mlogloss:0.22741\teval-mlogloss:0.22771\n",
      "[136]\ttrain-mlogloss:0.22721\teval-mlogloss:0.22773\n",
      "[137]\ttrain-mlogloss:0.22688\teval-mlogloss:0.22774\n",
      "[138]\ttrain-mlogloss:0.22667\teval-mlogloss:0.22777\n",
      "[139]\ttrain-mlogloss:0.22643\teval-mlogloss:0.22776\n",
      "[140]\ttrain-mlogloss:0.22624\teval-mlogloss:0.22781\n",
      "[141]\ttrain-mlogloss:0.22595\teval-mlogloss:0.22776\n",
      "[142]\ttrain-mlogloss:0.22572\teval-mlogloss:0.22765\n",
      "[143]\ttrain-mlogloss:0.22546\teval-mlogloss:0.22764\n",
      "[144]\ttrain-mlogloss:0.22526\teval-mlogloss:0.22754\n",
      "[145]\ttrain-mlogloss:0.22498\teval-mlogloss:0.22749\n",
      "[146]\ttrain-mlogloss:0.22475\teval-mlogloss:0.22740\n",
      "[147]\ttrain-mlogloss:0.22456\teval-mlogloss:0.22732\n",
      "[148]\ttrain-mlogloss:0.22435\teval-mlogloss:0.22724\n",
      "[149]\ttrain-mlogloss:0.22410\teval-mlogloss:0.22722\n",
      "[150]\ttrain-mlogloss:0.22387\teval-mlogloss:0.22719\n",
      "[151]\ttrain-mlogloss:0.22366\teval-mlogloss:0.22717\n",
      "[152]\ttrain-mlogloss:0.22340\teval-mlogloss:0.22716\n",
      "[153]\ttrain-mlogloss:0.22325\teval-mlogloss:0.22710\n",
      "[154]\ttrain-mlogloss:0.22302\teval-mlogloss:0.22710\n",
      "[155]\ttrain-mlogloss:0.22281\teval-mlogloss:0.22709\n",
      "[156]\ttrain-mlogloss:0.22254\teval-mlogloss:0.22702\n",
      "[157]\ttrain-mlogloss:0.22235\teval-mlogloss:0.22702\n",
      "[158]\ttrain-mlogloss:0.22208\teval-mlogloss:0.22703\n",
      "[159]\ttrain-mlogloss:0.22175\teval-mlogloss:0.22697\n",
      "[160]\ttrain-mlogloss:0.22160\teval-mlogloss:0.22693\n",
      "[161]\ttrain-mlogloss:0.22142\teval-mlogloss:0.22692\n",
      "[162]\ttrain-mlogloss:0.22115\teval-mlogloss:0.22694\n",
      "[163]\ttrain-mlogloss:0.22100\teval-mlogloss:0.22691\n",
      "[164]\ttrain-mlogloss:0.22076\teval-mlogloss:0.22687\n",
      "[165]\ttrain-mlogloss:0.22047\teval-mlogloss:0.22687\n",
      "[166]\ttrain-mlogloss:0.22025\teval-mlogloss:0.22687\n",
      "[167]\ttrain-mlogloss:0.22003\teval-mlogloss:0.22685\n",
      "[168]\ttrain-mlogloss:0.21988\teval-mlogloss:0.22682\n",
      "[169]\ttrain-mlogloss:0.21958\teval-mlogloss:0.22675\n",
      "[170]\ttrain-mlogloss:0.21931\teval-mlogloss:0.22680\n",
      "[171]\ttrain-mlogloss:0.21905\teval-mlogloss:0.22678\n",
      "[172]\ttrain-mlogloss:0.21889\teval-mlogloss:0.22673\n",
      "[173]\ttrain-mlogloss:0.21866\teval-mlogloss:0.22675\n",
      "[174]\ttrain-mlogloss:0.21848\teval-mlogloss:0.22673\n",
      "[175]\ttrain-mlogloss:0.21829\teval-mlogloss:0.22673\n",
      "[176]\ttrain-mlogloss:0.21817\teval-mlogloss:0.22670\n",
      "[177]\ttrain-mlogloss:0.21797\teval-mlogloss:0.22674\n",
      "[178]\ttrain-mlogloss:0.21783\teval-mlogloss:0.22673\n",
      "[179]\ttrain-mlogloss:0.21761\teval-mlogloss:0.22670\n",
      "[180]\ttrain-mlogloss:0.21744\teval-mlogloss:0.22673\n",
      "[181]\ttrain-mlogloss:0.21729\teval-mlogloss:0.22675\n",
      "[182]\ttrain-mlogloss:0.21702\teval-mlogloss:0.22671\n",
      "[183]\ttrain-mlogloss:0.21687\teval-mlogloss:0.22672\n",
      "[184]\ttrain-mlogloss:0.21667\teval-mlogloss:0.22676\n",
      "[185]\ttrain-mlogloss:0.21648\teval-mlogloss:0.22684\n",
      "[186]\ttrain-mlogloss:0.21631\teval-mlogloss:0.22680\n",
      "[187]\ttrain-mlogloss:0.21614\teval-mlogloss:0.22678\n",
      "[188]\ttrain-mlogloss:0.21597\teval-mlogloss:0.22677\n",
      "[189]\ttrain-mlogloss:0.21573\teval-mlogloss:0.22678\n",
      "[190]\ttrain-mlogloss:0.21555\teval-mlogloss:0.22676\n",
      "[191]\ttrain-mlogloss:0.21531\teval-mlogloss:0.22678\n",
      "[192]\ttrain-mlogloss:0.21509\teval-mlogloss:0.22681\n",
      "[193]\ttrain-mlogloss:0.21482\teval-mlogloss:0.22673\n",
      "[194]\ttrain-mlogloss:0.21459\teval-mlogloss:0.22674\n",
      "[195]\ttrain-mlogloss:0.21437\teval-mlogloss:0.22678\n",
      "[196]\ttrain-mlogloss:0.21407\teval-mlogloss:0.22681\n",
      "[197]\ttrain-mlogloss:0.21382\teval-mlogloss:0.22678\n",
      "[198]\ttrain-mlogloss:0.21362\teval-mlogloss:0.22678\n",
      "[199]\ttrain-mlogloss:0.21338\teval-mlogloss:0.22681\n",
      "[200]\ttrain-mlogloss:0.21322\teval-mlogloss:0.22677\n",
      "[201]\ttrain-mlogloss:0.21304\teval-mlogloss:0.22680\n",
      "[202]\ttrain-mlogloss:0.21283\teval-mlogloss:0.22677\n",
      "[203]\ttrain-mlogloss:0.21257\teval-mlogloss:0.22675\n",
      "[204]\ttrain-mlogloss:0.21235\teval-mlogloss:0.22668\n",
      "[205]\ttrain-mlogloss:0.21221\teval-mlogloss:0.22666\n",
      "[206]\ttrain-mlogloss:0.21205\teval-mlogloss:0.22664\n",
      "[207]\ttrain-mlogloss:0.21176\teval-mlogloss:0.22670\n",
      "[208]\ttrain-mlogloss:0.21163\teval-mlogloss:0.22664\n",
      "[209]\ttrain-mlogloss:0.21145\teval-mlogloss:0.22662\n",
      "[210]\ttrain-mlogloss:0.21135\teval-mlogloss:0.22665\n",
      "[211]\ttrain-mlogloss:0.21120\teval-mlogloss:0.22665\n",
      "[212]\ttrain-mlogloss:0.21109\teval-mlogloss:0.22667\n",
      "[213]\ttrain-mlogloss:0.21093\teval-mlogloss:0.22666\n",
      "[214]\ttrain-mlogloss:0.21072\teval-mlogloss:0.22671\n",
      "[215]\ttrain-mlogloss:0.21057\teval-mlogloss:0.22672\n",
      "[216]\ttrain-mlogloss:0.21041\teval-mlogloss:0.22666\n",
      "[217]\ttrain-mlogloss:0.21025\teval-mlogloss:0.22669\n",
      "[218]\ttrain-mlogloss:0.21005\teval-mlogloss:0.22667\n",
      "[219]\ttrain-mlogloss:0.20983\teval-mlogloss:0.22660\n",
      "[220]\ttrain-mlogloss:0.20970\teval-mlogloss:0.22660\n",
      "[221]\ttrain-mlogloss:0.20950\teval-mlogloss:0.22656\n",
      "[222]\ttrain-mlogloss:0.20927\teval-mlogloss:0.22651\n",
      "[223]\ttrain-mlogloss:0.20909\teval-mlogloss:0.22654\n",
      "[224]\ttrain-mlogloss:0.20887\teval-mlogloss:0.22648\n",
      "[225]\ttrain-mlogloss:0.20863\teval-mlogloss:0.22651\n",
      "[226]\ttrain-mlogloss:0.20853\teval-mlogloss:0.22650\n",
      "[227]\ttrain-mlogloss:0.20841\teval-mlogloss:0.22653\n",
      "[228]\ttrain-mlogloss:0.20818\teval-mlogloss:0.22645\n",
      "[229]\ttrain-mlogloss:0.20806\teval-mlogloss:0.22644\n",
      "[230]\ttrain-mlogloss:0.20782\teval-mlogloss:0.22648\n",
      "[231]\ttrain-mlogloss:0.20757\teval-mlogloss:0.22647\n",
      "[232]\ttrain-mlogloss:0.20740\teval-mlogloss:0.22653\n",
      "[233]\ttrain-mlogloss:0.20732\teval-mlogloss:0.22645\n",
      "[234]\ttrain-mlogloss:0.20717\teval-mlogloss:0.22644\n",
      "[235]\ttrain-mlogloss:0.20704\teval-mlogloss:0.22643\n",
      "[236]\ttrain-mlogloss:0.20686\teval-mlogloss:0.22643\n",
      "[237]\ttrain-mlogloss:0.20663\teval-mlogloss:0.22638\n",
      "[238]\ttrain-mlogloss:0.20649\teval-mlogloss:0.22640\n",
      "[239]\ttrain-mlogloss:0.20629\teval-mlogloss:0.22638\n",
      "[240]\ttrain-mlogloss:0.20604\teval-mlogloss:0.22636\n",
      "[241]\ttrain-mlogloss:0.20588\teval-mlogloss:0.22635\n",
      "[242]\ttrain-mlogloss:0.20571\teval-mlogloss:0.22638\n",
      "[243]\ttrain-mlogloss:0.20560\teval-mlogloss:0.22635\n",
      "[244]\ttrain-mlogloss:0.20541\teval-mlogloss:0.22637\n",
      "[245]\ttrain-mlogloss:0.20527\teval-mlogloss:0.22638\n",
      "[246]\ttrain-mlogloss:0.20513\teval-mlogloss:0.22637\n",
      "[247]\ttrain-mlogloss:0.20503\teval-mlogloss:0.22632\n",
      "[248]\ttrain-mlogloss:0.20489\teval-mlogloss:0.22631\n",
      "[249]\ttrain-mlogloss:0.20476\teval-mlogloss:0.22632\n",
      "[250]\ttrain-mlogloss:0.20463\teval-mlogloss:0.22633\n",
      "[251]\ttrain-mlogloss:0.20449\teval-mlogloss:0.22634\n",
      "[252]\ttrain-mlogloss:0.20439\teval-mlogloss:0.22631\n",
      "[253]\ttrain-mlogloss:0.20420\teval-mlogloss:0.22638\n",
      "[254]\ttrain-mlogloss:0.20408\teval-mlogloss:0.22634\n",
      "[255]\ttrain-mlogloss:0.20398\teval-mlogloss:0.22638\n",
      "[256]\ttrain-mlogloss:0.20376\teval-mlogloss:0.22642\n",
      "[257]\ttrain-mlogloss:0.20351\teval-mlogloss:0.22644\n",
      "[258]\ttrain-mlogloss:0.20333\teval-mlogloss:0.22645\n",
      "[259]\ttrain-mlogloss:0.20311\teval-mlogloss:0.22647\n",
      "[260]\ttrain-mlogloss:0.20282\teval-mlogloss:0.22635\n",
      "[261]\ttrain-mlogloss:0.20270\teval-mlogloss:0.22639\n",
      "[262]\ttrain-mlogloss:0.20248\teval-mlogloss:0.22642\n",
      "[263]\ttrain-mlogloss:0.20229\teval-mlogloss:0.22637\n",
      "[264]\ttrain-mlogloss:0.20212\teval-mlogloss:0.22635\n",
      "[265]\ttrain-mlogloss:0.20200\teval-mlogloss:0.22643\n",
      "[266]\ttrain-mlogloss:0.20186\teval-mlogloss:0.22643\n",
      "[267]\ttrain-mlogloss:0.20174\teval-mlogloss:0.22648\n",
      "[268]\ttrain-mlogloss:0.20157\teval-mlogloss:0.22648\n",
      "[269]\ttrain-mlogloss:0.20144\teval-mlogloss:0.22650\n",
      "[270]\ttrain-mlogloss:0.20124\teval-mlogloss:0.22650\n",
      "[271]\ttrain-mlogloss:0.20103\teval-mlogloss:0.22651\n",
      "[272]\ttrain-mlogloss:0.20093\teval-mlogloss:0.22649\n",
      "[273]\ttrain-mlogloss:0.20075\teval-mlogloss:0.22652\n",
      "[274]\ttrain-mlogloss:0.20056\teval-mlogloss:0.22648\n",
      "[275]\ttrain-mlogloss:0.20039\teval-mlogloss:0.22649\n",
      "[276]\ttrain-mlogloss:0.20029\teval-mlogloss:0.22652\n",
      "[277]\ttrain-mlogloss:0.20016\teval-mlogloss:0.22655\n",
      "[278]\ttrain-mlogloss:0.19995\teval-mlogloss:0.22658\n",
      "[279]\ttrain-mlogloss:0.19981\teval-mlogloss:0.22657\n",
      "[280]\ttrain-mlogloss:0.19960\teval-mlogloss:0.22659\n",
      "[281]\ttrain-mlogloss:0.19946\teval-mlogloss:0.22655\n",
      "[282]\ttrain-mlogloss:0.19935\teval-mlogloss:0.22659\n",
      "[283]\ttrain-mlogloss:0.19910\teval-mlogloss:0.22655\n",
      "[284]\ttrain-mlogloss:0.19902\teval-mlogloss:0.22650\n",
      "[285]\ttrain-mlogloss:0.19889\teval-mlogloss:0.22648\n",
      "[286]\ttrain-mlogloss:0.19866\teval-mlogloss:0.22649\n",
      "[287]\ttrain-mlogloss:0.19856\teval-mlogloss:0.22653\n",
      "[288]\ttrain-mlogloss:0.19841\teval-mlogloss:0.22656\n",
      "[289]\ttrain-mlogloss:0.19828\teval-mlogloss:0.22657\n",
      "[290]\ttrain-mlogloss:0.19820\teval-mlogloss:0.22651\n",
      "[291]\ttrain-mlogloss:0.19804\teval-mlogloss:0.22654\n",
      "[292]\ttrain-mlogloss:0.19784\teval-mlogloss:0.22660\n",
      "[293]\ttrain-mlogloss:0.19769\teval-mlogloss:0.22660\n",
      "[294]\ttrain-mlogloss:0.19755\teval-mlogloss:0.22659\n",
      "[295]\ttrain-mlogloss:0.19738\teval-mlogloss:0.22666\n",
      "[296]\ttrain-mlogloss:0.19717\teval-mlogloss:0.22671\n",
      "[297]\ttrain-mlogloss:0.19703\teval-mlogloss:0.22673\n",
      "[298]\ttrain-mlogloss:0.19693\teval-mlogloss:0.22673\n",
      "[299]\ttrain-mlogloss:0.19682\teval-mlogloss:0.22672\n",
      "[300]\ttrain-mlogloss:0.19668\teval-mlogloss:0.22676\n",
      "[301]\ttrain-mlogloss:0.19648\teval-mlogloss:0.22676\n",
      "[302]\ttrain-mlogloss:0.19635\teval-mlogloss:0.22681\n",
      "[303]\ttrain-mlogloss:0.19623\teval-mlogloss:0.22679\n",
      "[304]\ttrain-mlogloss:0.19612\teval-mlogloss:0.22676\n",
      "[305]\ttrain-mlogloss:0.19600\teval-mlogloss:0.22679\n",
      "[306]\ttrain-mlogloss:0.19582\teval-mlogloss:0.22675\n",
      "[307]\ttrain-mlogloss:0.19566\teval-mlogloss:0.22675\n",
      "[308]\ttrain-mlogloss:0.19553\teval-mlogloss:0.22675\n",
      "[309]\ttrain-mlogloss:0.19536\teval-mlogloss:0.22674\n",
      "[310]\ttrain-mlogloss:0.19522\teval-mlogloss:0.22672\n",
      "[311]\ttrain-mlogloss:0.19501\teval-mlogloss:0.22672\n",
      "[312]\ttrain-mlogloss:0.19481\teval-mlogloss:0.22674\n",
      "[313]\ttrain-mlogloss:0.19466\teval-mlogloss:0.22668\n",
      "[314]\ttrain-mlogloss:0.19456\teval-mlogloss:0.22667\n",
      "[315]\ttrain-mlogloss:0.19443\teval-mlogloss:0.22667\n",
      "[316]\ttrain-mlogloss:0.19424\teval-mlogloss:0.22665\n",
      "[317]\ttrain-mlogloss:0.19411\teval-mlogloss:0.22666\n",
      "[318]\ttrain-mlogloss:0.19395\teval-mlogloss:0.22673\n",
      "[319]\ttrain-mlogloss:0.19384\teval-mlogloss:0.22678\n",
      "[320]\ttrain-mlogloss:0.19369\teval-mlogloss:0.22680\n",
      "[321]\ttrain-mlogloss:0.19354\teval-mlogloss:0.22672\n",
      "[322]\ttrain-mlogloss:0.19335\teval-mlogloss:0.22674\n",
      "[323]\ttrain-mlogloss:0.19316\teval-mlogloss:0.22673\n",
      "[324]\ttrain-mlogloss:0.19306\teval-mlogloss:0.22674\n",
      "[325]\ttrain-mlogloss:0.19290\teval-mlogloss:0.22672\n",
      "[326]\ttrain-mlogloss:0.19278\teval-mlogloss:0.22673\n",
      "[327]\ttrain-mlogloss:0.19265\teval-mlogloss:0.22673\n",
      "[328]\ttrain-mlogloss:0.19256\teval-mlogloss:0.22680\n",
      "[329]\ttrain-mlogloss:0.19242\teval-mlogloss:0.22677\n",
      "[330]\ttrain-mlogloss:0.19229\teval-mlogloss:0.22676\n",
      "[331]\ttrain-mlogloss:0.19215\teval-mlogloss:0.22677\n",
      "[332]\ttrain-mlogloss:0.19207\teval-mlogloss:0.22679\n",
      "[333]\ttrain-mlogloss:0.19195\teval-mlogloss:0.22684\n",
      "[334]\ttrain-mlogloss:0.19181\teval-mlogloss:0.22688\n",
      "[335]\ttrain-mlogloss:0.19163\teval-mlogloss:0.22689\n",
      "[336]\ttrain-mlogloss:0.19149\teval-mlogloss:0.22692\n",
      "[337]\ttrain-mlogloss:0.19133\teval-mlogloss:0.22688\n",
      "[338]\ttrain-mlogloss:0.19113\teval-mlogloss:0.22696\n",
      "[339]\ttrain-mlogloss:0.19106\teval-mlogloss:0.22693\n",
      "[340]\ttrain-mlogloss:0.19089\teval-mlogloss:0.22691\n",
      "[341]\ttrain-mlogloss:0.19080\teval-mlogloss:0.22687\n",
      "[342]\ttrain-mlogloss:0.19071\teval-mlogloss:0.22691\n",
      "[343]\ttrain-mlogloss:0.19063\teval-mlogloss:0.22691\n",
      "[344]\ttrain-mlogloss:0.19055\teval-mlogloss:0.22689\n",
      "[345]\ttrain-mlogloss:0.19043\teval-mlogloss:0.22691\n",
      "[346]\ttrain-mlogloss:0.19035\teval-mlogloss:0.22695\n",
      "[347]\ttrain-mlogloss:0.19023\teval-mlogloss:0.22694\n",
      "[348]\ttrain-mlogloss:0.19002\teval-mlogloss:0.22700\n",
      "xgb now score is: [2.627641465068789]\n",
      "[0]\ttrain-mlogloss:0.67104\teval-mlogloss:0.67108\n",
      "[1]\ttrain-mlogloss:0.65040\teval-mlogloss:0.65047\n",
      "[2]\ttrain-mlogloss:0.63085\teval-mlogloss:0.63095\n",
      "[3]\ttrain-mlogloss:0.61239\teval-mlogloss:0.61251\n",
      "[4]\ttrain-mlogloss:0.59489\teval-mlogloss:0.59505\n",
      "[5]\ttrain-mlogloss:0.57829\teval-mlogloss:0.57858\n",
      "[6]\ttrain-mlogloss:0.56267\teval-mlogloss:0.56297\n",
      "[7]\ttrain-mlogloss:0.54778\teval-mlogloss:0.54811\n",
      "[8]\ttrain-mlogloss:0.53358\teval-mlogloss:0.53394\n",
      "[9]\ttrain-mlogloss:0.52024\teval-mlogloss:0.52059\n",
      "[10]\ttrain-mlogloss:0.50748\teval-mlogloss:0.50800\n",
      "[11]\ttrain-mlogloss:0.49539\teval-mlogloss:0.49605\n",
      "[12]\ttrain-mlogloss:0.48392\teval-mlogloss:0.48467\n",
      "[13]\ttrain-mlogloss:0.47303\teval-mlogloss:0.47382\n",
      "[14]\ttrain-mlogloss:0.46256\teval-mlogloss:0.46341\n",
      "[15]\ttrain-mlogloss:0.45264\teval-mlogloss:0.45352\n",
      "[16]\ttrain-mlogloss:0.44308\teval-mlogloss:0.44406\n",
      "[17]\ttrain-mlogloss:0.43405\teval-mlogloss:0.43512\n",
      "[18]\ttrain-mlogloss:0.42543\teval-mlogloss:0.42649\n",
      "[19]\ttrain-mlogloss:0.41709\teval-mlogloss:0.41831\n",
      "[20]\ttrain-mlogloss:0.40924\teval-mlogloss:0.41060\n",
      "[21]\ttrain-mlogloss:0.40162\teval-mlogloss:0.40324\n",
      "[22]\ttrain-mlogloss:0.39439\teval-mlogloss:0.39614\n",
      "[23]\ttrain-mlogloss:0.38758\teval-mlogloss:0.38947\n",
      "[24]\ttrain-mlogloss:0.38102\teval-mlogloss:0.38301\n",
      "[25]\ttrain-mlogloss:0.37470\teval-mlogloss:0.37678\n",
      "[26]\ttrain-mlogloss:0.36861\teval-mlogloss:0.37084\n",
      "[27]\ttrain-mlogloss:0.36286\teval-mlogloss:0.36526\n",
      "[28]\ttrain-mlogloss:0.35743\teval-mlogloss:0.35986\n",
      "[29]\ttrain-mlogloss:0.35209\teval-mlogloss:0.35471\n",
      "[30]\ttrain-mlogloss:0.34697\teval-mlogloss:0.34984\n",
      "[31]\ttrain-mlogloss:0.34211\teval-mlogloss:0.34510\n",
      "[32]\ttrain-mlogloss:0.33745\teval-mlogloss:0.34053\n",
      "[33]\ttrain-mlogloss:0.33300\teval-mlogloss:0.33624\n",
      "[34]\ttrain-mlogloss:0.32870\teval-mlogloss:0.33213\n",
      "[35]\ttrain-mlogloss:0.32463\teval-mlogloss:0.32818\n",
      "[36]\ttrain-mlogloss:0.32065\teval-mlogloss:0.32448\n",
      "[37]\ttrain-mlogloss:0.31684\teval-mlogloss:0.32090\n",
      "[38]\ttrain-mlogloss:0.31325\teval-mlogloss:0.31749\n",
      "[39]\ttrain-mlogloss:0.30986\teval-mlogloss:0.31419\n",
      "[40]\ttrain-mlogloss:0.30652\teval-mlogloss:0.31106\n",
      "[41]\ttrain-mlogloss:0.30324\teval-mlogloss:0.30800\n",
      "[42]\ttrain-mlogloss:0.30018\teval-mlogloss:0.30510\n",
      "[43]\ttrain-mlogloss:0.29727\teval-mlogloss:0.30233\n",
      "[44]\ttrain-mlogloss:0.29448\teval-mlogloss:0.29982\n",
      "[45]\ttrain-mlogloss:0.29176\teval-mlogloss:0.29725\n",
      "[46]\ttrain-mlogloss:0.28915\teval-mlogloss:0.29485\n",
      "[47]\ttrain-mlogloss:0.28672\teval-mlogloss:0.29267\n",
      "[48]\ttrain-mlogloss:0.28437\teval-mlogloss:0.29059\n",
      "[49]\ttrain-mlogloss:0.28210\teval-mlogloss:0.28846\n",
      "[50]\ttrain-mlogloss:0.27988\teval-mlogloss:0.28647\n",
      "[51]\ttrain-mlogloss:0.27776\teval-mlogloss:0.28463\n",
      "[52]\ttrain-mlogloss:0.27576\teval-mlogloss:0.28275\n",
      "[53]\ttrain-mlogloss:0.27377\teval-mlogloss:0.28097\n",
      "[54]\ttrain-mlogloss:0.27187\teval-mlogloss:0.27931\n",
      "[55]\ttrain-mlogloss:0.27008\teval-mlogloss:0.27772\n",
      "[56]\ttrain-mlogloss:0.26836\teval-mlogloss:0.27620\n",
      "[57]\ttrain-mlogloss:0.26670\teval-mlogloss:0.27474\n",
      "[58]\ttrain-mlogloss:0.26518\teval-mlogloss:0.27333\n",
      "[59]\ttrain-mlogloss:0.26365\teval-mlogloss:0.27199\n",
      "[60]\ttrain-mlogloss:0.26206\teval-mlogloss:0.27080\n",
      "[61]\ttrain-mlogloss:0.26068\teval-mlogloss:0.26963\n",
      "[62]\ttrain-mlogloss:0.25927\teval-mlogloss:0.26847\n",
      "[63]\ttrain-mlogloss:0.25803\teval-mlogloss:0.26736\n",
      "[64]\ttrain-mlogloss:0.25674\teval-mlogloss:0.26634\n",
      "[65]\ttrain-mlogloss:0.25559\teval-mlogloss:0.26534\n",
      "[66]\ttrain-mlogloss:0.25447\teval-mlogloss:0.26443\n",
      "[67]\ttrain-mlogloss:0.25337\teval-mlogloss:0.26357\n",
      "[68]\ttrain-mlogloss:0.25235\teval-mlogloss:0.26272\n",
      "[69]\ttrain-mlogloss:0.25132\teval-mlogloss:0.26193\n",
      "[70]\ttrain-mlogloss:0.25031\teval-mlogloss:0.26118\n",
      "[71]\ttrain-mlogloss:0.24932\teval-mlogloss:0.26044\n",
      "[72]\ttrain-mlogloss:0.24844\teval-mlogloss:0.25975\n",
      "[73]\ttrain-mlogloss:0.24754\teval-mlogloss:0.25909\n",
      "[74]\ttrain-mlogloss:0.24669\teval-mlogloss:0.25844\n",
      "[75]\ttrain-mlogloss:0.24586\teval-mlogloss:0.25787\n",
      "[76]\ttrain-mlogloss:0.24505\teval-mlogloss:0.25724\n",
      "[77]\ttrain-mlogloss:0.24431\teval-mlogloss:0.25672\n",
      "[78]\ttrain-mlogloss:0.24355\teval-mlogloss:0.25619\n",
      "[79]\ttrain-mlogloss:0.24284\teval-mlogloss:0.25570\n",
      "[80]\ttrain-mlogloss:0.24212\teval-mlogloss:0.25515\n",
      "[81]\ttrain-mlogloss:0.24147\teval-mlogloss:0.25473\n",
      "[82]\ttrain-mlogloss:0.24084\teval-mlogloss:0.25430\n",
      "[83]\ttrain-mlogloss:0.24031\teval-mlogloss:0.25385\n",
      "[84]\ttrain-mlogloss:0.23963\teval-mlogloss:0.25339\n",
      "[85]\ttrain-mlogloss:0.23907\teval-mlogloss:0.25304\n",
      "[86]\ttrain-mlogloss:0.23841\teval-mlogloss:0.25274\n",
      "[87]\ttrain-mlogloss:0.23781\teval-mlogloss:0.25245\n",
      "[88]\ttrain-mlogloss:0.23729\teval-mlogloss:0.25211\n",
      "[89]\ttrain-mlogloss:0.23663\teval-mlogloss:0.25184\n",
      "[90]\ttrain-mlogloss:0.23613\teval-mlogloss:0.25157\n",
      "[91]\ttrain-mlogloss:0.23566\teval-mlogloss:0.25126\n",
      "[92]\ttrain-mlogloss:0.23519\teval-mlogloss:0.25096\n",
      "[93]\ttrain-mlogloss:0.23467\teval-mlogloss:0.25073\n",
      "[94]\ttrain-mlogloss:0.23423\teval-mlogloss:0.25054\n",
      "[95]\ttrain-mlogloss:0.23371\teval-mlogloss:0.25032\n",
      "[96]\ttrain-mlogloss:0.23332\teval-mlogloss:0.25009\n",
      "[97]\ttrain-mlogloss:0.23279\teval-mlogloss:0.24991\n",
      "[98]\ttrain-mlogloss:0.23236\teval-mlogloss:0.24971\n",
      "[99]\ttrain-mlogloss:0.23200\teval-mlogloss:0.24945\n",
      "[100]\ttrain-mlogloss:0.23152\teval-mlogloss:0.24936\n",
      "[101]\ttrain-mlogloss:0.23114\teval-mlogloss:0.24929\n",
      "[102]\ttrain-mlogloss:0.23080\teval-mlogloss:0.24918\n",
      "[103]\ttrain-mlogloss:0.23040\teval-mlogloss:0.24906\n",
      "[104]\ttrain-mlogloss:0.23005\teval-mlogloss:0.24886\n",
      "[105]\ttrain-mlogloss:0.22971\teval-mlogloss:0.24862\n",
      "[106]\ttrain-mlogloss:0.22930\teval-mlogloss:0.24851\n",
      "[107]\ttrain-mlogloss:0.22880\teval-mlogloss:0.24846\n",
      "[108]\ttrain-mlogloss:0.22853\teval-mlogloss:0.24833\n",
      "[109]\ttrain-mlogloss:0.22820\teval-mlogloss:0.24821\n",
      "[110]\ttrain-mlogloss:0.22783\teval-mlogloss:0.24809\n",
      "[111]\ttrain-mlogloss:0.22747\teval-mlogloss:0.24796\n",
      "[112]\ttrain-mlogloss:0.22702\teval-mlogloss:0.24787\n",
      "[113]\ttrain-mlogloss:0.22673\teval-mlogloss:0.24778\n",
      "[114]\ttrain-mlogloss:0.22644\teval-mlogloss:0.24771\n",
      "[115]\ttrain-mlogloss:0.22609\teval-mlogloss:0.24760\n",
      "[116]\ttrain-mlogloss:0.22570\teval-mlogloss:0.24752\n",
      "[117]\ttrain-mlogloss:0.22534\teval-mlogloss:0.24742\n",
      "[118]\ttrain-mlogloss:0.22499\teval-mlogloss:0.24742\n",
      "[119]\ttrain-mlogloss:0.22466\teval-mlogloss:0.24739\n",
      "[120]\ttrain-mlogloss:0.22437\teval-mlogloss:0.24728\n",
      "[121]\ttrain-mlogloss:0.22403\teval-mlogloss:0.24726\n",
      "[122]\ttrain-mlogloss:0.22375\teval-mlogloss:0.24717\n",
      "[123]\ttrain-mlogloss:0.22344\teval-mlogloss:0.24710\n",
      "[124]\ttrain-mlogloss:0.22322\teval-mlogloss:0.24712\n",
      "[125]\ttrain-mlogloss:0.22298\teval-mlogloss:0.24708\n",
      "[126]\ttrain-mlogloss:0.22269\teval-mlogloss:0.24707\n",
      "[127]\ttrain-mlogloss:0.22235\teval-mlogloss:0.24698\n",
      "[128]\ttrain-mlogloss:0.22208\teval-mlogloss:0.24693\n",
      "[129]\ttrain-mlogloss:0.22181\teval-mlogloss:0.24690\n",
      "[130]\ttrain-mlogloss:0.22158\teval-mlogloss:0.24693\n",
      "[131]\ttrain-mlogloss:0.22131\teval-mlogloss:0.24691\n",
      "[132]\ttrain-mlogloss:0.22102\teval-mlogloss:0.24688\n",
      "[133]\ttrain-mlogloss:0.22079\teval-mlogloss:0.24683\n",
      "[134]\ttrain-mlogloss:0.22045\teval-mlogloss:0.24685\n",
      "[135]\ttrain-mlogloss:0.22024\teval-mlogloss:0.24680\n",
      "[136]\ttrain-mlogloss:0.21998\teval-mlogloss:0.24680\n",
      "[137]\ttrain-mlogloss:0.21974\teval-mlogloss:0.24682\n",
      "[138]\ttrain-mlogloss:0.21945\teval-mlogloss:0.24679\n",
      "[139]\ttrain-mlogloss:0.21931\teval-mlogloss:0.24683\n",
      "[140]\ttrain-mlogloss:0.21909\teval-mlogloss:0.24681\n",
      "[141]\ttrain-mlogloss:0.21883\teval-mlogloss:0.24676\n",
      "[142]\ttrain-mlogloss:0.21857\teval-mlogloss:0.24679\n",
      "[143]\ttrain-mlogloss:0.21837\teval-mlogloss:0.24677\n",
      "[144]\ttrain-mlogloss:0.21809\teval-mlogloss:0.24684\n",
      "[145]\ttrain-mlogloss:0.21773\teval-mlogloss:0.24687\n",
      "[146]\ttrain-mlogloss:0.21740\teval-mlogloss:0.24685\n",
      "[147]\ttrain-mlogloss:0.21720\teval-mlogloss:0.24683\n",
      "[148]\ttrain-mlogloss:0.21695\teval-mlogloss:0.24680\n",
      "[149]\ttrain-mlogloss:0.21670\teval-mlogloss:0.24680\n",
      "[150]\ttrain-mlogloss:0.21652\teval-mlogloss:0.24683\n",
      "[151]\ttrain-mlogloss:0.21638\teval-mlogloss:0.24684\n",
      "[152]\ttrain-mlogloss:0.21622\teval-mlogloss:0.24682\n",
      "[153]\ttrain-mlogloss:0.21595\teval-mlogloss:0.24677\n",
      "[154]\ttrain-mlogloss:0.21562\teval-mlogloss:0.24682\n",
      "[155]\ttrain-mlogloss:0.21532\teval-mlogloss:0.24685\n",
      "[156]\ttrain-mlogloss:0.21513\teval-mlogloss:0.24684\n",
      "[157]\ttrain-mlogloss:0.21491\teval-mlogloss:0.24684\n",
      "[158]\ttrain-mlogloss:0.21465\teval-mlogloss:0.24687\n",
      "[159]\ttrain-mlogloss:0.21438\teval-mlogloss:0.24684\n",
      "[160]\ttrain-mlogloss:0.21413\teval-mlogloss:0.24683\n",
      "[161]\ttrain-mlogloss:0.21396\teval-mlogloss:0.24681\n",
      "[162]\ttrain-mlogloss:0.21366\teval-mlogloss:0.24678\n",
      "[163]\ttrain-mlogloss:0.21336\teval-mlogloss:0.24671\n",
      "[164]\ttrain-mlogloss:0.21320\teval-mlogloss:0.24678\n",
      "[165]\ttrain-mlogloss:0.21301\teval-mlogloss:0.24678\n",
      "[166]\ttrain-mlogloss:0.21284\teval-mlogloss:0.24684\n",
      "[167]\ttrain-mlogloss:0.21260\teval-mlogloss:0.24690\n",
      "[168]\ttrain-mlogloss:0.21246\teval-mlogloss:0.24689\n",
      "[169]\ttrain-mlogloss:0.21220\teval-mlogloss:0.24690\n",
      "[170]\ttrain-mlogloss:0.21192\teval-mlogloss:0.24686\n",
      "[171]\ttrain-mlogloss:0.21164\teval-mlogloss:0.24690\n",
      "[172]\ttrain-mlogloss:0.21135\teval-mlogloss:0.24690\n",
      "[173]\ttrain-mlogloss:0.21106\teval-mlogloss:0.24698\n",
      "[174]\ttrain-mlogloss:0.21085\teval-mlogloss:0.24706\n",
      "[175]\ttrain-mlogloss:0.21074\teval-mlogloss:0.24705\n",
      "[176]\ttrain-mlogloss:0.21060\teval-mlogloss:0.24713\n",
      "[177]\ttrain-mlogloss:0.21035\teval-mlogloss:0.24714\n",
      "[178]\ttrain-mlogloss:0.21017\teval-mlogloss:0.24717\n",
      "[179]\ttrain-mlogloss:0.20994\teval-mlogloss:0.24723\n",
      "[180]\ttrain-mlogloss:0.20974\teval-mlogloss:0.24729\n",
      "[181]\ttrain-mlogloss:0.20954\teval-mlogloss:0.24727\n",
      "[182]\ttrain-mlogloss:0.20934\teval-mlogloss:0.24727\n",
      "[183]\ttrain-mlogloss:0.20917\teval-mlogloss:0.24729\n",
      "[184]\ttrain-mlogloss:0.20899\teval-mlogloss:0.24729\n",
      "[185]\ttrain-mlogloss:0.20877\teval-mlogloss:0.24730\n",
      "[186]\ttrain-mlogloss:0.20861\teval-mlogloss:0.24734\n",
      "[187]\ttrain-mlogloss:0.20843\teval-mlogloss:0.24732\n",
      "[188]\ttrain-mlogloss:0.20831\teval-mlogloss:0.24739\n",
      "[189]\ttrain-mlogloss:0.20817\teval-mlogloss:0.24738\n",
      "[190]\ttrain-mlogloss:0.20793\teval-mlogloss:0.24739\n",
      "[191]\ttrain-mlogloss:0.20769\teval-mlogloss:0.24743\n",
      "[192]\ttrain-mlogloss:0.20745\teval-mlogloss:0.24750\n",
      "[193]\ttrain-mlogloss:0.20729\teval-mlogloss:0.24752\n",
      "[194]\ttrain-mlogloss:0.20717\teval-mlogloss:0.24755\n",
      "[195]\ttrain-mlogloss:0.20699\teval-mlogloss:0.24759\n",
      "[196]\ttrain-mlogloss:0.20678\teval-mlogloss:0.24757\n",
      "[197]\ttrain-mlogloss:0.20658\teval-mlogloss:0.24772\n",
      "[198]\ttrain-mlogloss:0.20632\teval-mlogloss:0.24777\n",
      "[199]\ttrain-mlogloss:0.20619\teval-mlogloss:0.24776\n",
      "[200]\ttrain-mlogloss:0.20601\teval-mlogloss:0.24776\n",
      "[201]\ttrain-mlogloss:0.20579\teval-mlogloss:0.24782\n",
      "[202]\ttrain-mlogloss:0.20566\teval-mlogloss:0.24783\n",
      "[203]\ttrain-mlogloss:0.20544\teval-mlogloss:0.24781\n",
      "[204]\ttrain-mlogloss:0.20521\teval-mlogloss:0.24790\n",
      "[205]\ttrain-mlogloss:0.20503\teval-mlogloss:0.24789\n",
      "[206]\ttrain-mlogloss:0.20486\teval-mlogloss:0.24786\n",
      "[207]\ttrain-mlogloss:0.20467\teval-mlogloss:0.24786\n",
      "[208]\ttrain-mlogloss:0.20448\teval-mlogloss:0.24787\n",
      "[209]\ttrain-mlogloss:0.20421\teval-mlogloss:0.24791\n",
      "[210]\ttrain-mlogloss:0.20407\teval-mlogloss:0.24795\n",
      "[211]\ttrain-mlogloss:0.20396\teval-mlogloss:0.24800\n",
      "[212]\ttrain-mlogloss:0.20370\teval-mlogloss:0.24804\n",
      "[213]\ttrain-mlogloss:0.20352\teval-mlogloss:0.24799\n",
      "[214]\ttrain-mlogloss:0.20331\teval-mlogloss:0.24802\n",
      "[215]\ttrain-mlogloss:0.20316\teval-mlogloss:0.24805\n",
      "[216]\ttrain-mlogloss:0.20292\teval-mlogloss:0.24794\n",
      "[217]\ttrain-mlogloss:0.20271\teval-mlogloss:0.24795\n",
      "[218]\ttrain-mlogloss:0.20252\teval-mlogloss:0.24797\n",
      "[219]\ttrain-mlogloss:0.20235\teval-mlogloss:0.24795\n",
      "[220]\ttrain-mlogloss:0.20216\teval-mlogloss:0.24803\n",
      "[221]\ttrain-mlogloss:0.20196\teval-mlogloss:0.24807\n",
      "[222]\ttrain-mlogloss:0.20174\teval-mlogloss:0.24812\n",
      "[223]\ttrain-mlogloss:0.20151\teval-mlogloss:0.24806\n",
      "[224]\ttrain-mlogloss:0.20134\teval-mlogloss:0.24809\n",
      "[225]\ttrain-mlogloss:0.20116\teval-mlogloss:0.24814\n",
      "[226]\ttrain-mlogloss:0.20094\teval-mlogloss:0.24814\n",
      "[227]\ttrain-mlogloss:0.20073\teval-mlogloss:0.24815\n",
      "[228]\ttrain-mlogloss:0.20058\teval-mlogloss:0.24815\n",
      "[229]\ttrain-mlogloss:0.20040\teval-mlogloss:0.24813\n",
      "[230]\ttrain-mlogloss:0.20022\teval-mlogloss:0.24805\n",
      "[231]\ttrain-mlogloss:0.20003\teval-mlogloss:0.24804\n",
      "[232]\ttrain-mlogloss:0.19986\teval-mlogloss:0.24809\n",
      "[233]\ttrain-mlogloss:0.19966\teval-mlogloss:0.24811\n",
      "[234]\ttrain-mlogloss:0.19947\teval-mlogloss:0.24813\n",
      "[235]\ttrain-mlogloss:0.19914\teval-mlogloss:0.24817\n",
      "[236]\ttrain-mlogloss:0.19897\teval-mlogloss:0.24815\n",
      "[237]\ttrain-mlogloss:0.19884\teval-mlogloss:0.24816\n",
      "[238]\ttrain-mlogloss:0.19865\teval-mlogloss:0.24804\n",
      "[239]\ttrain-mlogloss:0.19853\teval-mlogloss:0.24807\n",
      "[240]\ttrain-mlogloss:0.19826\teval-mlogloss:0.24809\n",
      "[241]\ttrain-mlogloss:0.19812\teval-mlogloss:0.24814\n",
      "[242]\ttrain-mlogloss:0.19791\teval-mlogloss:0.24819\n",
      "[243]\ttrain-mlogloss:0.19783\teval-mlogloss:0.24819\n",
      "[244]\ttrain-mlogloss:0.19769\teval-mlogloss:0.24821\n",
      "[245]\ttrain-mlogloss:0.19755\teval-mlogloss:0.24820\n",
      "[246]\ttrain-mlogloss:0.19734\teval-mlogloss:0.24823\n",
      "[247]\ttrain-mlogloss:0.19718\teval-mlogloss:0.24830\n",
      "[248]\ttrain-mlogloss:0.19696\teval-mlogloss:0.24830\n",
      "[249]\ttrain-mlogloss:0.19670\teval-mlogloss:0.24834\n",
      "[250]\ttrain-mlogloss:0.19657\teval-mlogloss:0.24835\n",
      "[251]\ttrain-mlogloss:0.19641\teval-mlogloss:0.24835\n",
      "[252]\ttrain-mlogloss:0.19619\teval-mlogloss:0.24828\n",
      "[253]\ttrain-mlogloss:0.19601\teval-mlogloss:0.24831\n",
      "[254]\ttrain-mlogloss:0.19585\teval-mlogloss:0.24834\n",
      "[255]\ttrain-mlogloss:0.19572\teval-mlogloss:0.24826\n",
      "[256]\ttrain-mlogloss:0.19552\teval-mlogloss:0.24822\n",
      "[257]\ttrain-mlogloss:0.19537\teval-mlogloss:0.24823\n",
      "[258]\ttrain-mlogloss:0.19521\teval-mlogloss:0.24826\n",
      "[259]\ttrain-mlogloss:0.19499\teval-mlogloss:0.24831\n",
      "[260]\ttrain-mlogloss:0.19492\teval-mlogloss:0.24831\n",
      "[261]\ttrain-mlogloss:0.19475\teval-mlogloss:0.24833\n",
      "[262]\ttrain-mlogloss:0.19453\teval-mlogloss:0.24833\n",
      "[263]\ttrain-mlogloss:0.19435\teval-mlogloss:0.24838\n",
      "xgb now score is: [2.627641465068789, 2.5421163955938]\n",
      "[0]\ttrain-mlogloss:0.67117\teval-mlogloss:0.67102\n",
      "[1]\ttrain-mlogloss:0.65043\teval-mlogloss:0.65009\n",
      "[2]\ttrain-mlogloss:0.63095\teval-mlogloss:0.63046\n",
      "[3]\ttrain-mlogloss:0.61251\teval-mlogloss:0.61184\n",
      "[4]\ttrain-mlogloss:0.59500\teval-mlogloss:0.59420\n",
      "[5]\ttrain-mlogloss:0.57850\teval-mlogloss:0.57759\n",
      "[6]\ttrain-mlogloss:0.56298\teval-mlogloss:0.56190\n",
      "[7]\ttrain-mlogloss:0.54819\teval-mlogloss:0.54701\n",
      "[8]\ttrain-mlogloss:0.53410\teval-mlogloss:0.53289\n",
      "[9]\ttrain-mlogloss:0.52079\teval-mlogloss:0.51947\n",
      "[10]\ttrain-mlogloss:0.50815\teval-mlogloss:0.50674\n",
      "[11]\ttrain-mlogloss:0.49610\teval-mlogloss:0.49466\n",
      "[12]\ttrain-mlogloss:0.48468\teval-mlogloss:0.48317\n",
      "[13]\ttrain-mlogloss:0.47370\teval-mlogloss:0.47212\n",
      "[14]\ttrain-mlogloss:0.46330\teval-mlogloss:0.46168\n",
      "[15]\ttrain-mlogloss:0.45336\teval-mlogloss:0.45168\n",
      "[16]\ttrain-mlogloss:0.44396\teval-mlogloss:0.44216\n",
      "[17]\ttrain-mlogloss:0.43503\teval-mlogloss:0.43320\n",
      "[18]\ttrain-mlogloss:0.42652\teval-mlogloss:0.42458\n",
      "[19]\ttrain-mlogloss:0.41832\teval-mlogloss:0.41630\n",
      "[20]\ttrain-mlogloss:0.41054\teval-mlogloss:0.40843\n",
      "[21]\ttrain-mlogloss:0.40313\teval-mlogloss:0.40093\n",
      "[22]\ttrain-mlogloss:0.39609\teval-mlogloss:0.39386\n",
      "[23]\ttrain-mlogloss:0.38920\teval-mlogloss:0.38691\n",
      "[24]\ttrain-mlogloss:0.38268\teval-mlogloss:0.38034\n",
      "[25]\ttrain-mlogloss:0.37646\teval-mlogloss:0.37409\n",
      "[26]\ttrain-mlogloss:0.37050\teval-mlogloss:0.36809\n",
      "[27]\ttrain-mlogloss:0.36480\teval-mlogloss:0.36239\n",
      "[28]\ttrain-mlogloss:0.35935\teval-mlogloss:0.35697\n",
      "[29]\ttrain-mlogloss:0.35406\teval-mlogloss:0.35160\n",
      "[30]\ttrain-mlogloss:0.34911\teval-mlogloss:0.34664\n",
      "[31]\ttrain-mlogloss:0.34423\teval-mlogloss:0.34181\n",
      "[32]\ttrain-mlogloss:0.33967\teval-mlogloss:0.33729\n",
      "[33]\ttrain-mlogloss:0.33519\teval-mlogloss:0.33289\n",
      "[34]\ttrain-mlogloss:0.33094\teval-mlogloss:0.32874\n",
      "[35]\ttrain-mlogloss:0.32689\teval-mlogloss:0.32476\n",
      "[36]\ttrain-mlogloss:0.32302\teval-mlogloss:0.32092\n",
      "[37]\ttrain-mlogloss:0.31923\teval-mlogloss:0.31723\n",
      "[38]\ttrain-mlogloss:0.31565\teval-mlogloss:0.31369\n",
      "[39]\ttrain-mlogloss:0.31226\teval-mlogloss:0.31030\n",
      "[40]\ttrain-mlogloss:0.30909\teval-mlogloss:0.30718\n",
      "[41]\ttrain-mlogloss:0.30600\teval-mlogloss:0.30417\n",
      "[42]\ttrain-mlogloss:0.30293\teval-mlogloss:0.30119\n",
      "[43]\ttrain-mlogloss:0.29992\teval-mlogloss:0.29825\n",
      "[44]\ttrain-mlogloss:0.29717\teval-mlogloss:0.29552\n",
      "[45]\ttrain-mlogloss:0.29455\teval-mlogloss:0.29290\n",
      "[46]\ttrain-mlogloss:0.29197\teval-mlogloss:0.29050\n",
      "[47]\ttrain-mlogloss:0.28959\teval-mlogloss:0.28809\n",
      "[48]\ttrain-mlogloss:0.28721\teval-mlogloss:0.28577\n",
      "[49]\ttrain-mlogloss:0.28500\teval-mlogloss:0.28361\n",
      "[50]\ttrain-mlogloss:0.28280\teval-mlogloss:0.28146\n",
      "[51]\ttrain-mlogloss:0.28076\teval-mlogloss:0.27949\n",
      "[52]\ttrain-mlogloss:0.27875\teval-mlogloss:0.27755\n",
      "[53]\ttrain-mlogloss:0.27678\teval-mlogloss:0.27568\n",
      "[54]\ttrain-mlogloss:0.27494\teval-mlogloss:0.27392\n",
      "[55]\ttrain-mlogloss:0.27323\teval-mlogloss:0.27230\n",
      "[56]\ttrain-mlogloss:0.27154\teval-mlogloss:0.27071\n",
      "[57]\ttrain-mlogloss:0.26995\teval-mlogloss:0.26916\n",
      "[58]\ttrain-mlogloss:0.26834\teval-mlogloss:0.26768\n",
      "[59]\ttrain-mlogloss:0.26691\teval-mlogloss:0.26636\n",
      "[60]\ttrain-mlogloss:0.26550\teval-mlogloss:0.26510\n",
      "[61]\ttrain-mlogloss:0.26412\teval-mlogloss:0.26392\n",
      "[62]\ttrain-mlogloss:0.26273\teval-mlogloss:0.26268\n",
      "[63]\ttrain-mlogloss:0.26144\teval-mlogloss:0.26151\n",
      "[64]\ttrain-mlogloss:0.26021\teval-mlogloss:0.26046\n",
      "[65]\ttrain-mlogloss:0.25912\teval-mlogloss:0.25944\n",
      "[66]\ttrain-mlogloss:0.25789\teval-mlogloss:0.25833\n",
      "[67]\ttrain-mlogloss:0.25678\teval-mlogloss:0.25742\n",
      "[68]\ttrain-mlogloss:0.25577\teval-mlogloss:0.25655\n",
      "[69]\ttrain-mlogloss:0.25483\teval-mlogloss:0.25571\n",
      "[70]\ttrain-mlogloss:0.25382\teval-mlogloss:0.25481\n",
      "[71]\ttrain-mlogloss:0.25291\teval-mlogloss:0.25401\n",
      "[72]\ttrain-mlogloss:0.25193\teval-mlogloss:0.25319\n",
      "[73]\ttrain-mlogloss:0.25093\teval-mlogloss:0.25241\n",
      "[74]\ttrain-mlogloss:0.25006\teval-mlogloss:0.25179\n",
      "[75]\ttrain-mlogloss:0.24923\teval-mlogloss:0.25113\n",
      "[76]\ttrain-mlogloss:0.24842\teval-mlogloss:0.25050\n",
      "[77]\ttrain-mlogloss:0.24760\teval-mlogloss:0.24992\n",
      "[78]\ttrain-mlogloss:0.24687\teval-mlogloss:0.24926\n",
      "[79]\ttrain-mlogloss:0.24615\teval-mlogloss:0.24867\n",
      "[80]\ttrain-mlogloss:0.24551\teval-mlogloss:0.24814\n",
      "[81]\ttrain-mlogloss:0.24478\teval-mlogloss:0.24767\n",
      "[82]\ttrain-mlogloss:0.24421\teval-mlogloss:0.24723\n",
      "[83]\ttrain-mlogloss:0.24356\teval-mlogloss:0.24681\n",
      "[84]\ttrain-mlogloss:0.24297\teval-mlogloss:0.24638\n",
      "[85]\ttrain-mlogloss:0.24238\teval-mlogloss:0.24603\n",
      "[86]\ttrain-mlogloss:0.24181\teval-mlogloss:0.24555\n",
      "[87]\ttrain-mlogloss:0.24129\teval-mlogloss:0.24522\n",
      "[88]\ttrain-mlogloss:0.24075\teval-mlogloss:0.24478\n",
      "[89]\ttrain-mlogloss:0.24013\teval-mlogloss:0.24450\n",
      "[90]\ttrain-mlogloss:0.23962\teval-mlogloss:0.24417\n",
      "[91]\ttrain-mlogloss:0.23914\teval-mlogloss:0.24385\n",
      "[92]\ttrain-mlogloss:0.23862\teval-mlogloss:0.24353\n",
      "[93]\ttrain-mlogloss:0.23820\teval-mlogloss:0.24323\n",
      "[94]\ttrain-mlogloss:0.23780\teval-mlogloss:0.24296\n",
      "[95]\ttrain-mlogloss:0.23729\teval-mlogloss:0.24267\n",
      "[96]\ttrain-mlogloss:0.23690\teval-mlogloss:0.24240\n",
      "[97]\ttrain-mlogloss:0.23643\teval-mlogloss:0.24219\n",
      "[98]\ttrain-mlogloss:0.23604\teval-mlogloss:0.24202\n",
      "[99]\ttrain-mlogloss:0.23561\teval-mlogloss:0.24184\n",
      "[100]\ttrain-mlogloss:0.23519\teval-mlogloss:0.24164\n",
      "[101]\ttrain-mlogloss:0.23479\teval-mlogloss:0.24141\n",
      "[102]\ttrain-mlogloss:0.23438\teval-mlogloss:0.24123\n",
      "[103]\ttrain-mlogloss:0.23401\teval-mlogloss:0.24111\n",
      "[104]\ttrain-mlogloss:0.23361\teval-mlogloss:0.24101\n",
      "[105]\ttrain-mlogloss:0.23332\teval-mlogloss:0.24092\n",
      "[106]\ttrain-mlogloss:0.23303\teval-mlogloss:0.24080\n",
      "[107]\ttrain-mlogloss:0.23267\teval-mlogloss:0.24064\n",
      "[108]\ttrain-mlogloss:0.23236\teval-mlogloss:0.24052\n",
      "[109]\ttrain-mlogloss:0.23200\teval-mlogloss:0.24043\n",
      "[110]\ttrain-mlogloss:0.23163\teval-mlogloss:0.24042\n",
      "[111]\ttrain-mlogloss:0.23124\teval-mlogloss:0.24032\n",
      "[112]\ttrain-mlogloss:0.23099\teval-mlogloss:0.24024\n",
      "[113]\ttrain-mlogloss:0.23075\teval-mlogloss:0.24014\n",
      "[114]\ttrain-mlogloss:0.23034\teval-mlogloss:0.24011\n",
      "[115]\ttrain-mlogloss:0.23008\teval-mlogloss:0.24000\n",
      "[116]\ttrain-mlogloss:0.22976\teval-mlogloss:0.23991\n",
      "[117]\ttrain-mlogloss:0.22948\teval-mlogloss:0.23975\n",
      "[118]\ttrain-mlogloss:0.22913\teval-mlogloss:0.23968\n",
      "[119]\ttrain-mlogloss:0.22891\teval-mlogloss:0.23964\n",
      "[120]\ttrain-mlogloss:0.22870\teval-mlogloss:0.23966\n",
      "[121]\ttrain-mlogloss:0.22839\teval-mlogloss:0.23956\n",
      "[122]\ttrain-mlogloss:0.22805\teval-mlogloss:0.23946\n",
      "[123]\ttrain-mlogloss:0.22775\teval-mlogloss:0.23939\n",
      "[124]\ttrain-mlogloss:0.22750\teval-mlogloss:0.23931\n",
      "[125]\ttrain-mlogloss:0.22727\teval-mlogloss:0.23932\n",
      "[126]\ttrain-mlogloss:0.22704\teval-mlogloss:0.23928\n",
      "[127]\ttrain-mlogloss:0.22684\teval-mlogloss:0.23925\n",
      "[128]\ttrain-mlogloss:0.22663\teval-mlogloss:0.23923\n",
      "[129]\ttrain-mlogloss:0.22639\teval-mlogloss:0.23922\n",
      "[130]\ttrain-mlogloss:0.22606\teval-mlogloss:0.23912\n",
      "[131]\ttrain-mlogloss:0.22579\teval-mlogloss:0.23903\n",
      "[132]\ttrain-mlogloss:0.22558\teval-mlogloss:0.23897\n",
      "[133]\ttrain-mlogloss:0.22532\teval-mlogloss:0.23898\n",
      "[134]\ttrain-mlogloss:0.22503\teval-mlogloss:0.23902\n",
      "[135]\ttrain-mlogloss:0.22481\teval-mlogloss:0.23904\n",
      "[136]\ttrain-mlogloss:0.22455\teval-mlogloss:0.23904\n",
      "[137]\ttrain-mlogloss:0.22432\teval-mlogloss:0.23899\n",
      "[138]\ttrain-mlogloss:0.22408\teval-mlogloss:0.23899\n",
      "[139]\ttrain-mlogloss:0.22389\teval-mlogloss:0.23897\n",
      "[140]\ttrain-mlogloss:0.22373\teval-mlogloss:0.23903\n",
      "[141]\ttrain-mlogloss:0.22343\teval-mlogloss:0.23897\n",
      "[142]\ttrain-mlogloss:0.22317\teval-mlogloss:0.23895\n",
      "[143]\ttrain-mlogloss:0.22298\teval-mlogloss:0.23899\n",
      "[144]\ttrain-mlogloss:0.22272\teval-mlogloss:0.23895\n",
      "[145]\ttrain-mlogloss:0.22240\teval-mlogloss:0.23892\n",
      "[146]\ttrain-mlogloss:0.22217\teval-mlogloss:0.23893\n",
      "[147]\ttrain-mlogloss:0.22192\teval-mlogloss:0.23899\n",
      "[148]\ttrain-mlogloss:0.22167\teval-mlogloss:0.23902\n",
      "[149]\ttrain-mlogloss:0.22146\teval-mlogloss:0.23899\n",
      "[150]\ttrain-mlogloss:0.22128\teval-mlogloss:0.23902\n",
      "[151]\ttrain-mlogloss:0.22113\teval-mlogloss:0.23911\n",
      "[152]\ttrain-mlogloss:0.22089\teval-mlogloss:0.23912\n",
      "[153]\ttrain-mlogloss:0.22063\teval-mlogloss:0.23910\n",
      "[154]\ttrain-mlogloss:0.22043\teval-mlogloss:0.23914\n",
      "[155]\ttrain-mlogloss:0.22025\teval-mlogloss:0.23915\n",
      "[156]\ttrain-mlogloss:0.22005\teval-mlogloss:0.23927\n",
      "[157]\ttrain-mlogloss:0.21974\teval-mlogloss:0.23933\n",
      "[158]\ttrain-mlogloss:0.21955\teval-mlogloss:0.23934\n",
      "[159]\ttrain-mlogloss:0.21932\teval-mlogloss:0.23940\n",
      "[160]\ttrain-mlogloss:0.21912\teval-mlogloss:0.23937\n",
      "[161]\ttrain-mlogloss:0.21884\teval-mlogloss:0.23938\n",
      "[162]\ttrain-mlogloss:0.21856\teval-mlogloss:0.23949\n",
      "[163]\ttrain-mlogloss:0.21830\teval-mlogloss:0.23953\n",
      "[164]\ttrain-mlogloss:0.21800\teval-mlogloss:0.23961\n",
      "[165]\ttrain-mlogloss:0.21780\teval-mlogloss:0.23964\n",
      "[166]\ttrain-mlogloss:0.21765\teval-mlogloss:0.23962\n",
      "[167]\ttrain-mlogloss:0.21732\teval-mlogloss:0.23964\n",
      "[168]\ttrain-mlogloss:0.21708\teval-mlogloss:0.23975\n",
      "[169]\ttrain-mlogloss:0.21695\teval-mlogloss:0.23980\n",
      "[170]\ttrain-mlogloss:0.21672\teval-mlogloss:0.23984\n",
      "[171]\ttrain-mlogloss:0.21652\teval-mlogloss:0.23992\n",
      "[172]\ttrain-mlogloss:0.21634\teval-mlogloss:0.23994\n",
      "[173]\ttrain-mlogloss:0.21613\teval-mlogloss:0.23998\n",
      "[174]\ttrain-mlogloss:0.21595\teval-mlogloss:0.24007\n",
      "[175]\ttrain-mlogloss:0.21563\teval-mlogloss:0.24017\n",
      "[176]\ttrain-mlogloss:0.21549\teval-mlogloss:0.24018\n",
      "[177]\ttrain-mlogloss:0.21527\teval-mlogloss:0.24024\n",
      "[178]\ttrain-mlogloss:0.21503\teval-mlogloss:0.24018\n",
      "[179]\ttrain-mlogloss:0.21482\teval-mlogloss:0.24021\n",
      "[180]\ttrain-mlogloss:0.21459\teval-mlogloss:0.24023\n",
      "[181]\ttrain-mlogloss:0.21436\teval-mlogloss:0.24028\n",
      "[182]\ttrain-mlogloss:0.21407\teval-mlogloss:0.24035\n",
      "[183]\ttrain-mlogloss:0.21380\teval-mlogloss:0.24039\n",
      "[184]\ttrain-mlogloss:0.21356\teval-mlogloss:0.24043\n",
      "[185]\ttrain-mlogloss:0.21337\teval-mlogloss:0.24049\n",
      "[186]\ttrain-mlogloss:0.21320\teval-mlogloss:0.24065\n",
      "[187]\ttrain-mlogloss:0.21293\teval-mlogloss:0.24072\n",
      "[188]\ttrain-mlogloss:0.21268\teval-mlogloss:0.24077\n",
      "[189]\ttrain-mlogloss:0.21252\teval-mlogloss:0.24080\n",
      "[190]\ttrain-mlogloss:0.21234\teval-mlogloss:0.24083\n",
      "[191]\ttrain-mlogloss:0.21212\teval-mlogloss:0.24084\n",
      "[192]\ttrain-mlogloss:0.21199\teval-mlogloss:0.24085\n",
      "[193]\ttrain-mlogloss:0.21181\teval-mlogloss:0.24084\n",
      "[194]\ttrain-mlogloss:0.21156\teval-mlogloss:0.24092\n",
      "[195]\ttrain-mlogloss:0.21144\teval-mlogloss:0.24097\n",
      "[196]\ttrain-mlogloss:0.21116\teval-mlogloss:0.24112\n",
      "[197]\ttrain-mlogloss:0.21092\teval-mlogloss:0.24108\n",
      "[198]\ttrain-mlogloss:0.21071\teval-mlogloss:0.24117\n",
      "[199]\ttrain-mlogloss:0.21050\teval-mlogloss:0.24120\n",
      "[200]\ttrain-mlogloss:0.21030\teval-mlogloss:0.24126\n",
      "[201]\ttrain-mlogloss:0.21007\teval-mlogloss:0.24128\n",
      "[202]\ttrain-mlogloss:0.20994\teval-mlogloss:0.24132\n",
      "[203]\ttrain-mlogloss:0.20970\teval-mlogloss:0.24141\n",
      "[204]\ttrain-mlogloss:0.20950\teval-mlogloss:0.24148\n",
      "[205]\ttrain-mlogloss:0.20934\teval-mlogloss:0.24150\n",
      "[206]\ttrain-mlogloss:0.20908\teval-mlogloss:0.24148\n",
      "[207]\ttrain-mlogloss:0.20884\teval-mlogloss:0.24144\n",
      "[208]\ttrain-mlogloss:0.20866\teval-mlogloss:0.24143\n",
      "[209]\ttrain-mlogloss:0.20842\teval-mlogloss:0.24143\n",
      "[210]\ttrain-mlogloss:0.20823\teval-mlogloss:0.24148\n",
      "[211]\ttrain-mlogloss:0.20797\teval-mlogloss:0.24144\n",
      "[212]\ttrain-mlogloss:0.20777\teval-mlogloss:0.24143\n",
      "[213]\ttrain-mlogloss:0.20767\teval-mlogloss:0.24154\n",
      "[214]\ttrain-mlogloss:0.20748\teval-mlogloss:0.24155\n",
      "[215]\ttrain-mlogloss:0.20735\teval-mlogloss:0.24159\n",
      "[216]\ttrain-mlogloss:0.20711\teval-mlogloss:0.24160\n",
      "[217]\ttrain-mlogloss:0.20696\teval-mlogloss:0.24161\n",
      "[218]\ttrain-mlogloss:0.20681\teval-mlogloss:0.24164\n",
      "[219]\ttrain-mlogloss:0.20661\teval-mlogloss:0.24172\n",
      "[220]\ttrain-mlogloss:0.20640\teval-mlogloss:0.24180\n",
      "[221]\ttrain-mlogloss:0.20623\teval-mlogloss:0.24187\n",
      "[222]\ttrain-mlogloss:0.20609\teval-mlogloss:0.24191\n",
      "[223]\ttrain-mlogloss:0.20600\teval-mlogloss:0.24198\n",
      "[224]\ttrain-mlogloss:0.20583\teval-mlogloss:0.24204\n",
      "[225]\ttrain-mlogloss:0.20571\teval-mlogloss:0.24213\n",
      "[226]\ttrain-mlogloss:0.20550\teval-mlogloss:0.24211\n",
      "[227]\ttrain-mlogloss:0.20529\teval-mlogloss:0.24208\n",
      "[228]\ttrain-mlogloss:0.20518\teval-mlogloss:0.24208\n",
      "[229]\ttrain-mlogloss:0.20495\teval-mlogloss:0.24217\n",
      "[230]\ttrain-mlogloss:0.20478\teval-mlogloss:0.24218\n",
      "[231]\ttrain-mlogloss:0.20451\teval-mlogloss:0.24214\n",
      "[232]\ttrain-mlogloss:0.20424\teval-mlogloss:0.24223\n",
      "[233]\ttrain-mlogloss:0.20406\teval-mlogloss:0.24221\n",
      "[234]\ttrain-mlogloss:0.20397\teval-mlogloss:0.24224\n",
      "[235]\ttrain-mlogloss:0.20380\teval-mlogloss:0.24230\n",
      "[236]\ttrain-mlogloss:0.20364\teval-mlogloss:0.24237\n",
      "[237]\ttrain-mlogloss:0.20336\teval-mlogloss:0.24239\n",
      "[238]\ttrain-mlogloss:0.20319\teval-mlogloss:0.24247\n",
      "[239]\ttrain-mlogloss:0.20296\teval-mlogloss:0.24252\n",
      "[240]\ttrain-mlogloss:0.20274\teval-mlogloss:0.24256\n",
      "[241]\ttrain-mlogloss:0.20259\teval-mlogloss:0.24259\n",
      "[242]\ttrain-mlogloss:0.20246\teval-mlogloss:0.24265\n",
      "[243]\ttrain-mlogloss:0.20225\teval-mlogloss:0.24269\n",
      "[244]\ttrain-mlogloss:0.20208\teval-mlogloss:0.24268\n",
      "xgb now score is: [2.627641465068789, 2.5421163955938, 2.4962673833575]\n",
      "[0]\ttrain-mlogloss:0.67090\teval-mlogloss:0.67155\n",
      "[1]\ttrain-mlogloss:0.64989\teval-mlogloss:0.65118\n",
      "[2]\ttrain-mlogloss:0.63009\teval-mlogloss:0.63202\n",
      "[3]\ttrain-mlogloss:0.61152\teval-mlogloss:0.61404\n",
      "[4]\ttrain-mlogloss:0.59396\teval-mlogloss:0.59707\n",
      "[5]\ttrain-mlogloss:0.57722\teval-mlogloss:0.58091\n",
      "[6]\ttrain-mlogloss:0.56130\teval-mlogloss:0.56555\n",
      "[7]\ttrain-mlogloss:0.54628\teval-mlogloss:0.55108\n",
      "[8]\ttrain-mlogloss:0.53199\teval-mlogloss:0.53733\n",
      "[9]\ttrain-mlogloss:0.51840\teval-mlogloss:0.52431\n",
      "[10]\ttrain-mlogloss:0.50563\teval-mlogloss:0.51202\n",
      "[11]\ttrain-mlogloss:0.49341\teval-mlogloss:0.50035\n",
      "[12]\ttrain-mlogloss:0.48176\teval-mlogloss:0.48921\n",
      "[13]\ttrain-mlogloss:0.47057\teval-mlogloss:0.47856\n",
      "[14]\ttrain-mlogloss:0.46001\teval-mlogloss:0.46850\n",
      "[15]\ttrain-mlogloss:0.44990\teval-mlogloss:0.45893\n",
      "[16]\ttrain-mlogloss:0.44026\teval-mlogloss:0.44978\n",
      "[17]\ttrain-mlogloss:0.43112\teval-mlogloss:0.44113\n",
      "[18]\ttrain-mlogloss:0.42237\teval-mlogloss:0.43289\n",
      "[19]\ttrain-mlogloss:0.41403\teval-mlogloss:0.42505\n",
      "[20]\ttrain-mlogloss:0.40610\teval-mlogloss:0.41760\n",
      "[21]\ttrain-mlogloss:0.39859\teval-mlogloss:0.41057\n",
      "[22]\ttrain-mlogloss:0.39133\teval-mlogloss:0.40378\n",
      "[23]\ttrain-mlogloss:0.38439\teval-mlogloss:0.39731\n",
      "[24]\ttrain-mlogloss:0.37780\teval-mlogloss:0.39121\n",
      "[25]\ttrain-mlogloss:0.37145\teval-mlogloss:0.38534\n",
      "[26]\ttrain-mlogloss:0.36544\teval-mlogloss:0.37972\n",
      "[27]\ttrain-mlogloss:0.35961\teval-mlogloss:0.37435\n",
      "[28]\ttrain-mlogloss:0.35408\teval-mlogloss:0.36926\n",
      "[29]\ttrain-mlogloss:0.34872\teval-mlogloss:0.36436\n",
      "[30]\ttrain-mlogloss:0.34358\teval-mlogloss:0.35966\n",
      "[31]\ttrain-mlogloss:0.33867\teval-mlogloss:0.35521\n",
      "[32]\ttrain-mlogloss:0.33393\teval-mlogloss:0.35095\n",
      "[33]\ttrain-mlogloss:0.32948\teval-mlogloss:0.34695\n",
      "[34]\ttrain-mlogloss:0.32508\teval-mlogloss:0.34307\n",
      "[35]\ttrain-mlogloss:0.32093\teval-mlogloss:0.33937\n",
      "[36]\ttrain-mlogloss:0.31693\teval-mlogloss:0.33584\n",
      "[37]\ttrain-mlogloss:0.31308\teval-mlogloss:0.33240\n",
      "[38]\ttrain-mlogloss:0.30945\teval-mlogloss:0.32922\n",
      "[39]\ttrain-mlogloss:0.30605\teval-mlogloss:0.32626\n",
      "[40]\ttrain-mlogloss:0.30268\teval-mlogloss:0.32337\n",
      "[41]\ttrain-mlogloss:0.29956\teval-mlogloss:0.32057\n",
      "[42]\ttrain-mlogloss:0.29643\teval-mlogloss:0.31787\n",
      "[43]\ttrain-mlogloss:0.29333\teval-mlogloss:0.31530\n",
      "[44]\ttrain-mlogloss:0.29051\teval-mlogloss:0.31293\n",
      "[45]\ttrain-mlogloss:0.28781\teval-mlogloss:0.31070\n",
      "[46]\ttrain-mlogloss:0.28524\teval-mlogloss:0.30858\n",
      "[47]\ttrain-mlogloss:0.28279\teval-mlogloss:0.30651\n",
      "[48]\ttrain-mlogloss:0.28037\teval-mlogloss:0.30459\n",
      "[49]\ttrain-mlogloss:0.27807\teval-mlogloss:0.30270\n",
      "[50]\ttrain-mlogloss:0.27580\teval-mlogloss:0.30082\n",
      "[51]\ttrain-mlogloss:0.27367\teval-mlogloss:0.29913\n",
      "[52]\ttrain-mlogloss:0.27168\teval-mlogloss:0.29761\n",
      "[53]\ttrain-mlogloss:0.26955\teval-mlogloss:0.29601\n",
      "[54]\ttrain-mlogloss:0.26763\teval-mlogloss:0.29457\n",
      "[55]\ttrain-mlogloss:0.26583\teval-mlogloss:0.29324\n",
      "[56]\ttrain-mlogloss:0.26408\teval-mlogloss:0.29194\n",
      "[57]\ttrain-mlogloss:0.26238\teval-mlogloss:0.29069\n",
      "[58]\ttrain-mlogloss:0.26077\teval-mlogloss:0.28958\n",
      "[59]\ttrain-mlogloss:0.25923\teval-mlogloss:0.28845\n",
      "[60]\ttrain-mlogloss:0.25772\teval-mlogloss:0.28731\n",
      "[61]\ttrain-mlogloss:0.25627\teval-mlogloss:0.28633\n",
      "[62]\ttrain-mlogloss:0.25478\teval-mlogloss:0.28531\n",
      "[63]\ttrain-mlogloss:0.25345\teval-mlogloss:0.28438\n",
      "[64]\ttrain-mlogloss:0.25220\teval-mlogloss:0.28350\n",
      "[65]\ttrain-mlogloss:0.25097\teval-mlogloss:0.28271\n",
      "[66]\ttrain-mlogloss:0.24978\teval-mlogloss:0.28194\n",
      "[67]\ttrain-mlogloss:0.24860\teval-mlogloss:0.28123\n",
      "[68]\ttrain-mlogloss:0.24756\teval-mlogloss:0.28057\n",
      "[69]\ttrain-mlogloss:0.24645\teval-mlogloss:0.27994\n",
      "[70]\ttrain-mlogloss:0.24538\teval-mlogloss:0.27937\n",
      "[71]\ttrain-mlogloss:0.24432\teval-mlogloss:0.27875\n",
      "[72]\ttrain-mlogloss:0.24330\teval-mlogloss:0.27815\n",
      "[73]\ttrain-mlogloss:0.24243\teval-mlogloss:0.27766\n",
      "[74]\ttrain-mlogloss:0.24156\teval-mlogloss:0.27724\n",
      "[75]\ttrain-mlogloss:0.24065\teval-mlogloss:0.27672\n",
      "[76]\ttrain-mlogloss:0.23981\teval-mlogloss:0.27626\n",
      "[77]\ttrain-mlogloss:0.23901\teval-mlogloss:0.27585\n",
      "[78]\ttrain-mlogloss:0.23826\teval-mlogloss:0.27550\n",
      "[79]\ttrain-mlogloss:0.23757\teval-mlogloss:0.27522\n",
      "[80]\ttrain-mlogloss:0.23685\teval-mlogloss:0.27487\n",
      "[81]\ttrain-mlogloss:0.23608\teval-mlogloss:0.27458\n",
      "[82]\ttrain-mlogloss:0.23540\teval-mlogloss:0.27431\n",
      "[83]\ttrain-mlogloss:0.23473\teval-mlogloss:0.27403\n",
      "[84]\ttrain-mlogloss:0.23407\teval-mlogloss:0.27375\n",
      "[85]\ttrain-mlogloss:0.23332\teval-mlogloss:0.27342\n",
      "[86]\ttrain-mlogloss:0.23271\teval-mlogloss:0.27323\n",
      "[87]\ttrain-mlogloss:0.23213\teval-mlogloss:0.27304\n",
      "[88]\ttrain-mlogloss:0.23158\teval-mlogloss:0.27288\n",
      "[89]\ttrain-mlogloss:0.23096\teval-mlogloss:0.27271\n",
      "[90]\ttrain-mlogloss:0.23048\teval-mlogloss:0.27260\n",
      "[91]\ttrain-mlogloss:0.22997\teval-mlogloss:0.27248\n",
      "[92]\ttrain-mlogloss:0.22944\teval-mlogloss:0.27234\n",
      "[93]\ttrain-mlogloss:0.22900\teval-mlogloss:0.27225\n",
      "[94]\ttrain-mlogloss:0.22843\teval-mlogloss:0.27216\n",
      "[95]\ttrain-mlogloss:0.22794\teval-mlogloss:0.27206\n",
      "[96]\ttrain-mlogloss:0.22749\teval-mlogloss:0.27198\n",
      "[97]\ttrain-mlogloss:0.22697\teval-mlogloss:0.27197\n",
      "[98]\ttrain-mlogloss:0.22647\teval-mlogloss:0.27191\n",
      "[99]\ttrain-mlogloss:0.22610\teval-mlogloss:0.27190\n",
      "[100]\ttrain-mlogloss:0.22567\teval-mlogloss:0.27182\n",
      "[101]\ttrain-mlogloss:0.22530\teval-mlogloss:0.27174\n",
      "[102]\ttrain-mlogloss:0.22490\teval-mlogloss:0.27173\n",
      "[103]\ttrain-mlogloss:0.22461\teval-mlogloss:0.27169\n",
      "[104]\ttrain-mlogloss:0.22425\teval-mlogloss:0.27172\n",
      "[105]\ttrain-mlogloss:0.22391\teval-mlogloss:0.27172\n",
      "[106]\ttrain-mlogloss:0.22351\teval-mlogloss:0.27176\n",
      "[107]\ttrain-mlogloss:0.22308\teval-mlogloss:0.27167\n",
      "[108]\ttrain-mlogloss:0.22277\teval-mlogloss:0.27162\n",
      "[109]\ttrain-mlogloss:0.22236\teval-mlogloss:0.27153\n",
      "[110]\ttrain-mlogloss:0.22192\teval-mlogloss:0.27153\n",
      "[111]\ttrain-mlogloss:0.22152\teval-mlogloss:0.27155\n",
      "[112]\ttrain-mlogloss:0.22112\teval-mlogloss:0.27154\n",
      "[113]\ttrain-mlogloss:0.22072\teval-mlogloss:0.27150\n",
      "[114]\ttrain-mlogloss:0.22042\teval-mlogloss:0.27153\n",
      "[115]\ttrain-mlogloss:0.22002\teval-mlogloss:0.27155\n",
      "[116]\ttrain-mlogloss:0.21973\teval-mlogloss:0.27154\n",
      "[117]\ttrain-mlogloss:0.21939\teval-mlogloss:0.27160\n",
      "[118]\ttrain-mlogloss:0.21905\teval-mlogloss:0.27160\n",
      "[119]\ttrain-mlogloss:0.21878\teval-mlogloss:0.27164\n",
      "[120]\ttrain-mlogloss:0.21855\teval-mlogloss:0.27162\n",
      "[121]\ttrain-mlogloss:0.21832\teval-mlogloss:0.27165\n",
      "[122]\ttrain-mlogloss:0.21799\teval-mlogloss:0.27173\n",
      "[123]\ttrain-mlogloss:0.21766\teval-mlogloss:0.27172\n",
      "[124]\ttrain-mlogloss:0.21740\teval-mlogloss:0.27172\n",
      "[125]\ttrain-mlogloss:0.21712\teval-mlogloss:0.27180\n",
      "[126]\ttrain-mlogloss:0.21680\teval-mlogloss:0.27187\n",
      "[127]\ttrain-mlogloss:0.21656\teval-mlogloss:0.27192\n",
      "[128]\ttrain-mlogloss:0.21629\teval-mlogloss:0.27200\n",
      "[129]\ttrain-mlogloss:0.21609\teval-mlogloss:0.27202\n",
      "[130]\ttrain-mlogloss:0.21581\teval-mlogloss:0.27205\n",
      "[131]\ttrain-mlogloss:0.21545\teval-mlogloss:0.27203\n",
      "[132]\ttrain-mlogloss:0.21524\teval-mlogloss:0.27212\n",
      "[133]\ttrain-mlogloss:0.21493\teval-mlogloss:0.27213\n",
      "[134]\ttrain-mlogloss:0.21470\teval-mlogloss:0.27217\n",
      "[135]\ttrain-mlogloss:0.21439\teval-mlogloss:0.27228\n",
      "[136]\ttrain-mlogloss:0.21417\teval-mlogloss:0.27236\n",
      "[137]\ttrain-mlogloss:0.21390\teval-mlogloss:0.27245\n",
      "[138]\ttrain-mlogloss:0.21368\teval-mlogloss:0.27250\n",
      "[139]\ttrain-mlogloss:0.21342\teval-mlogloss:0.27261\n",
      "[140]\ttrain-mlogloss:0.21322\teval-mlogloss:0.27261\n",
      "[141]\ttrain-mlogloss:0.21298\teval-mlogloss:0.27259\n",
      "[142]\ttrain-mlogloss:0.21278\teval-mlogloss:0.27267\n",
      "[143]\ttrain-mlogloss:0.21256\teval-mlogloss:0.27276\n",
      "[144]\ttrain-mlogloss:0.21235\teval-mlogloss:0.27277\n",
      "[145]\ttrain-mlogloss:0.21216\teval-mlogloss:0.27282\n",
      "[146]\ttrain-mlogloss:0.21180\teval-mlogloss:0.27283\n",
      "[147]\ttrain-mlogloss:0.21166\teval-mlogloss:0.27293\n",
      "[148]\ttrain-mlogloss:0.21141\teval-mlogloss:0.27302\n",
      "[149]\ttrain-mlogloss:0.21119\teval-mlogloss:0.27304\n",
      "[150]\ttrain-mlogloss:0.21089\teval-mlogloss:0.27311\n",
      "[151]\ttrain-mlogloss:0.21068\teval-mlogloss:0.27315\n",
      "[152]\ttrain-mlogloss:0.21048\teval-mlogloss:0.27322\n",
      "[153]\ttrain-mlogloss:0.21025\teval-mlogloss:0.27332\n",
      "[154]\ttrain-mlogloss:0.20999\teval-mlogloss:0.27336\n",
      "[155]\ttrain-mlogloss:0.20978\teval-mlogloss:0.27343\n",
      "[156]\ttrain-mlogloss:0.20955\teval-mlogloss:0.27352\n",
      "[157]\ttrain-mlogloss:0.20920\teval-mlogloss:0.27364\n",
      "[158]\ttrain-mlogloss:0.20885\teval-mlogloss:0.27371\n",
      "[159]\ttrain-mlogloss:0.20855\teval-mlogloss:0.27380\n",
      "[160]\ttrain-mlogloss:0.20837\teval-mlogloss:0.27387\n",
      "[161]\ttrain-mlogloss:0.20815\teval-mlogloss:0.27392\n",
      "[162]\ttrain-mlogloss:0.20790\teval-mlogloss:0.27399\n",
      "[163]\ttrain-mlogloss:0.20773\teval-mlogloss:0.27405\n",
      "[164]\ttrain-mlogloss:0.20751\teval-mlogloss:0.27412\n",
      "[165]\ttrain-mlogloss:0.20735\teval-mlogloss:0.27418\n",
      "[166]\ttrain-mlogloss:0.20705\teval-mlogloss:0.27421\n",
      "[167]\ttrain-mlogloss:0.20688\teval-mlogloss:0.27418\n",
      "[168]\ttrain-mlogloss:0.20665\teval-mlogloss:0.27428\n",
      "[169]\ttrain-mlogloss:0.20645\teval-mlogloss:0.27437\n",
      "[170]\ttrain-mlogloss:0.20626\teval-mlogloss:0.27441\n",
      "[171]\ttrain-mlogloss:0.20605\teval-mlogloss:0.27451\n",
      "[172]\ttrain-mlogloss:0.20581\teval-mlogloss:0.27458\n",
      "[173]\ttrain-mlogloss:0.20557\teval-mlogloss:0.27460\n",
      "[174]\ttrain-mlogloss:0.20530\teval-mlogloss:0.27461\n",
      "[175]\ttrain-mlogloss:0.20507\teval-mlogloss:0.27468\n",
      "[176]\ttrain-mlogloss:0.20488\teval-mlogloss:0.27474\n",
      "[177]\ttrain-mlogloss:0.20469\teval-mlogloss:0.27480\n",
      "[178]\ttrain-mlogloss:0.20451\teval-mlogloss:0.27485\n",
      "[179]\ttrain-mlogloss:0.20429\teval-mlogloss:0.27481\n",
      "[180]\ttrain-mlogloss:0.20410\teval-mlogloss:0.27491\n",
      "[181]\ttrain-mlogloss:0.20394\teval-mlogloss:0.27493\n",
      "[182]\ttrain-mlogloss:0.20373\teval-mlogloss:0.27499\n",
      "[183]\ttrain-mlogloss:0.20356\teval-mlogloss:0.27501\n",
      "[184]\ttrain-mlogloss:0.20341\teval-mlogloss:0.27502\n",
      "[185]\ttrain-mlogloss:0.20315\teval-mlogloss:0.27505\n",
      "[186]\ttrain-mlogloss:0.20291\teval-mlogloss:0.27516\n",
      "[187]\ttrain-mlogloss:0.20275\teval-mlogloss:0.27517\n",
      "[188]\ttrain-mlogloss:0.20255\teval-mlogloss:0.27518\n",
      "[189]\ttrain-mlogloss:0.20241\teval-mlogloss:0.27525\n",
      "[190]\ttrain-mlogloss:0.20219\teval-mlogloss:0.27526\n",
      "[191]\ttrain-mlogloss:0.20209\teval-mlogloss:0.27524\n",
      "[192]\ttrain-mlogloss:0.20193\teval-mlogloss:0.27527\n",
      "[193]\ttrain-mlogloss:0.20173\teval-mlogloss:0.27522\n",
      "[194]\ttrain-mlogloss:0.20155\teval-mlogloss:0.27527\n",
      "[195]\ttrain-mlogloss:0.20126\teval-mlogloss:0.27540\n",
      "[196]\ttrain-mlogloss:0.20107\teval-mlogloss:0.27534\n",
      "[197]\ttrain-mlogloss:0.20089\teval-mlogloss:0.27540\n",
      "[198]\ttrain-mlogloss:0.20070\teval-mlogloss:0.27541\n",
      "[199]\ttrain-mlogloss:0.20053\teval-mlogloss:0.27542\n",
      "[200]\ttrain-mlogloss:0.20039\teval-mlogloss:0.27542\n",
      "[201]\ttrain-mlogloss:0.20019\teval-mlogloss:0.27554\n",
      "[202]\ttrain-mlogloss:0.19998\teval-mlogloss:0.27556\n",
      "[203]\ttrain-mlogloss:0.19977\teval-mlogloss:0.27556\n",
      "[204]\ttrain-mlogloss:0.19962\teval-mlogloss:0.27564\n",
      "[205]\ttrain-mlogloss:0.19944\teval-mlogloss:0.27564\n",
      "[206]\ttrain-mlogloss:0.19916\teval-mlogloss:0.27573\n",
      "[207]\ttrain-mlogloss:0.19896\teval-mlogloss:0.27573\n",
      "[208]\ttrain-mlogloss:0.19877\teval-mlogloss:0.27573\n",
      "[209]\ttrain-mlogloss:0.19853\teval-mlogloss:0.27561\n",
      "[210]\ttrain-mlogloss:0.19832\teval-mlogloss:0.27578\n",
      "[211]\ttrain-mlogloss:0.19813\teval-mlogloss:0.27586\n",
      "[212]\ttrain-mlogloss:0.19795\teval-mlogloss:0.27589\n",
      "[213]\ttrain-mlogloss:0.19772\teval-mlogloss:0.27593\n",
      "xgb now score is: [2.627641465068789, 2.5421163955938, 2.4962673833575, 2.393391320956645]\n",
      "[0]\ttrain-mlogloss:0.67105\teval-mlogloss:0.67122\n",
      "[1]\ttrain-mlogloss:0.65019\teval-mlogloss:0.65062\n",
      "[2]\ttrain-mlogloss:0.63061\teval-mlogloss:0.63123\n",
      "[3]\ttrain-mlogloss:0.61204\teval-mlogloss:0.61283\n",
      "[4]\ttrain-mlogloss:0.59445\teval-mlogloss:0.59543\n",
      "[5]\ttrain-mlogloss:0.57778\teval-mlogloss:0.57893\n",
      "[6]\ttrain-mlogloss:0.56214\teval-mlogloss:0.56342\n",
      "[7]\ttrain-mlogloss:0.54727\teval-mlogloss:0.54874\n",
      "[8]\ttrain-mlogloss:0.53323\teval-mlogloss:0.53485\n",
      "[9]\ttrain-mlogloss:0.51979\teval-mlogloss:0.52154\n",
      "[10]\ttrain-mlogloss:0.50710\teval-mlogloss:0.50899\n",
      "[11]\ttrain-mlogloss:0.49502\teval-mlogloss:0.49718\n",
      "[12]\ttrain-mlogloss:0.48351\teval-mlogloss:0.48584\n",
      "[13]\ttrain-mlogloss:0.47254\teval-mlogloss:0.47505\n",
      "[14]\ttrain-mlogloss:0.46201\teval-mlogloss:0.46464\n",
      "[15]\ttrain-mlogloss:0.45206\teval-mlogloss:0.45483\n",
      "[16]\ttrain-mlogloss:0.44249\teval-mlogloss:0.44544\n",
      "[17]\ttrain-mlogloss:0.43343\teval-mlogloss:0.43656\n",
      "[18]\ttrain-mlogloss:0.42471\teval-mlogloss:0.42800\n",
      "[19]\ttrain-mlogloss:0.41647\teval-mlogloss:0.41993\n",
      "[20]\ttrain-mlogloss:0.40861\teval-mlogloss:0.41222\n",
      "[21]\ttrain-mlogloss:0.40105\teval-mlogloss:0.40488\n",
      "[22]\ttrain-mlogloss:0.39388\teval-mlogloss:0.39790\n",
      "[23]\ttrain-mlogloss:0.38701\teval-mlogloss:0.39119\n",
      "[24]\ttrain-mlogloss:0.38038\teval-mlogloss:0.38471\n",
      "[25]\ttrain-mlogloss:0.37405\teval-mlogloss:0.37867\n",
      "[26]\ttrain-mlogloss:0.36803\teval-mlogloss:0.37285\n",
      "[27]\ttrain-mlogloss:0.36237\teval-mlogloss:0.36735\n",
      "[28]\ttrain-mlogloss:0.35676\teval-mlogloss:0.36194\n",
      "[29]\ttrain-mlogloss:0.35154\teval-mlogloss:0.35689\n",
      "[30]\ttrain-mlogloss:0.34643\teval-mlogloss:0.35199\n",
      "[31]\ttrain-mlogloss:0.34164\teval-mlogloss:0.34732\n",
      "[32]\ttrain-mlogloss:0.33697\teval-mlogloss:0.34292\n",
      "[33]\ttrain-mlogloss:0.33257\teval-mlogloss:0.33873\n",
      "[34]\ttrain-mlogloss:0.32833\teval-mlogloss:0.33471\n",
      "[35]\ttrain-mlogloss:0.32428\teval-mlogloss:0.33087\n",
      "[36]\ttrain-mlogloss:0.32038\teval-mlogloss:0.32715\n",
      "[37]\ttrain-mlogloss:0.31656\teval-mlogloss:0.32363\n",
      "[38]\ttrain-mlogloss:0.31301\teval-mlogloss:0.32029\n",
      "[39]\ttrain-mlogloss:0.30955\teval-mlogloss:0.31704\n",
      "[40]\ttrain-mlogloss:0.30624\teval-mlogloss:0.31397\n",
      "[41]\ttrain-mlogloss:0.30310\teval-mlogloss:0.31102\n",
      "[42]\ttrain-mlogloss:0.30012\teval-mlogloss:0.30819\n",
      "[43]\ttrain-mlogloss:0.29717\teval-mlogloss:0.30548\n",
      "[44]\ttrain-mlogloss:0.29444\teval-mlogloss:0.30286\n",
      "[45]\ttrain-mlogloss:0.29180\teval-mlogloss:0.30042\n",
      "[46]\ttrain-mlogloss:0.28918\teval-mlogloss:0.29806\n",
      "[47]\ttrain-mlogloss:0.28678\teval-mlogloss:0.29578\n",
      "[48]\ttrain-mlogloss:0.28433\teval-mlogloss:0.29357\n",
      "[49]\ttrain-mlogloss:0.28216\teval-mlogloss:0.29163\n",
      "[50]\ttrain-mlogloss:0.28003\teval-mlogloss:0.28966\n",
      "[51]\ttrain-mlogloss:0.27800\teval-mlogloss:0.28781\n",
      "[52]\ttrain-mlogloss:0.27598\teval-mlogloss:0.28609\n",
      "[53]\ttrain-mlogloss:0.27408\teval-mlogloss:0.28440\n",
      "[54]\ttrain-mlogloss:0.27218\teval-mlogloss:0.28274\n",
      "[55]\ttrain-mlogloss:0.27040\teval-mlogloss:0.28118\n",
      "[56]\ttrain-mlogloss:0.26867\teval-mlogloss:0.27964\n",
      "[57]\ttrain-mlogloss:0.26706\teval-mlogloss:0.27829\n",
      "[58]\ttrain-mlogloss:0.26550\teval-mlogloss:0.27695\n",
      "[59]\ttrain-mlogloss:0.26394\teval-mlogloss:0.27566\n",
      "[60]\ttrain-mlogloss:0.26248\teval-mlogloss:0.27437\n",
      "[61]\ttrain-mlogloss:0.26100\teval-mlogloss:0.27328\n",
      "[62]\ttrain-mlogloss:0.25960\teval-mlogloss:0.27209\n",
      "[63]\ttrain-mlogloss:0.25829\teval-mlogloss:0.27095\n",
      "[64]\ttrain-mlogloss:0.25711\teval-mlogloss:0.26996\n",
      "[65]\ttrain-mlogloss:0.25591\teval-mlogloss:0.26894\n",
      "[66]\ttrain-mlogloss:0.25474\teval-mlogloss:0.26808\n",
      "[67]\ttrain-mlogloss:0.25366\teval-mlogloss:0.26716\n",
      "[68]\ttrain-mlogloss:0.25260\teval-mlogloss:0.26630\n",
      "[69]\ttrain-mlogloss:0.25155\teval-mlogloss:0.26547\n",
      "[70]\ttrain-mlogloss:0.25056\teval-mlogloss:0.26473\n",
      "[71]\ttrain-mlogloss:0.24960\teval-mlogloss:0.26391\n",
      "[72]\ttrain-mlogloss:0.24859\teval-mlogloss:0.26322\n",
      "[73]\ttrain-mlogloss:0.24759\teval-mlogloss:0.26264\n",
      "[74]\ttrain-mlogloss:0.24673\teval-mlogloss:0.26201\n",
      "[75]\ttrain-mlogloss:0.24583\teval-mlogloss:0.26140\n",
      "[76]\ttrain-mlogloss:0.24506\teval-mlogloss:0.26086\n",
      "[77]\ttrain-mlogloss:0.24418\teval-mlogloss:0.26030\n",
      "[78]\ttrain-mlogloss:0.24340\teval-mlogloss:0.25980\n",
      "[79]\ttrain-mlogloss:0.24275\teval-mlogloss:0.25931\n",
      "[80]\ttrain-mlogloss:0.24203\teval-mlogloss:0.25885\n",
      "[81]\ttrain-mlogloss:0.24132\teval-mlogloss:0.25838\n",
      "[82]\ttrain-mlogloss:0.24070\teval-mlogloss:0.25793\n",
      "[83]\ttrain-mlogloss:0.24006\teval-mlogloss:0.25756\n",
      "[84]\ttrain-mlogloss:0.23947\teval-mlogloss:0.25717\n",
      "[85]\ttrain-mlogloss:0.23887\teval-mlogloss:0.25684\n",
      "[86]\ttrain-mlogloss:0.23828\teval-mlogloss:0.25647\n",
      "[87]\ttrain-mlogloss:0.23770\teval-mlogloss:0.25613\n",
      "[88]\ttrain-mlogloss:0.23726\teval-mlogloss:0.25577\n",
      "[89]\ttrain-mlogloss:0.23668\teval-mlogloss:0.25547\n",
      "[90]\ttrain-mlogloss:0.23618\teval-mlogloss:0.25517\n",
      "[91]\ttrain-mlogloss:0.23570\teval-mlogloss:0.25489\n",
      "[92]\ttrain-mlogloss:0.23518\teval-mlogloss:0.25459\n",
      "[93]\ttrain-mlogloss:0.23470\teval-mlogloss:0.25426\n",
      "[94]\ttrain-mlogloss:0.23418\teval-mlogloss:0.25411\n",
      "[95]\ttrain-mlogloss:0.23367\teval-mlogloss:0.25385\n",
      "[96]\ttrain-mlogloss:0.23322\teval-mlogloss:0.25364\n",
      "[97]\ttrain-mlogloss:0.23281\teval-mlogloss:0.25351\n",
      "[98]\ttrain-mlogloss:0.23233\teval-mlogloss:0.25328\n",
      "[99]\ttrain-mlogloss:0.23186\teval-mlogloss:0.25315\n",
      "[100]\ttrain-mlogloss:0.23151\teval-mlogloss:0.25301\n",
      "[101]\ttrain-mlogloss:0.23109\teval-mlogloss:0.25290\n",
      "[102]\ttrain-mlogloss:0.23074\teval-mlogloss:0.25273\n",
      "[103]\ttrain-mlogloss:0.23041\teval-mlogloss:0.25260\n",
      "[104]\ttrain-mlogloss:0.23001\teval-mlogloss:0.25246\n",
      "[105]\ttrain-mlogloss:0.22969\teval-mlogloss:0.25236\n",
      "[106]\ttrain-mlogloss:0.22933\teval-mlogloss:0.25227\n",
      "[107]\ttrain-mlogloss:0.22900\teval-mlogloss:0.25219\n",
      "[108]\ttrain-mlogloss:0.22867\teval-mlogloss:0.25204\n",
      "[109]\ttrain-mlogloss:0.22830\teval-mlogloss:0.25197\n",
      "[110]\ttrain-mlogloss:0.22790\teval-mlogloss:0.25187\n",
      "[111]\ttrain-mlogloss:0.22757\teval-mlogloss:0.25181\n",
      "[112]\ttrain-mlogloss:0.22726\teval-mlogloss:0.25173\n",
      "[113]\ttrain-mlogloss:0.22697\teval-mlogloss:0.25165\n",
      "[114]\ttrain-mlogloss:0.22661\teval-mlogloss:0.25159\n",
      "[115]\ttrain-mlogloss:0.22637\teval-mlogloss:0.25153\n",
      "[116]\ttrain-mlogloss:0.22599\teval-mlogloss:0.25151\n",
      "[117]\ttrain-mlogloss:0.22563\teval-mlogloss:0.25149\n",
      "[118]\ttrain-mlogloss:0.22532\teval-mlogloss:0.25148\n",
      "[119]\ttrain-mlogloss:0.22500\teval-mlogloss:0.25145\n",
      "[120]\ttrain-mlogloss:0.22465\teval-mlogloss:0.25137\n",
      "[121]\ttrain-mlogloss:0.22434\teval-mlogloss:0.25139\n",
      "[122]\ttrain-mlogloss:0.22408\teval-mlogloss:0.25131\n",
      "[123]\ttrain-mlogloss:0.22371\teval-mlogloss:0.25126\n",
      "[124]\ttrain-mlogloss:0.22345\teval-mlogloss:0.25123\n",
      "[125]\ttrain-mlogloss:0.22325\teval-mlogloss:0.25116\n",
      "[126]\ttrain-mlogloss:0.22294\teval-mlogloss:0.25112\n",
      "[127]\ttrain-mlogloss:0.22263\teval-mlogloss:0.25107\n",
      "[128]\ttrain-mlogloss:0.22242\teval-mlogloss:0.25105\n",
      "[129]\ttrain-mlogloss:0.22224\teval-mlogloss:0.25095\n",
      "[130]\ttrain-mlogloss:0.22202\teval-mlogloss:0.25093\n",
      "[131]\ttrain-mlogloss:0.22164\teval-mlogloss:0.25091\n",
      "[132]\ttrain-mlogloss:0.22140\teval-mlogloss:0.25091\n",
      "[133]\ttrain-mlogloss:0.22117\teval-mlogloss:0.25091\n",
      "[134]\ttrain-mlogloss:0.22084\teval-mlogloss:0.25093\n",
      "[135]\ttrain-mlogloss:0.22054\teval-mlogloss:0.25093\n",
      "[136]\ttrain-mlogloss:0.22036\teval-mlogloss:0.25092\n",
      "[137]\ttrain-mlogloss:0.22010\teval-mlogloss:0.25099\n",
      "[138]\ttrain-mlogloss:0.21985\teval-mlogloss:0.25094\n",
      "[139]\ttrain-mlogloss:0.21966\teval-mlogloss:0.25093\n",
      "[140]\ttrain-mlogloss:0.21941\teval-mlogloss:0.25100\n",
      "[141]\ttrain-mlogloss:0.21913\teval-mlogloss:0.25088\n",
      "[142]\ttrain-mlogloss:0.21885\teval-mlogloss:0.25084\n",
      "[143]\ttrain-mlogloss:0.21866\teval-mlogloss:0.25088\n",
      "[144]\ttrain-mlogloss:0.21843\teval-mlogloss:0.25084\n",
      "[145]\ttrain-mlogloss:0.21820\teval-mlogloss:0.25089\n",
      "[146]\ttrain-mlogloss:0.21788\teval-mlogloss:0.25089\n",
      "[147]\ttrain-mlogloss:0.21764\teval-mlogloss:0.25086\n",
      "[148]\ttrain-mlogloss:0.21745\teval-mlogloss:0.25088\n",
      "[149]\ttrain-mlogloss:0.21715\teval-mlogloss:0.25086\n",
      "[150]\ttrain-mlogloss:0.21701\teval-mlogloss:0.25088\n",
      "[151]\ttrain-mlogloss:0.21678\teval-mlogloss:0.25093\n",
      "[152]\ttrain-mlogloss:0.21655\teval-mlogloss:0.25089\n",
      "[153]\ttrain-mlogloss:0.21634\teval-mlogloss:0.25088\n",
      "[154]\ttrain-mlogloss:0.21605\teval-mlogloss:0.25091\n",
      "[155]\ttrain-mlogloss:0.21583\teval-mlogloss:0.25093\n",
      "[156]\ttrain-mlogloss:0.21563\teval-mlogloss:0.25095\n",
      "[157]\ttrain-mlogloss:0.21535\teval-mlogloss:0.25092\n",
      "[158]\ttrain-mlogloss:0.21519\teval-mlogloss:0.25094\n",
      "[159]\ttrain-mlogloss:0.21498\teval-mlogloss:0.25095\n",
      "[160]\ttrain-mlogloss:0.21478\teval-mlogloss:0.25103\n",
      "[161]\ttrain-mlogloss:0.21455\teval-mlogloss:0.25106\n",
      "[162]\ttrain-mlogloss:0.21429\teval-mlogloss:0.25108\n",
      "[163]\ttrain-mlogloss:0.21402\teval-mlogloss:0.25109\n",
      "[164]\ttrain-mlogloss:0.21375\teval-mlogloss:0.25115\n",
      "[165]\ttrain-mlogloss:0.21356\teval-mlogloss:0.25113\n",
      "[166]\ttrain-mlogloss:0.21343\teval-mlogloss:0.25115\n",
      "[167]\ttrain-mlogloss:0.21318\teval-mlogloss:0.25119\n",
      "[168]\ttrain-mlogloss:0.21299\teval-mlogloss:0.25127\n",
      "[169]\ttrain-mlogloss:0.21275\teval-mlogloss:0.25136\n",
      "[170]\ttrain-mlogloss:0.21253\teval-mlogloss:0.25135\n",
      "[171]\ttrain-mlogloss:0.21237\teval-mlogloss:0.25140\n",
      "[172]\ttrain-mlogloss:0.21223\teval-mlogloss:0.25146\n",
      "[173]\ttrain-mlogloss:0.21208\teval-mlogloss:0.25149\n",
      "[174]\ttrain-mlogloss:0.21191\teval-mlogloss:0.25151\n",
      "[175]\ttrain-mlogloss:0.21164\teval-mlogloss:0.25148\n",
      "[176]\ttrain-mlogloss:0.21150\teval-mlogloss:0.25147\n",
      "[177]\ttrain-mlogloss:0.21132\teval-mlogloss:0.25152\n",
      "[178]\ttrain-mlogloss:0.21113\teval-mlogloss:0.25152\n",
      "[179]\ttrain-mlogloss:0.21096\teval-mlogloss:0.25148\n",
      "[180]\ttrain-mlogloss:0.21070\teval-mlogloss:0.25146\n",
      "[181]\ttrain-mlogloss:0.21048\teval-mlogloss:0.25147\n",
      "[182]\ttrain-mlogloss:0.21026\teval-mlogloss:0.25144\n",
      "[183]\ttrain-mlogloss:0.21014\teval-mlogloss:0.25142\n",
      "[184]\ttrain-mlogloss:0.20997\teval-mlogloss:0.25144\n",
      "[185]\ttrain-mlogloss:0.20983\teval-mlogloss:0.25144\n",
      "[186]\ttrain-mlogloss:0.20966\teval-mlogloss:0.25142\n",
      "[187]\ttrain-mlogloss:0.20953\teval-mlogloss:0.25147\n",
      "[188]\ttrain-mlogloss:0.20936\teval-mlogloss:0.25144\n",
      "[189]\ttrain-mlogloss:0.20916\teval-mlogloss:0.25145\n",
      "[190]\ttrain-mlogloss:0.20902\teval-mlogloss:0.25147\n",
      "[191]\ttrain-mlogloss:0.20871\teval-mlogloss:0.25147\n",
      "[192]\ttrain-mlogloss:0.20853\teval-mlogloss:0.25143\n",
      "[193]\ttrain-mlogloss:0.20839\teval-mlogloss:0.25144\n",
      "[194]\ttrain-mlogloss:0.20822\teval-mlogloss:0.25149\n",
      "[195]\ttrain-mlogloss:0.20800\teval-mlogloss:0.25160\n",
      "[196]\ttrain-mlogloss:0.20785\teval-mlogloss:0.25162\n",
      "[197]\ttrain-mlogloss:0.20759\teval-mlogloss:0.25168\n",
      "[198]\ttrain-mlogloss:0.20744\teval-mlogloss:0.25174\n",
      "[199]\ttrain-mlogloss:0.20731\teval-mlogloss:0.25182\n",
      "[200]\ttrain-mlogloss:0.20714\teval-mlogloss:0.25190\n",
      "[201]\ttrain-mlogloss:0.20696\teval-mlogloss:0.25185\n",
      "[202]\ttrain-mlogloss:0.20683\teval-mlogloss:0.25184\n",
      "[203]\ttrain-mlogloss:0.20657\teval-mlogloss:0.25183\n",
      "[204]\ttrain-mlogloss:0.20635\teval-mlogloss:0.25186\n",
      "[205]\ttrain-mlogloss:0.20611\teval-mlogloss:0.25182\n",
      "[206]\ttrain-mlogloss:0.20597\teval-mlogloss:0.25180\n",
      "[207]\ttrain-mlogloss:0.20588\teval-mlogloss:0.25183\n",
      "[208]\ttrain-mlogloss:0.20573\teval-mlogloss:0.25193\n",
      "[209]\ttrain-mlogloss:0.20558\teval-mlogloss:0.25200\n",
      "[210]\ttrain-mlogloss:0.20545\teval-mlogloss:0.25206\n",
      "[211]\ttrain-mlogloss:0.20534\teval-mlogloss:0.25205\n",
      "[212]\ttrain-mlogloss:0.20519\teval-mlogloss:0.25207\n",
      "[213]\ttrain-mlogloss:0.20505\teval-mlogloss:0.25211\n",
      "[214]\ttrain-mlogloss:0.20487\teval-mlogloss:0.25212\n",
      "[215]\ttrain-mlogloss:0.20478\teval-mlogloss:0.25214\n",
      "[216]\ttrain-mlogloss:0.20460\teval-mlogloss:0.25212\n",
      "[217]\ttrain-mlogloss:0.20439\teval-mlogloss:0.25209\n",
      "[218]\ttrain-mlogloss:0.20418\teval-mlogloss:0.25217\n",
      "[219]\ttrain-mlogloss:0.20403\teval-mlogloss:0.25223\n",
      "[220]\ttrain-mlogloss:0.20381\teval-mlogloss:0.25228\n",
      "[221]\ttrain-mlogloss:0.20360\teval-mlogloss:0.25230\n",
      "[222]\ttrain-mlogloss:0.20340\teval-mlogloss:0.25237\n",
      "[223]\ttrain-mlogloss:0.20322\teval-mlogloss:0.25243\n",
      "[224]\ttrain-mlogloss:0.20311\teval-mlogloss:0.25249\n",
      "[225]\ttrain-mlogloss:0.20292\teval-mlogloss:0.25245\n",
      "[226]\ttrain-mlogloss:0.20279\teval-mlogloss:0.25247\n",
      "[227]\ttrain-mlogloss:0.20256\teval-mlogloss:0.25249\n",
      "[228]\ttrain-mlogloss:0.20230\teval-mlogloss:0.25240\n",
      "[229]\ttrain-mlogloss:0.20202\teval-mlogloss:0.25248\n",
      "[230]\ttrain-mlogloss:0.20186\teval-mlogloss:0.25252\n",
      "[231]\ttrain-mlogloss:0.20175\teval-mlogloss:0.25255\n",
      "[232]\ttrain-mlogloss:0.20153\teval-mlogloss:0.25249\n",
      "[233]\ttrain-mlogloss:0.20127\teval-mlogloss:0.25246\n",
      "[234]\ttrain-mlogloss:0.20110\teval-mlogloss:0.25256\n",
      "[235]\ttrain-mlogloss:0.20088\teval-mlogloss:0.25261\n",
      "[236]\ttrain-mlogloss:0.20067\teval-mlogloss:0.25264\n",
      "[237]\ttrain-mlogloss:0.20051\teval-mlogloss:0.25267\n",
      "[238]\ttrain-mlogloss:0.20039\teval-mlogloss:0.25271\n",
      "[239]\ttrain-mlogloss:0.20017\teval-mlogloss:0.25266\n",
      "[240]\ttrain-mlogloss:0.20001\teval-mlogloss:0.25270\n",
      "[241]\ttrain-mlogloss:0.19978\teval-mlogloss:0.25265\n",
      "[242]\ttrain-mlogloss:0.19951\teval-mlogloss:0.25263\n",
      "[243]\ttrain-mlogloss:0.19937\teval-mlogloss:0.25266\n",
      "[244]\ttrain-mlogloss:0.19918\teval-mlogloss:0.25267\n",
      "xgb now score is: [2.627641465068789, 2.5421163955938, 2.4962673833575, 2.393391320956645, 2.488317792597411]\n",
      "xgb_score_list: [2.627641465068789, 2.5421163955938, 2.4962673833575, 2.393391320956645, 2.488317792597411]\n",
      "xgb_score_mean: 2.5095468715148295\n"
     ]
    }
   ],
   "source": [
    "clf_list = clf_list\n",
    "column_list = []\n",
    "train_data_list=[]\n",
    "test_data_list=[]\n",
    "for clf in clf_list:\n",
    "    train_data,test_data,clf_name=clf(x_train, y_train, x_valid, kf, label_split=None)\n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data)\n",
    "train_stacking = np.concatenate(train_data_list, axis=1)\n",
    "test_stacking = np.concatenate(test_data_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （5）原始特征和stacking特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并所有特征\n",
    "train = pd.DataFrame(np.concatenate([x_train, train_stacking], axis=1))\n",
    "test = np.concatenate([x_valid, test_stacking], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征重命名\n",
    "df_train_all = pd.DataFrame(train)\n",
    "df_train_all.columns = features_columns + clf_list_col\n",
    "df_test_all = pd.DataFrame(test)\n",
    "df_test_all.columns = features_columns + clf_list_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据ID以及特征标签label\n",
    "df_train_all['label'] = all_data_test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 保存特征数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all.to_csv('./data/train_all.csv',header=True,index=False)\n",
    "df_test_all.to_csv('./data/test_all.csv',header=True,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
