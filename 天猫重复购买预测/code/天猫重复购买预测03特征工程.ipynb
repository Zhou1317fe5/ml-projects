{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 工具导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import gc\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    " \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据集\n",
    "test_data = pd.read_csv('./data/data_format1/test_format1.csv')\n",
    "train_data = pd.read_csv('./data/data_format1/train_format1.csv')\n",
    "user_info = pd.read_csv('./data/data_format1/user_info_format1.csv')\n",
    "user_log = pd.read_csv('./data/data_format1/user_log_format1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据资源查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260864 entries, 0 to 260863\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype\n",
      "---  ------       --------------   -----\n",
      " 0   user_id      260864 non-null  int64\n",
      " 1   merchant_id  260864 non-null  int64\n",
      " 2   label        260864 non-null  int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 6.0 MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 261477 entries, 0 to 261476\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   user_id      261477 non-null  int64  \n",
      " 1   merchant_id  261477 non-null  int64  \n",
      " 2   prob         0 non-null       float64\n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 6.0 MB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 424170 entries, 0 to 424169\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   user_id    424170 non-null  int64  \n",
      " 1   age_range  421953 non-null  float64\n",
      " 2   gender     417734 non-null  float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 9.7 MB\n"
     ]
    }
   ],
   "source": [
    "user_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54925330 entries, 0 to 54925329\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   user_id      int64  \n",
      " 1   item_id      int64  \n",
      " 2   cat_id       int64  \n",
      " 3   seller_id    int64  \n",
      " 4   brand_id     float64\n",
      " 5   time_stamp   int64  \n",
      " 6   action_type  int64  \n",
      "dtypes: float64(1), int64(6)\n",
      "memory usage: 2.9 GB\n"
     ]
    }
   ],
   "source": [
    "user_log.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据资源非常大，甚至达到2.9GB，需要进行数据压缩 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_name, num_rows):\n",
    "    return pd.read_csv(file_name, nrows=num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内存压缩方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码解释\n",
    "这段代码是用Python编写的，定义了一个名为`reduce_mem_usage`的函数，用于优化数据帧（dataframe）的内存使用。主要目的是将数据帧中的整数和浮点数类型转换为更小的数据类型，从而减少内存占用。\n",
    "\n",
    "以下是代码的详细解释：\n",
    "\n",
    "1. 定义一个名为`reduce_mem_usage`的函数，接受一个数据帧`df`和一个布尔值`verbose`作为参数。\n",
    "\n",
    "2. 计算原始数据帧的内存使用情况，并将结果除以1024^2以转换为MB。\n",
    "\n",
    "3. 定义一个包含整数和浮点数类型的列表`numerics`。\n",
    "\n",
    "4. 遍历数据帧的列（`for col in df.columns:`）。\n",
    "\n",
    "5. 获取当前列的数据类型（`col_type = df[col].dtypes`）。\n",
    "\n",
    "6. 如果当前列的数据类型在`numerics`列表中（`if col_type in numerics:`），则进行以下操作：\n",
    "\n",
    "  a. 计算当前列的最小值和最大值（`c_min = df[col].min()`和`c_max = df[col].max()`）。\n",
    "\n",
    "  b. 如果当前列的数据类型是整数类型（`if str(col_type)[:3] == 'int'`），则检查当前列的最小值和最大值是否在整数类型的范围中。如果是，则将当前列转换为更小的整数类型（`df[col] = df[col].astype(np.int8)`、`df[col] = df[col].astype(np.int16)`、`df[col] = df[col].astype(np.int32)`或`df[col] = df[col].astype(np.int64)`）。\n",
    "\n",
    "  c. 如果当前列的数据类型是浮点数类型，则检查当前列的最小值和最大值是否在浮点数类型的范围中。如果是，则将当前列转换为更小的浮点数类型（`df[col] = df[col].astype(np.float16)`、`df[col] = df[col].astype(np.float32)`或`df[col] = df[col].astype(np.float64)`）。\n",
    "\n",
    "7. 计算优化后的数据帧的内存使用情况，并将结果除以1024^2以转换为MB。\n",
    "\n",
    "8. 打印优化后的数据帧的内存使用情况，以及与原始数据帧的内存使用情况的百分比差异。\n",
    "\n",
    "9. 返回优化后的数据帧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据进行内存压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 1.74 MB\n",
      "Decreased by 70.8%\n",
      "Memory usage after optimization is: 3.49 MB\n",
      "Decreased by 41.7%\n",
      "Memory usage after optimization is: 3.24 MB\n",
      "Decreased by 66.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 32.43 MB\n",
      "Decreased by 69.6%\n"
     ]
    }
   ],
   "source": [
    "num_rows = None\n",
    "num_rows = 200 * 10000 # 1000条测试代码使用\n",
    "# num_rows = 1000\n",
    "\n",
    "train_file = './data/data_format1/train_format1.csv'\n",
    "test_file = './data/data_format1/test_format1.csv'\n",
    "\n",
    "user_info_file = './data/data_format1/user_info_format1.csv'\n",
    "user_log_file = './data/data_format1/user_log_format1.csv'\n",
    "\n",
    "train_data = reduce_mem_usage(read_csv(train_file, num_rows))\n",
    "test_data = reduce_mem_usage(read_csv(test_file, num_rows))\n",
    "\n",
    "user_info = reduce_mem_usage(read_csv(user_info_file, num_rows))\n",
    "user_log = reduce_mem_usage(read_csv(user_log_file, num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260864 entries, 0 to 260863\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype\n",
      "---  ------       --------------   -----\n",
      " 0   user_id      260864 non-null  int32\n",
      " 1   merchant_id  260864 non-null  int16\n",
      " 2   label        260864 non-null  int8 \n",
      "dtypes: int16(1), int32(1), int8(1)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 261477 entries, 0 to 261476\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   user_id      261477 non-null  int32  \n",
      " 1   merchant_id  261477 non-null  int16  \n",
      " 2   prob         0 non-null       float64\n",
      "dtypes: float64(1), int16(1), int32(1)\n",
      "memory usage: 3.5 MB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 424170 entries, 0 to 424169\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   user_id    424170 non-null  int32  \n",
      " 1   age_range  421953 non-null  float16\n",
      " 2   gender     417734 non-null  float16\n",
      "dtypes: float16(2), int32(1)\n",
      "memory usage: 3.2 MB\n"
     ]
    }
   ],
   "source": [
    "user_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000000 entries, 0 to 1999999\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   user_id      int32  \n",
      " 1   item_id      int32  \n",
      " 2   cat_id       int16  \n",
      " 3   seller_id    int16  \n",
      " 4   brand_id     float16\n",
      " 5   time_stamp   int16  \n",
      " 6   action_type  int8   \n",
      "dtypes: float16(1), int16(3), int32(2), int8(1)\n",
      "memory usage: 32.4 MB\n"
     ]
    }
   ],
   "source": [
    "user_log.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "\n",
    "### 合并用户信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 合并训练集、测试集、用户信息表\n",
    "del test_data['prob']\n",
    "all_data = train_data.append(test_data)\n",
    "all_data = all_data.merge(user_info,on=['user_id'],how='left')\n",
    "del train_data, test_data, user_info\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34176</td>\n",
       "      <td>3906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34176</td>\n",
       "      <td>121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34176</td>\n",
       "      <td>4356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34176</td>\n",
       "      <td>2217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230784</td>\n",
       "      <td>4818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  age_range  gender\n",
       "0    34176         3906    0.0        6.0     0.0\n",
       "1    34176          121    0.0        6.0     0.0\n",
       "2    34176         4356    1.0        6.0     0.0\n",
       "3    34176         2217    0.0        6.0     0.0\n",
       "4   230784         4818    0.0        0.0     0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码解释\n",
    "这段代码的主要目的是合并训练集、测试集和用户信息表，并将它们合并为一个新的数据集all_data。\n",
    "\n",
    "1. 首先，代码删除了测试集中名为'prob'的列，因为它在训练集中不存在。\n",
    "2. 然后，将训练集和测试集合并为一个新的数据集all_data。这里使用了`append()`方法将测试集添加到训练集的末尾。。\n",
    "3. 接着，使用`merge()`方法将用户信息表和合并后的数据集all_data合并，使用'on'参数指定连接的列，使用'how'参数指定连接方式为左连接（即保留左表中的所有行，即使右表中没有匹配的行）。\n",
    "4. 最后，删除了训练集、测试集和用户信息表，使用`del`关键字。同时，调用`gc.collect()`函数释放内存。\n",
    "\n",
    "- `del`关键字用于删除变量或对象。当你在Python中使用`del`关键字时，它会从内存中删除变量或对象的引用，从而释放该对象所占用的内存空间。\n",
    "\n",
    "\n",
    "- `gc.collect()`函数是Python的垃圾回收器（Garbage Collector）的接口，用于手动触发垃圾回收操作。在某些情况下，Python可能会无法及时回收垃圾，这时可以使用`gc.collect()`函数手动触发垃圾回收，以释放被占用的内存空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用户行为日志信息按时间进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "按时间排序\n",
    "\"\"\"\n",
    "user_log = user_log.sort_values(['user_id','time_stamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>action_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61975</th>\n",
       "      <td>16</td>\n",
       "      <td>980982</td>\n",
       "      <td>437</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61976</th>\n",
       "      <td>16</td>\n",
       "      <td>980982</td>\n",
       "      <td>437</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61977</th>\n",
       "      <td>16</td>\n",
       "      <td>980982</td>\n",
       "      <td>437</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61978</th>\n",
       "      <td>16</td>\n",
       "      <td>962763</td>\n",
       "      <td>19</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61979</th>\n",
       "      <td>16</td>\n",
       "      <td>391126</td>\n",
       "      <td>437</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  cat_id  seller_id  brand_id  time_stamp  action_type\n",
       "61975       16   980982     437        650    4276.0         914            0\n",
       "61976       16   980982     437        650    4276.0         914            0\n",
       "61977       16   980982     437        650    4276.0         914            0\n",
       "61978       16   962763      19        650    4276.0         914            0\n",
       "61979       16   391126     437        650    4276.0         914            0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码解释\n",
    "对user_id列和time_stamp列进行排序的具体步骤如下：\n",
    "\n",
    "1. 首先，使用`sort_values()`方法对user_id列进行升序排序。升序排序意味着从小到大排列，即按user_id的顺序排列。\n",
    "2. 然后，使用`sort_values()`方法对time_stamp列进行升序排序。升序排序意味着从小到大排列，即按时间戳的顺序排列。\n",
    "\n",
    "因此，经过这两步排序后，user_log数据框中的数据将按照用户ID和时间戳的顺序进行排列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = user_log.groupby('user_id').agg(agg_dict).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.rename(columns=rename_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对每个用户逐个合并所有的字段\n",
    "合并字段为item_id, cat_id,seller_id,brand_id,time_stamp, action_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "合并数据\n",
    "\"\"\"\n",
    "list_join_func = lambda x: \" \".join([str(i) for i in x])\n",
    "\n",
    "\n",
    "agg_dict = {\n",
    "            'item_id' : list_join_func,\t\n",
    "            'cat_id' : list_join_func,\n",
    "            'seller_id' : list_join_func,\n",
    "            'brand_id' : list_join_func,\n",
    "            'time_stamp' : list_join_func,\n",
    "            'action_type' : list_join_func\n",
    "        }\n",
    "\n",
    "rename_dict = {\n",
    "            'item_id' : 'item_path',\n",
    "            'cat_id' : 'cat_path',\n",
    "            'seller_id' : 'seller_path',\n",
    "            'brand_id' : 'brand_path',\n",
    "            'time_stamp' : 'time_stamp_path',\n",
    "            'action_type' : 'action_type_path'\n",
    "        }\n",
    "\n",
    "# def merge_list(df_ID, join_columns, df_data, agg_dict, rename_dict):\n",
    "#     df_data = df_data.\\\n",
    "#                 groupby(join_columns).\\\n",
    "#                 agg(agg_dict).\\\n",
    "#                 reset_index().\\\n",
    "#                 rename(columns=rename_dict)\n",
    "    \n",
    "#     df_ID = df_ID.merge(df_data, on=join_columns, how=\"left\") \n",
    "#     return df_data,df_ID\n",
    "# all_data = merge_list(all_data, 'user_id', user_log, agg_dict, rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_path = user_log.groupby('user_id').agg(agg_dict).reset_index().rename(columns=rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>action_type_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>980982 980982 980982 962763 391126 827174 6731...</td>\n",
       "      <td>437 437 437 19 437 437 437 437 895 19 437 437 ...</td>\n",
       "      <td>650 650 650 650 650 650 650 650 3948 650 650 6...</td>\n",
       "      <td>4276.0 4276.0 4276.0 4276.0 4276.0 4276.0 4276...</td>\n",
       "      <td>914 914 914 914 914 914 914 914 914 914 914 91...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>388018 388018 88673 88673 88673 88673 846066 5...</td>\n",
       "      <td>949 949 614 614 614 614 420 1401 948 948 513 1...</td>\n",
       "      <td>2772 2772 4066 4066 4066 4066 4951 4951 2872 2...</td>\n",
       "      <td>2112.0 2112.0 1552.0 1552.0 1552.0 1552.0 5200...</td>\n",
       "      <td>710 710 711 711 711 711 908 908 1105 1105 1105...</td>\n",
       "      <td>0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>60215 1004605 60215 60215 60215 60215 628525 5...</td>\n",
       "      <td>1308 1308 1308 1308 1308 1308 1271 656 656 656...</td>\n",
       "      <td>2128 3207 2128 2128 2128 2128 3142 4618 4618 4...</td>\n",
       "      <td>3848.0 3848.0 3848.0 3848.0 3848.0 3848.0 1014...</td>\n",
       "      <td>521 521 521 521 521 522 529 828 828 828 828 82...</td>\n",
       "      <td>0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>889499 528459 765746 553259 889499 22435 40047...</td>\n",
       "      <td>662 1075 662 1577 662 11 184 1604 11 11 177 11...</td>\n",
       "      <td>4048 601 3104 3828 4048 4766 2419 2768 2565 26...</td>\n",
       "      <td>5360.0 1040.0 8240.0 1446.0 5360.0 4360.0 3428...</td>\n",
       "      <td>517 520 525 528 602 602 610 610 610 610 610 61...</td>\n",
       "      <td>3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155</td>\n",
       "      <td>979639 890128 981780 211366 211366 797946 4567...</td>\n",
       "      <td>267 1271 1505 267 267 1075 1075 407 407 1075 4...</td>\n",
       "      <td>2429 4785 3784 800 800 1595 1418 2662 2662 315...</td>\n",
       "      <td>2276.0 1422.0 5692.0 6328.0 6328.0 5800.0 7140...</td>\n",
       "      <td>529 529 602 604 604 607 607 607 607 607 607 60...</td>\n",
       "      <td>0 0 0 2 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 2 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                          item_path  \\\n",
       "0       16  980982 980982 980982 962763 391126 827174 6731...   \n",
       "1       19  388018 388018 88673 88673 88673 88673 846066 5...   \n",
       "2       41  60215 1004605 60215 60215 60215 60215 628525 5...   \n",
       "3       56  889499 528459 765746 553259 889499 22435 40047...   \n",
       "4      155  979639 890128 981780 211366 211366 797946 4567...   \n",
       "\n",
       "                                            cat_path  \\\n",
       "0  437 437 437 19 437 437 437 437 895 19 437 437 ...   \n",
       "1  949 949 614 614 614 614 420 1401 948 948 513 1...   \n",
       "2  1308 1308 1308 1308 1308 1308 1271 656 656 656...   \n",
       "3  662 1075 662 1577 662 11 184 1604 11 11 177 11...   \n",
       "4  267 1271 1505 267 267 1075 1075 407 407 1075 4...   \n",
       "\n",
       "                                         seller_path  \\\n",
       "0  650 650 650 650 650 650 650 650 3948 650 650 6...   \n",
       "1  2772 2772 4066 4066 4066 4066 4951 4951 2872 2...   \n",
       "2  2128 3207 2128 2128 2128 2128 3142 4618 4618 4...   \n",
       "3  4048 601 3104 3828 4048 4766 2419 2768 2565 26...   \n",
       "4  2429 4785 3784 800 800 1595 1418 2662 2662 315...   \n",
       "\n",
       "                                          brand_path  \\\n",
       "0  4276.0 4276.0 4276.0 4276.0 4276.0 4276.0 4276...   \n",
       "1  2112.0 2112.0 1552.0 1552.0 1552.0 1552.0 5200...   \n",
       "2  3848.0 3848.0 3848.0 3848.0 3848.0 3848.0 1014...   \n",
       "3  5360.0 1040.0 8240.0 1446.0 5360.0 4360.0 3428...   \n",
       "4  2276.0 1422.0 5692.0 6328.0 6328.0 5800.0 7140...   \n",
       "\n",
       "                                     time_stamp_path  \\\n",
       "0  914 914 914 914 914 914 914 914 914 914 914 91...   \n",
       "1  710 710 711 711 711 711 908 908 1105 1105 1105...   \n",
       "2  521 521 521 521 521 522 529 828 828 828 828 82...   \n",
       "3  517 520 525 528 602 602 610 610 610 610 610 61...   \n",
       "4  529 529 602 604 604 607 607 607 607 607 607 60...   \n",
       "\n",
       "                                    action_type_path  \n",
       "0  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 0 ...  \n",
       "1  0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "2  0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ...  \n",
       "3  3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ...  \n",
       "4  0 0 0 2 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 2 ...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log_path.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_path = all_data.merge(user_log_path,on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>action_type_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>986160 681407 681407 910680 681407 592698 3693...</td>\n",
       "      <td>35 1554 1554 119 1554 662 1095 662 35 833 833 ...</td>\n",
       "      <td>4811 4811 4811 1897 4811 3315 2925 1340 1875 4...</td>\n",
       "      <td>127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...</td>\n",
       "      <td>518 518 518 520 520 524 524 524 525 525 525 52...</td>\n",
       "      <td>2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396970 961553 627712 926681 1012423 825576 149...</td>\n",
       "      <td>1023 420 407 1505 962 602 184 1606 351 1505 11...</td>\n",
       "      <td>1435 1648 223 3178 2418 1614 3004 2511 2285 78...</td>\n",
       "      <td>5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...</td>\n",
       "      <td>517 520 522 522 527 530 530 530 601 601 602 60...</td>\n",
       "      <td>2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400</td>\n",
       "      <td>302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256546 202393 927572 2587 10956 549283 270303 ...</td>\n",
       "      <td>1188 646 1175 1188 1414 681 1175 681 681 115 1...</td>\n",
       "      <td>805 390 4252 3979 1228 2029 2029 2029 4252 923...</td>\n",
       "      <td>1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...</td>\n",
       "      <td>517 604 604 604 607 609 609 609 609 615 621 62...</td>\n",
       "      <td>2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  age_range  gender  \\\n",
       "0   105600         1487    0.0        6.0     1.0   \n",
       "1   110976          159    0.0        5.0     0.0   \n",
       "2   374400          302    0.0        5.0     1.0   \n",
       "3   189312         1760    0.0        4.0     0.0   \n",
       "4   189312         2511    0.0        4.0     0.0   \n",
       "\n",
       "                                           item_path  \\\n",
       "0  986160 681407 681407 910680 681407 592698 3693...   \n",
       "1  396970 961553 627712 926681 1012423 825576 149...   \n",
       "2  256546 202393 927572 2587 10956 549283 270303 ...   \n",
       "3  290583 166235 556025 217894 166235 556025 5589...   \n",
       "4  290583 166235 556025 217894 166235 556025 5589...   \n",
       "\n",
       "                                            cat_path  \\\n",
       "0  35 1554 1554 119 1554 662 1095 662 35 833 833 ...   \n",
       "1  1023 420 407 1505 962 602 184 1606 351 1505 11...   \n",
       "2  1188 646 1175 1188 1414 681 1175 681 681 115 1...   \n",
       "3  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "4  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "\n",
       "                                         seller_path  \\\n",
       "0  4811 4811 4811 1897 4811 3315 2925 1340 1875 4...   \n",
       "1  1435 1648 223 3178 2418 1614 3004 2511 2285 78...   \n",
       "2  805 390 4252 3979 1228 2029 2029 2029 4252 923...   \n",
       "3  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "4  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "\n",
       "                                          brand_path  \\\n",
       "0  127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...   \n",
       "1  5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...   \n",
       "2  1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...   \n",
       "3  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "4  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "\n",
       "                                     time_stamp_path  \\\n",
       "0  518 518 518 520 520 524 524 524 525 525 525 52...   \n",
       "1  517 520 522 522 527 530 530 530 601 601 602 60...   \n",
       "2  517 604 604 604 607 609 609 609 609 615 621 62...   \n",
       "3  924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "4  924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "\n",
       "                                    action_type_path  \n",
       "0  2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "1  2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...  \n",
       "2  2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "3  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "4  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_path.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除数据并回收内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "删除不需要的数据\n",
    "\"\"\"\n",
    "del user_log\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义数据统计函数\n",
    "\n",
    "### 统计数据的总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnt_(x):\n",
    "    try:\n",
    "        return len(x.split(' '))\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计唯一数据总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nunique_(x):\n",
    "    try:\n",
    "        return len(set(x.split(' ')))\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计数据最大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_(x):\n",
    "    try:\n",
    "        return np.max([int(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计数据最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_(x):\n",
    "    try:\n",
    "        return np.min([int(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计数据的标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_(x):\n",
    "    try:\n",
    "        return np.std([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计数据中top N的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_n(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][0]\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计数据中top N数据的总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_n_cnt(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][1]\n",
    "    except:\n",
    "        return -1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "def user_cnt(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(cnt_)\n",
    "    return df_data\n",
    "\n",
    "def user_nunique(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(nunique_)\n",
    "    return df_data\n",
    "    \n",
    "def user_max(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(max_)\n",
    "    return df_data\n",
    "\n",
    "def user_min(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(min_)\n",
    "    return df_data\n",
    "    \n",
    "def user_std(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(std_)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n_cnt(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n_cnt(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取商铺的基本统计特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    提取基本统计特征\n",
    "\"\"\"\n",
    "all_data_test = all_data_path.head(2000)\n",
    "#all_data_test = all_data_path\n",
    "# 统计用户 点击、浏览、加购、购买行为\n",
    "# 总次数\n",
    "all_data_test = user_cnt(all_data_test,  'seller_path', 'user_cnt')\n",
    "# 不同店铺个数\n",
    "all_data_test = user_nunique(all_data_test,  'seller_path', 'seller_nunique')\n",
    "# 不同品类个数\n",
    "all_data_test = user_nunique(all_data_test,  'cat_path', 'cat_nunique')\n",
    "# 不同品牌个数\n",
    "all_data_test = user_nunique(all_data_test,  'brand_path', 'brand_nunique')\n",
    "# 不同商品个数\n",
    "all_data_test = user_nunique(all_data_test,  'item_path', 'item_nunique')\n",
    "# 活跃天数\n",
    "all_data_test = user_nunique(all_data_test,  'time_stamp_path', 'time_stamp_nunique')\n",
    "# 不用行为种数\n",
    "all_data_test = user_nunique(all_data_test,  'action_type_path', 'action_type_nunique')\n",
    "# ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>action_type_path</th>\n",
       "      <th>user_cnt</th>\n",
       "      <th>seller_nunique</th>\n",
       "      <th>cat_nunique</th>\n",
       "      <th>brand_nunique</th>\n",
       "      <th>item_nunique</th>\n",
       "      <th>time_stamp_nunique</th>\n",
       "      <th>action_type_nunique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>986160 681407 681407 910680 681407 592698 3693...</td>\n",
       "      <td>35 1554 1554 119 1554 662 1095 662 35 833 833 ...</td>\n",
       "      <td>4811 4811 4811 1897 4811 3315 2925 1340 1875 4...</td>\n",
       "      <td>127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...</td>\n",
       "      <td>518 518 518 520 520 524 524 524 525 525 525 52...</td>\n",
       "      <td>2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>310</td>\n",
       "      <td>96</td>\n",
       "      <td>37</td>\n",
       "      <td>88</td>\n",
       "      <td>217</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396970 961553 627712 926681 1012423 825576 149...</td>\n",
       "      <td>1023 420 407 1505 962 602 184 1606 351 1505 11...</td>\n",
       "      <td>1435 1648 223 3178 2418 1614 3004 2511 2285 78...</td>\n",
       "      <td>5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...</td>\n",
       "      <td>517 520 522 522 527 530 530 530 601 601 602 60...</td>\n",
       "      <td>2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...</td>\n",
       "      <td>274</td>\n",
       "      <td>181</td>\n",
       "      <td>70</td>\n",
       "      <td>159</td>\n",
       "      <td>233</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400</td>\n",
       "      <td>302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256546 202393 927572 2587 10956 549283 270303 ...</td>\n",
       "      <td>1188 646 1175 1188 1414 681 1175 681 681 115 1...</td>\n",
       "      <td>805 390 4252 3979 1228 2029 2029 2029 4252 923...</td>\n",
       "      <td>1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...</td>\n",
       "      <td>517 604 604 604 607 609 609 609 609 615 621 62...</td>\n",
       "      <td>2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>278</td>\n",
       "      <td>57</td>\n",
       "      <td>59</td>\n",
       "      <td>62</td>\n",
       "      <td>148</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>237</td>\n",
       "      <td>49</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>170</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>237</td>\n",
       "      <td>49</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>170</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  age_range  gender  \\\n",
       "0   105600         1487    0.0        6.0     1.0   \n",
       "1   110976          159    0.0        5.0     0.0   \n",
       "2   374400          302    0.0        5.0     1.0   \n",
       "3   189312         1760    0.0        4.0     0.0   \n",
       "4   189312         2511    0.0        4.0     0.0   \n",
       "\n",
       "                                           item_path  \\\n",
       "0  986160 681407 681407 910680 681407 592698 3693...   \n",
       "1  396970 961553 627712 926681 1012423 825576 149...   \n",
       "2  256546 202393 927572 2587 10956 549283 270303 ...   \n",
       "3  290583 166235 556025 217894 166235 556025 5589...   \n",
       "4  290583 166235 556025 217894 166235 556025 5589...   \n",
       "\n",
       "                                            cat_path  \\\n",
       "0  35 1554 1554 119 1554 662 1095 662 35 833 833 ...   \n",
       "1  1023 420 407 1505 962 602 184 1606 351 1505 11...   \n",
       "2  1188 646 1175 1188 1414 681 1175 681 681 115 1...   \n",
       "3  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "4  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "\n",
       "                                         seller_path  \\\n",
       "0  4811 4811 4811 1897 4811 3315 2925 1340 1875 4...   \n",
       "1  1435 1648 223 3178 2418 1614 3004 2511 2285 78...   \n",
       "2  805 390 4252 3979 1228 2029 2029 2029 4252 923...   \n",
       "3  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "4  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "\n",
       "                                          brand_path  \\\n",
       "0  127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...   \n",
       "1  5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...   \n",
       "2  1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...   \n",
       "3  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "4  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "\n",
       "                                     time_stamp_path  \\\n",
       "0  518 518 518 520 520 524 524 524 525 525 525 52...   \n",
       "1  517 520 522 522 527 530 530 530 601 601 602 60...   \n",
       "2  517 604 604 604 607 609 609 609 609 615 621 62...   \n",
       "3  924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "4  924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "\n",
       "                                    action_type_path  user_cnt  \\\n",
       "0  2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...       310   \n",
       "1  2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...       274   \n",
       "2  2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...       278   \n",
       "3  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...       237   \n",
       "4  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...       237   \n",
       "\n",
       "   seller_nunique  cat_nunique  brand_nunique  item_nunique  \\\n",
       "0              96           37             88           217   \n",
       "1             181           70            159           233   \n",
       "2              57           59             62           148   \n",
       "3              49           35             45           170   \n",
       "4              49           35             45           170   \n",
       "\n",
       "   time_stamp_nunique  action_type_nunique  \n",
       "0                  29                    2  \n",
       "1                  52                    3  \n",
       "2                  35                    3  \n",
       "3                   9                    2  \n",
       "4                   9                    2  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最晚时间\n",
    "all_data_test = user_max(all_data_test,  'action_type_path', 'time_stamp_max')\n",
    "# 最早时间\n",
    "all_data_test = user_min(all_data_test,  'action_type_path', 'time_stamp_min')\n",
    "# 活跃天数方差\n",
    "all_data_test = user_std(all_data_test,  'action_type_path', 'time_stamp_std')\n",
    "# 最早和最晚相差天数\n",
    "all_data_test['time_stamp_range'] = all_data_test['time_stamp_max'] - all_data_test['time_stamp_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户最喜欢的店铺\n",
    "all_data_test = user_most_n(all_data_test, 'seller_path', 'seller_most_1', n=1)\n",
    "# 最喜欢的类目\n",
    "all_data_test = user_most_n(all_data_test, 'cat_path', 'cat_most_1', n=1)\n",
    "# 最喜欢的品牌\n",
    "all_data_test = user_most_n(all_data_test, 'brand_path', 'brand_most_1', n=1)\n",
    "# 最常见的行为动作\n",
    "all_data_test = user_most_n(all_data_test, 'action_type_path', 'action_type_1', n=1)\n",
    "# ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户最喜欢的店铺 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'seller_path', 'seller_most_1_cnt', n=1)\n",
    "# 最喜欢的类目 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'cat_path', 'cat_most_1_cnt', n=1)\n",
    "# 最喜欢的品牌 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'brand_path', 'brand_most_1_cnt', n=1)\n",
    "# 最常见的行为动作 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'action_type_path', 'action_type_1_cnt', n=1)\n",
    "# ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分开统计用户的点击，加购，购买，收藏特征\n",
    "\n",
    "### 不同行为的业务函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 点击、加购、购买、收藏 分开统计\n",
    "\"\"\"\n",
    "统计基本特征函数  \n",
    "-- 知识点二\n",
    "-- 根据不同行为的业务函数\n",
    "-- 提取不同特征\n",
    "\"\"\"\n",
    "def col_cnt_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "\n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col])\n",
    "\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "\n",
    "        return len(data_out)  \n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def col_nuique_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "\n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col])\n",
    "\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "\n",
    "        return len(set(data_out))\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "\n",
    "def user_col_cnt(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_cnt_(x, columns_list, action_type), axis=1)\n",
    "    return df_data\n",
    "\n",
    "def user_col_nunique(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_nuique_(x, columns_list, action_type), axis=1)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计店铺被用户点击次数，加购次数，购买次数，收藏次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '0', 'user_cnt_0')\n",
    "# 加购次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '1', 'user_cnt_1')\n",
    "# 购买次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '2', 'user_cnt_2')\n",
    "# 收藏次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '3', 'user_cnt_3')\n",
    "\n",
    "\n",
    "# 不同店铺个数\n",
    "all_data_test = user_col_nunique(all_data_test,  ['seller_path'], '0', 'seller_nunique_0')\n",
    "# ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>...</th>\n",
       "      <th>action_type_1</th>\n",
       "      <th>seller_most_1_cnt</th>\n",
       "      <th>cat_most_1_cnt</th>\n",
       "      <th>brand_most_1_cnt</th>\n",
       "      <th>action_type_1_cnt</th>\n",
       "      <th>user_cnt_0</th>\n",
       "      <th>user_cnt_1</th>\n",
       "      <th>user_cnt_2</th>\n",
       "      <th>user_cnt_3</th>\n",
       "      <th>seller_nunique_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>986160 681407 681407 910680 681407 592698 3693...</td>\n",
       "      <td>35 1554 1554 119 1554 662 1095 662 35 833 833 ...</td>\n",
       "      <td>4811 4811 4811 1897 4811 3315 2925 1340 1875 4...</td>\n",
       "      <td>127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...</td>\n",
       "      <td>518 518 518 520 520 524 524 524 525 525 525 52...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>43</td>\n",
       "      <td>35</td>\n",
       "      <td>299</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396970 961553 627712 926681 1012423 825576 149...</td>\n",
       "      <td>1023 420 407 1505 962 602 184 1606 351 1505 11...</td>\n",
       "      <td>1435 1648 223 3178 2418 1614 3004 2511 2285 78...</td>\n",
       "      <td>5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...</td>\n",
       "      <td>517 520 522 522 527 530 530 530 601 601 602 60...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>259</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400</td>\n",
       "      <td>302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256546 202393 927572 2587 10956 549283 270303 ...</td>\n",
       "      <td>1188 646 1175 1188 1414 681 1175 681 681 115 1...</td>\n",
       "      <td>805 390 4252 3979 1228 2029 2029 2029 4252 923...</td>\n",
       "      <td>1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...</td>\n",
       "      <td>517 604 604 604 607 609 609 609 609 615 621 62...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>29</td>\n",
       "      <td>48</td>\n",
       "      <td>241</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>228</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>228</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>305721</td>\n",
       "      <td>3734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>195526 195526 195526 195526 1002121 36379 8135...</td>\n",
       "      <td>384 384 384 384 1075 1075 1075 1349 1349 1349 ...</td>\n",
       "      <td>4696 4696 4696 4696 494 3863 4741 1251 2269 12...</td>\n",
       "      <td>6548.0 6548.0 6548.0 6548.0 7288.0 2960.0 7088...</td>\n",
       "      <td>610 610 610 630 707 707 707 1109 1109 1109 110...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>109881</td>\n",
       "      <td>2639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288225 812720 500918 500918 698153 507072 5009...</td>\n",
       "      <td>35 681 295 295 1518 1518 295 1468 1518 1468 81...</td>\n",
       "      <td>696 899 3398 3398 3398 3398 3398 3398 3398 339...</td>\n",
       "      <td>3600.0 6568.0 30.0 30.0 30.0 30.0 30.0 30.0 30...</td>\n",
       "      <td>524 528 530 530 530 530 530 530 530 530 705 70...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>32</td>\n",
       "      <td>66</td>\n",
       "      <td>264</td>\n",
       "      <td>284</td>\n",
       "      <td>284</td>\n",
       "      <td>284</td>\n",
       "      <td>284</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>185145</td>\n",
       "      <td>4950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>952981 502801 917988 917988 143799 667127 4004...</td>\n",
       "      <td>898 795 354 354 833 1150 843 898 898 898 898 8...</td>\n",
       "      <td>3319 390 1338 1338 3149 3319 1338 3319 3319 33...</td>\n",
       "      <td>5508.0 6876.0 4740.0 4740.0 5388.0 1283.0 4740...</td>\n",
       "      <td>515 516 517 517 604 606 606 619 619 619 619 61...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>74</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>131385</td>\n",
       "      <td>1582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>771829 332521 332521 60522 60522 672487 711909...</td>\n",
       "      <td>1213 1075 1075 1213 1213 1656 1075 1213 1213 1...</td>\n",
       "      <td>2588 3168 3168 2160 2160 2439 3168 2160 2588 4...</td>\n",
       "      <td>2760.0 4504.0 4504.0 1195.0 1195.0 1307.0 4504...</td>\n",
       "      <td>531 531 531 531 531 531 531 531 531 531 531 53...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>84</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>7737</td>\n",
       "      <td>2066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>305283 990086 985582 741215 741215 741215 6061...</td>\n",
       "      <td>1271 1271 812 464 464 464 1577 464 464 464 464...</td>\n",
       "      <td>1500 540 1849 176 176 176 3028 176 176 176 176...</td>\n",
       "      <td>7284.0 5384.0 5576.0 6664.0 6664.0 6664.0 4608...</td>\n",
       "      <td>511 511 514 520 520 520 520 520 520 520 520 52...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>189</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  merchant_id  label  age_range  gender  \\\n",
       "0      105600         1487    0.0        6.0     1.0   \n",
       "1      110976          159    0.0        5.0     0.0   \n",
       "2      374400          302    0.0        5.0     1.0   \n",
       "3      189312         1760    0.0        4.0     0.0   \n",
       "4      189312         2511    0.0        4.0     0.0   \n",
       "...       ...          ...    ...        ...     ...   \n",
       "1995   305721         3734    0.0        3.0     1.0   \n",
       "1996   109881         2639    0.0        4.0     0.0   \n",
       "1997   185145         4950    0.0        4.0     1.0   \n",
       "1998   131385         1582    0.0        3.0     1.0   \n",
       "1999     7737         2066    0.0        4.0     0.0   \n",
       "\n",
       "                                              item_path  \\\n",
       "0     986160 681407 681407 910680 681407 592698 3693...   \n",
       "1     396970 961553 627712 926681 1012423 825576 149...   \n",
       "2     256546 202393 927572 2587 10956 549283 270303 ...   \n",
       "3     290583 166235 556025 217894 166235 556025 5589...   \n",
       "4     290583 166235 556025 217894 166235 556025 5589...   \n",
       "...                                                 ...   \n",
       "1995  195526 195526 195526 195526 1002121 36379 8135...   \n",
       "1996  288225 812720 500918 500918 698153 507072 5009...   \n",
       "1997  952981 502801 917988 917988 143799 667127 4004...   \n",
       "1998  771829 332521 332521 60522 60522 672487 711909...   \n",
       "1999  305283 990086 985582 741215 741215 741215 6061...   \n",
       "\n",
       "                                               cat_path  \\\n",
       "0     35 1554 1554 119 1554 662 1095 662 35 833 833 ...   \n",
       "1     1023 420 407 1505 962 602 184 1606 351 1505 11...   \n",
       "2     1188 646 1175 1188 1414 681 1175 681 681 115 1...   \n",
       "3     601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "4     601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "...                                                 ...   \n",
       "1995  384 384 384 384 1075 1075 1075 1349 1349 1349 ...   \n",
       "1996  35 681 295 295 1518 1518 295 1468 1518 1468 81...   \n",
       "1997  898 795 354 354 833 1150 843 898 898 898 898 8...   \n",
       "1998  1213 1075 1075 1213 1213 1656 1075 1213 1213 1...   \n",
       "1999  1271 1271 812 464 464 464 1577 464 464 464 464...   \n",
       "\n",
       "                                            seller_path  \\\n",
       "0     4811 4811 4811 1897 4811 3315 2925 1340 1875 4...   \n",
       "1     1435 1648 223 3178 2418 1614 3004 2511 2285 78...   \n",
       "2     805 390 4252 3979 1228 2029 2029 2029 4252 923...   \n",
       "3     3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "4     3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "...                                                 ...   \n",
       "1995  4696 4696 4696 4696 494 3863 4741 1251 2269 12...   \n",
       "1996  696 899 3398 3398 3398 3398 3398 3398 3398 339...   \n",
       "1997  3319 390 1338 1338 3149 3319 1338 3319 3319 33...   \n",
       "1998  2588 3168 3168 2160 2160 2439 3168 2160 2588 4...   \n",
       "1999  1500 540 1849 176 176 176 3028 176 176 176 176...   \n",
       "\n",
       "                                             brand_path  \\\n",
       "0     127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...   \n",
       "1     5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...   \n",
       "2     1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...   \n",
       "3     549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "4     549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "...                                                 ...   \n",
       "1995  6548.0 6548.0 6548.0 6548.0 7288.0 2960.0 7088...   \n",
       "1996  3600.0 6568.0 30.0 30.0 30.0 30.0 30.0 30.0 30...   \n",
       "1997  5508.0 6876.0 4740.0 4740.0 5388.0 1283.0 4740...   \n",
       "1998  2760.0 4504.0 4504.0 1195.0 1195.0 1307.0 4504...   \n",
       "1999  7284.0 5384.0 5576.0 6664.0 6664.0 6664.0 4608...   \n",
       "\n",
       "                                        time_stamp_path  ... action_type_1  \\\n",
       "0     518 518 518 520 520 524 524 524 525 525 525 52...  ...             0   \n",
       "1     517 520 522 522 527 530 530 530 601 601 602 60...  ...             0   \n",
       "2     517 604 604 604 607 609 609 609 609 615 621 62...  ...             0   \n",
       "3     924 924 924 924 924 924 924 924 924 924 924 92...  ...             0   \n",
       "4     924 924 924 924 924 924 924 924 924 924 924 92...  ...             0   \n",
       "...                                                 ...  ...           ...   \n",
       "1995  610 610 610 630 707 707 707 1109 1109 1109 110...  ...             0   \n",
       "1996  524 528 530 530 530 530 530 530 530 530 705 70...  ...             0   \n",
       "1997  515 516 517 517 604 606 606 619 619 619 619 61...  ...             0   \n",
       "1998  531 531 531 531 531 531 531 531 531 531 531 53...  ...             0   \n",
       "1999  511 511 514 520 520 520 520 520 520 520 520 52...  ...             0   \n",
       "\n",
       "      seller_most_1_cnt  cat_most_1_cnt  brand_most_1_cnt  action_type_1_cnt  \\\n",
       "0                    35              43                35                299   \n",
       "1                     9              56                11                259   \n",
       "2                    93              29                48                241   \n",
       "3                    45              68                45                228   \n",
       "4                    45              68                45                228   \n",
       "...                 ...             ...               ...                ...   \n",
       "1995                 11              11                11                 32   \n",
       "1996                 67              32                66                264   \n",
       "1997                 15              12                12                 74   \n",
       "1998                 16              21                17                 84   \n",
       "1999                 55              22                28                189   \n",
       "\n",
       "      user_cnt_0  user_cnt_1  user_cnt_2  user_cnt_3  seller_nunique_0  \n",
       "0            310         310         310         310                97  \n",
       "1            274         274         274         274               181  \n",
       "2            278         278         278         278                56  \n",
       "3            237         237         237         237                50  \n",
       "4            237         237         237         237                50  \n",
       "...          ...         ...         ...         ...               ...  \n",
       "1995          35          35          35          35                14  \n",
       "1996         284         284         284         284                52  \n",
       "1997          84          84          84          84                30  \n",
       "1998          92          92          92          92                39  \n",
       "1999         210         210         210         210                46  \n",
       "\n",
       "[2000 rows x 35 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 组合特征\n",
    "\n",
    "### 特征组合进行业务特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path', 'item_path'], '0', 'user_cnt_0')\n",
    "\n",
    "# 不同店铺个数\n",
    "all_data_test = user_col_nunique(all_data_test,  ['seller_path', 'item_path'], '0', 'seller_nunique_0')\n",
    "# ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看提取的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'merchant_id', 'label', 'age_range', 'gender', 'item_path',\n",
       "       'cat_path', 'seller_path', 'brand_path', 'time_stamp_path',\n",
       "       'action_type_path', 'user_cnt', 'seller_nunique', 'cat_nunique',\n",
       "       'brand_nunique', 'item_nunique', 'time_stamp_nunique',\n",
       "       'action_type_nunique', 'time_stamp_max', 'time_stamp_min',\n",
       "       'time_stamp_std', 'time_stamp_range', 'seller_most_1', 'cat_most_1',\n",
       "       'brand_most_1', 'action_type_1', 'seller_most_1_cnt', 'cat_most_1_cnt',\n",
       "       'brand_most_1_cnt', 'action_type_1_cnt', 'user_cnt_0', 'user_cnt_1',\n",
       "       'user_cnt_2', 'user_cnt_3', 'seller_nunique_0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'merchant_id',\n",
       " 'label',\n",
       " 'age_range',\n",
       " 'gender',\n",
       " 'item_path',\n",
       " 'cat_path',\n",
       " 'seller_path',\n",
       " 'brand_path',\n",
       " 'time_stamp_path',\n",
       " 'action_type_path',\n",
       " 'user_cnt',\n",
       " 'seller_nunique',\n",
       " 'cat_nunique',\n",
       " 'brand_nunique',\n",
       " 'item_nunique',\n",
       " 'time_stamp_nunique',\n",
       " 'action_type_nunique',\n",
       " 'time_stamp_max',\n",
       " 'time_stamp_min',\n",
       " 'time_stamp_std',\n",
       " 'time_stamp_range',\n",
       " 'seller_most_1',\n",
       " 'cat_most_1',\n",
       " 'brand_most_1',\n",
       " 'action_type_1',\n",
       " 'seller_most_1_cnt',\n",
       " 'cat_most_1_cnt',\n",
       " 'brand_most_1_cnt',\n",
       " 'action_type_1_cnt',\n",
       " 'user_cnt_0',\n",
       " 'user_cnt_1',\n",
       " 'user_cnt_2',\n",
       " 'user_cnt_3',\n",
       " 'seller_nunique_0']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(all_data_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用countvector，tfidf提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 知识点四\n",
    "-- 利用countvector，tfidf提取特征\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from scipy import sparse\n",
    "# cntVec = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 1), max_features=100)\n",
    "tfidfVec = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 1), max_features=100)\n",
    "\n",
    "\n",
    "# columns_list = ['seller_path', 'cat_path', 'brand_path', 'action_type_path', 'item_path', 'time_stamp_path']\n",
    "columns_list = ['seller_path']\n",
    "for i, col in enumerate(columns_list):\n",
    "    all_data_test[col] = all_data_test[col].astype(str)\n",
    "    tfidfVec.fit(all_data_test[col])\n",
    "    data_ = tfidfVec.transform(all_data_test[col])\n",
    "    if i == 0:\n",
    "        data_cat = data_\n",
    "    else:\n",
    "        data_cat = sparse.hstack((data_cat, data_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征重命名 特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(data_cat.toarray())\n",
    "df_tfidf.columns = ['tfidf_' + str(i) for i in df_tfidf.columns]\n",
    "all_data_test = pd.concat([all_data_test, df_tfidf],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embeeding特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18248\\876498458.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Train Word2Vec model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_data_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'seller_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# Train Word2Vec model\n",
    "\n",
    "model = gensim.models.Word2Vec(all_data_test['seller_path'].apply(lambda x: x.split(' ')), vector_size=100, window=5, min_count=5, workers=4)\n",
    "# model.save(\"product2vec.model\")\n",
    "# model = gensim.models.Word2Vec.load(\"product2vec.model\")\n",
    "\n",
    "def mean_w2v_(x, model, vector_size=100):\n",
    "    try:\n",
    "        i = 0\n",
    "        for word in x.split(' '):\n",
    "            if word in model.wv.vocab:\n",
    "                i += 1\n",
    "                if i == 1:\n",
    "                    vec = np.zeros(vector_size)\n",
    "                vec += model.wv[word]\n",
    "        return vec / i \n",
    "    except:\n",
    "        return  np.zeros(vector_size)\n",
    "\n",
    "\n",
    "def get_mean_w2v(df_data, columns, model, vector_size):\n",
    "    data_array = []\n",
    "    for index, row in df_data.iterrows():\n",
    "        w2v = mean_w2v_(row[columns], model, vector_size)\n",
    "        data_array.append(w2v)\n",
    "    return pd.DataFrame(data_array)\n",
    "\n",
    "df_embeeding = get_mean_w2v(all_data_test, 'seller_path', model, 100)\n",
    "df_embeeding.columns = ['embeeding_' + str(i) for i in df_embeeding.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embeeding特征和原始特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_test = pd.concat([all_data_test, df_embeeding],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacking特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 知识点六\n",
    "-- stacking特征\n",
    "\"\"\"\n",
    "# from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss,mean_absolute_error,mean_squared_error\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking 回归特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 回归\n",
    "-- stacking 回归特征\n",
    "\"\"\"\n",
    "def stacking_reg(clf,train_x,train_y,test_x,clf_name,kf,label_split=None):\n",
    "    train=np.zeros((train_x.shape[0],1))\n",
    "    test=np.zeros((test_x.shape[0],1))\n",
    "    test_pre=np.empty((folds,test_x.shape[0],1))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf.split(train_x,label_split)):       \n",
    "        tr_x=train_x[train_index]\n",
    "        tr_y=train_y[train_index]\n",
    "        te_x=train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "        if clf_name in [\"rf\",\"ada\",\"gb\",\"et\",\"lr\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre=clf.predict(te_x).reshape(-1,1)\n",
    "            train[test_index]=pre\n",
    "            test_pre[i,:]=clf.predict(test_x).reshape(-1,1)\n",
    "            cv_scores.append(mean_squared_error(te_y, pre))\n",
    "        elif clf_name in [\"xgb\"]:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x, label=te_y, missing=-1)\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'eval_metric': 'rmse',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 12\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'),\n",
    "                         (test_matrix, 'eval')\n",
    "                         ]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_boost_round=num_round,evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(test_matrix,ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(z, ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "                cv_scores.append(mean_squared_error(te_y, pre))\n",
    "\n",
    "        elif clf_name in [\"lgb\"]:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      'objective': 'regression_l2',\n",
    "                      'metric': 'mse',\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'num_leaves': 2**5,\n",
    "                      'lambda_l2': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'learning_rate': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 12,\n",
    "                      'silent': True,\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            #early_stopping_rounds = 100\n",
    "            #callbacks=[lightgbm.log_evaluation(period=100), lightgbm.early_stopping(stopping_rounds=100)]\n",
    "            callbacks=[lightgbm.early_stopping(stopping_rounds=100)]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                                  #early_stopping_rounds=early_stopping_rounds\n",
    "                                  callbacks=callbacks\n",
    "                                  )\n",
    "                pre= model.predict(te_x,num_iteration=model.best_iteration).reshape(-1,1)\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(test_x, num_iteration=model.best_iteration).reshape(-1,1)\n",
    "                cv_scores.append(mean_squared_error(te_y, pre))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "    return train.reshape(-1,1),test.reshape(-1,1)\n",
    "\n",
    "def rf_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestRegressor(n_estimators=600, max_depth=20, n_jobs=-1, random_state=2017, max_features=\"auto\",verbose=1)\n",
    "    rf_train, rf_test = stacking_reg(randomforest, x_train, y_train, x_valid, \"rf\", kf, label_split=label_split)\n",
    "    return rf_train, rf_test,\"rf_reg\"\n",
    "\n",
    "def ada_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostRegressor(n_estimators=30, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_reg(adaboost, x_train, y_train, x_valid, \"ada\", kf, label_split=label_split)\n",
    "    return ada_train, ada_test,\"ada_reg\"\n",
    "\n",
    "def gb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingRegressor(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking_reg(gbdt, x_train, y_train, x_valid, \"gb\", kf, label_split=label_split)\n",
    "    return gbdt_train, gbdt_test,\"gb_reg\"\n",
    "\n",
    "def et_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesRegressor(n_estimators=600, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking_reg(extratree, x_train, y_train, x_valid, \"et\", kf, label_split=label_split)\n",
    "    return et_train, et_test,\"et_reg\"\n",
    "\n",
    "def lr_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lr_reg=LinearRegression(n_jobs=-1)\n",
    "    lr_train, lr_test = stacking_reg(lr_reg, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return lr_train, lr_test, \"lr_reg\"\n",
    "\n",
    "def xgb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_reg(xgboost, x_train, y_train, x_valid, \"xgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"xgb_reg\"\n",
    "\n",
    "def lgb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lgb_train, lgb_test = stacking_reg(lightgbm, x_train, y_train, x_valid, \"lgb\", kf, label_split=label_split)\n",
    "    return lgb_train, lgb_test,\"lgb_reg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking 分类特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 分类\n",
    "-- stacking 分类特征\n",
    "\"\"\"\n",
    "def stacking_clf(clf,train_x,train_y,test_x,clf_name,kf,label_split=None):\n",
    "    train=np.zeros((train_x.shape[0],1))\n",
    "    test=np.zeros((test_x.shape[0],1))\n",
    "    test_pre=np.empty((folds,test_x.shape[0],1))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf.split(train_x,label_split)):       \n",
    "        tr_x=train_x[train_index]\n",
    "        tr_y=train_y[train_index]\n",
    "        te_x=train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "\n",
    "        if clf_name in [\"rf\",\"ada\",\"gb\",\"et\",\"lr\",\"knn\",\"gnb\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre=clf.predict_proba(te_x)\n",
    "            \n",
    "            train[test_index]=pre[:,0].reshape(-1,1)\n",
    "            test_pre[i,:]=clf.predict_proba(test_x)[:,0].reshape(-1,1)\n",
    "            \n",
    "            cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        elif clf_name in [\"xgb\"]:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x)\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'objective': 'multi:softprob',\n",
    "                      'eval_metric': 'mlogloss',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      \"num_class\": 2\n",
    "                      }\n",
    "\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'),\n",
    "                         (test_matrix, 'eval')\n",
    "                         ]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_boost_round=num_round,evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(test_matrix,ntree_limit=model.best_ntree_limit)\n",
    "                train[test_index]=pre[:,0].reshape(-1,1)\n",
    "                test_pre[i, :]= model.predict(z, ntree_limit=model.best_ntree_limit)[:,0].reshape(-1,1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        elif clf_name in [\"lgb\"]:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      #'boosting_type': 'dart',\n",
    "                      'objective': 'multiclass',\n",
    "                      'metric': 'multi_logloss',\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'num_leaves': 2**5,\n",
    "                      'lambda_l2': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'learning_rate': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      \"num_class\": 2,\n",
    "                      'silent': True,\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            #early_stopping_rounds = 100\n",
    "            #callbacks=[lightgbm.log_evaluation(period=100), lightgbm.early_stopping(stopping_rounds=100)]\n",
    "            callbacks=[lightgbm.early_stopping(stopping_rounds=100)]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                                  #early_stopping_rounds=early_stopping_rounds\n",
    "                                  callbacks=callbacks\n",
    "                                  )\n",
    "                pre= model.predict(te_x,num_iteration=model.best_iteration)\n",
    "                train[test_index]=pre[:,0].reshape(-1,1)\n",
    "                test_pre[i, :]= model.predict(test_x, num_iteration=model.best_iteration)[:,0].reshape(-1,1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "    return train.reshape(-1,1),test.reshape(-1,1)\n",
    "\n",
    "def rf_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestClassifier(n_estimators=1200, max_depth=20, n_jobs=-1, random_state=2017, max_features=\"auto\",verbose=1)\n",
    "    rf_train, rf_test = stacking_clf(randomforest, x_train, y_train, x_valid, \"rf\", kf, label_split=label_split)\n",
    "    return rf_train, rf_test,\"rf\"\n",
    "\n",
    "def ada_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostClassifier(n_estimators=50, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_clf(adaboost, x_train, y_train, x_valid, \"ada\", kf, label_split=label_split)\n",
    "    return ada_train, ada_test,\"ada\"\n",
    "\n",
    "def gb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingClassifier(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking_clf(gbdt, x_train, y_train, x_valid, \"gb\", kf, label_split=label_split)\n",
    "    return gbdt_train, gbdt_test,\"gb\"\n",
    "\n",
    "def et_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesClassifier(n_estimators=1200, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking_clf(extratree, x_train, y_train, x_valid, \"et\", kf, label_split=label_split)\n",
    "    return et_train, et_test,\"et\"\n",
    "\n",
    "def xgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(xgboost, x_train, y_train, x_valid, \"xgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"xgb\"\n",
    "\n",
    "def lgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(lightgbm, x_train, y_train, x_valid, \"lgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"lgb\"\n",
    "\n",
    "def gnb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gnb=GaussianNB()\n",
    "    gnb_train, gnb_test = stacking_clf(gnb, x_train, y_train, x_valid, \"gnb\", kf, label_split=label_split)\n",
    "    return gnb_train, gnb_test,\"gnb\"\n",
    "\n",
    "def lr_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    logisticregression=LogisticRegression(n_jobs=-1,random_state=2017,C=0.1,max_iter=200)\n",
    "    lr_train, lr_test = stacking_clf(logisticregression, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return lr_train, lr_test, \"lr\"\n",
    "\n",
    "def knn_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    kneighbors=KNeighborsClassifier(n_neighbors=200,n_jobs=-1)\n",
    "    knn_train, knn_test = stacking_clf(kneighbors, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return knn_train, knn_test, \"knn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取训练和验证数据(为stacking特征做准备)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_columns = [c for c in all_data_test.columns if c not in ['label', 'prob', 'seller_path', 'cat_path', 'brand_path', 'action_type_path', 'item_path', 'time_stamp_path']]\n",
    "x_train = all_data_test[~all_data_test['label'].isna()][features_columns].values\n",
    "y_train = all_data_test[~all_data_test['label'].isna()]['label'].values\n",
    "x_valid = all_data_test[all_data_test['label'].isna()][features_columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理函数值inf以及nan情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix(data):\n",
    "    where_are_nan = np.isnan(data)\n",
    "    where_are_inf = np.isinf(data)\n",
    "    data[where_are_nan] = 0\n",
    "    data[where_are_inf] = 0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.float_(get_matrix(np.float_(x_train)))\n",
    "y_train = np.int_(y_train)\n",
    "x_valid = x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入划分数据函数 设stacking特征为5折"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "folds = 5\n",
    "seed = 1\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用lgb和xgb分类模型构造stacking特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_list = [lgb_clf, xgb_clf, lgb_reg, xgb_reg]\n",
    "# clf_list_col = ['lgb_clf', 'xgb_clf', 'lgb_reg', 'xgb_reg']\n",
    "\n",
    "clf_list = [lgb_clf, xgb_clf]\n",
    "clf_list_col = ['lgb_clf', 'xgb_clf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型，获取stacking特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7149\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 127\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.065873\n",
      "[LightGBM] [Info] Start training from score -2.752786\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's multi_logloss: 0.292112\n",
      "lgb now score is: [2.56250120147197]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7188\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 127\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.073243\n",
      "[LightGBM] [Info] Start training from score -2.650371\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's multi_logloss: 0.218491\n",
      "lgb now score is: [2.56250120147197, 2.565956170400075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7166\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 127\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.073243\n",
      "[LightGBM] [Info] Start training from score -2.650371\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_logloss: 0.221528\n",
      "lgb now score is: [2.56250120147197, 2.565956170400075, 2.5049622881701676]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7089\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 126\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.065873\n",
      "[LightGBM] [Info] Start training from score -2.752786\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's multi_logloss: 0.28981\n",
      "lgb now score is: [2.56250120147197, 2.565956170400075, 2.5049622881701676, 2.5875705148000474]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7210\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 126\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.073916\n",
      "[LightGBM] [Info] Start training from score -2.641560\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's multi_logloss: 0.213354\n",
      "lgb now score is: [2.56250120147197, 2.565956170400075, 2.5049622881701676, 2.5875705148000474, 2.56177386068988]\n",
      "lgb_score_list: [2.56250120147197, 2.565956170400075, 2.5049622881701676, 2.5875705148000474, 2.56177386068988]\n",
      "lgb_score_mean: 2.556552807106428\n",
      "[0]\ttrain-mlogloss:0.67087\teval-mlogloss:0.67237\n",
      "[1]\ttrain-mlogloss:0.64996\teval-mlogloss:0.65274\n",
      "[2]\ttrain-mlogloss:0.62993\teval-mlogloss:0.63393\n",
      "[3]\ttrain-mlogloss:0.61127\teval-mlogloss:0.61656\n",
      "[4]\ttrain-mlogloss:0.59351\teval-mlogloss:0.59998\n",
      "[5]\ttrain-mlogloss:0.57657\teval-mlogloss:0.58419\n",
      "[6]\ttrain-mlogloss:0.56064\teval-mlogloss:0.56940\n",
      "[7]\ttrain-mlogloss:0.54550\teval-mlogloss:0.55517\n",
      "[8]\ttrain-mlogloss:0.53123\teval-mlogloss:0.54203\n",
      "[9]\ttrain-mlogloss:0.51758\teval-mlogloss:0.52964\n",
      "[10]\ttrain-mlogloss:0.50460\teval-mlogloss:0.51774\n",
      "[11]\ttrain-mlogloss:0.49242\teval-mlogloss:0.50664\n",
      "[12]\ttrain-mlogloss:0.48057\teval-mlogloss:0.49591\n",
      "[13]\ttrain-mlogloss:0.46934\teval-mlogloss:0.48582\n",
      "[14]\ttrain-mlogloss:0.45863\teval-mlogloss:0.47599\n",
      "[15]\ttrain-mlogloss:0.44839\teval-mlogloss:0.46672\n",
      "[16]\ttrain-mlogloss:0.43870\teval-mlogloss:0.45804\n",
      "[17]\ttrain-mlogloss:0.42953\teval-mlogloss:0.44992\n",
      "[18]\ttrain-mlogloss:0.42068\teval-mlogloss:0.44205\n",
      "[19]\ttrain-mlogloss:0.41222\teval-mlogloss:0.43448\n",
      "[20]\ttrain-mlogloss:0.40404\teval-mlogloss:0.42738\n",
      "[21]\ttrain-mlogloss:0.39585\teval-mlogloss:0.42009\n",
      "[22]\ttrain-mlogloss:0.38835\teval-mlogloss:0.41363\n",
      "[23]\ttrain-mlogloss:0.38110\teval-mlogloss:0.40720\n",
      "[24]\ttrain-mlogloss:0.37426\teval-mlogloss:0.40127\n",
      "[25]\ttrain-mlogloss:0.36764\teval-mlogloss:0.39545\n",
      "[26]\ttrain-mlogloss:0.36136\teval-mlogloss:0.39001\n",
      "[27]\ttrain-mlogloss:0.35543\teval-mlogloss:0.38510\n",
      "[28]\ttrain-mlogloss:0.34956\teval-mlogloss:0.38007\n",
      "[29]\ttrain-mlogloss:0.34398\teval-mlogloss:0.37540\n",
      "[30]\ttrain-mlogloss:0.33860\teval-mlogloss:0.37102\n",
      "[31]\ttrain-mlogloss:0.33369\teval-mlogloss:0.36674\n",
      "[32]\ttrain-mlogloss:0.32884\teval-mlogloss:0.36301\n",
      "[33]\ttrain-mlogloss:0.32425\teval-mlogloss:0.35932\n",
      "[34]\ttrain-mlogloss:0.31967\teval-mlogloss:0.35552\n",
      "[35]\ttrain-mlogloss:0.31517\teval-mlogloss:0.35180\n",
      "[36]\ttrain-mlogloss:0.31082\teval-mlogloss:0.34848\n",
      "[37]\ttrain-mlogloss:0.30683\teval-mlogloss:0.34556\n",
      "[38]\ttrain-mlogloss:0.30291\teval-mlogloss:0.34265\n",
      "[39]\ttrain-mlogloss:0.29930\teval-mlogloss:0.33982\n",
      "[40]\ttrain-mlogloss:0.29583\teval-mlogloss:0.33721\n",
      "[41]\ttrain-mlogloss:0.29235\teval-mlogloss:0.33442\n",
      "[42]\ttrain-mlogloss:0.28903\teval-mlogloss:0.33199\n",
      "[43]\ttrain-mlogloss:0.28586\teval-mlogloss:0.32965\n",
      "[44]\ttrain-mlogloss:0.28278\teval-mlogloss:0.32734\n",
      "[45]\ttrain-mlogloss:0.27969\teval-mlogloss:0.32531\n",
      "[46]\ttrain-mlogloss:0.27693\teval-mlogloss:0.32321\n",
      "[47]\ttrain-mlogloss:0.27422\teval-mlogloss:0.32138\n",
      "[48]\ttrain-mlogloss:0.27160\teval-mlogloss:0.31962\n",
      "[49]\ttrain-mlogloss:0.26900\teval-mlogloss:0.31772\n",
      "[50]\ttrain-mlogloss:0.26655\teval-mlogloss:0.31608\n",
      "[51]\ttrain-mlogloss:0.26403\teval-mlogloss:0.31447\n",
      "[52]\ttrain-mlogloss:0.26161\teval-mlogloss:0.31315\n",
      "[53]\ttrain-mlogloss:0.25944\teval-mlogloss:0.31166\n",
      "[54]\ttrain-mlogloss:0.25732\teval-mlogloss:0.31021\n",
      "[55]\ttrain-mlogloss:0.25527\teval-mlogloss:0.30895\n",
      "[56]\ttrain-mlogloss:0.25333\teval-mlogloss:0.30794\n",
      "[57]\ttrain-mlogloss:0.25131\teval-mlogloss:0.30682\n",
      "[58]\ttrain-mlogloss:0.24934\teval-mlogloss:0.30566\n",
      "[59]\ttrain-mlogloss:0.24751\teval-mlogloss:0.30472\n",
      "[60]\ttrain-mlogloss:0.24569\teval-mlogloss:0.30379\n",
      "[61]\ttrain-mlogloss:0.24412\teval-mlogloss:0.30289\n",
      "[62]\ttrain-mlogloss:0.24235\teval-mlogloss:0.30196\n",
      "[63]\ttrain-mlogloss:0.24056\teval-mlogloss:0.30117\n",
      "[64]\ttrain-mlogloss:0.23889\teval-mlogloss:0.30046\n",
      "[65]\ttrain-mlogloss:0.23735\teval-mlogloss:0.29977\n",
      "[66]\ttrain-mlogloss:0.23609\teval-mlogloss:0.29913\n",
      "[67]\ttrain-mlogloss:0.23458\teval-mlogloss:0.29850\n",
      "[68]\ttrain-mlogloss:0.23334\teval-mlogloss:0.29795\n",
      "[69]\ttrain-mlogloss:0.23201\teval-mlogloss:0.29755\n",
      "[70]\ttrain-mlogloss:0.23083\teval-mlogloss:0.29708\n",
      "[71]\ttrain-mlogloss:0.22953\teval-mlogloss:0.29648\n",
      "[72]\ttrain-mlogloss:0.22821\teval-mlogloss:0.29598\n",
      "[73]\ttrain-mlogloss:0.22703\teval-mlogloss:0.29545\n",
      "[74]\ttrain-mlogloss:0.22580\teval-mlogloss:0.29492\n",
      "[75]\ttrain-mlogloss:0.22466\teval-mlogloss:0.29459\n",
      "[76]\ttrain-mlogloss:0.22359\teval-mlogloss:0.29446\n",
      "[77]\ttrain-mlogloss:0.22245\teval-mlogloss:0.29434\n",
      "[78]\ttrain-mlogloss:0.22131\teval-mlogloss:0.29407\n",
      "[79]\ttrain-mlogloss:0.22030\teval-mlogloss:0.29377\n",
      "[80]\ttrain-mlogloss:0.21938\teval-mlogloss:0.29352\n",
      "[81]\ttrain-mlogloss:0.21855\teval-mlogloss:0.29326\n",
      "[82]\ttrain-mlogloss:0.21758\teval-mlogloss:0.29316\n",
      "[83]\ttrain-mlogloss:0.21663\teval-mlogloss:0.29283\n",
      "[84]\ttrain-mlogloss:0.21583\teval-mlogloss:0.29259\n",
      "[85]\ttrain-mlogloss:0.21490\teval-mlogloss:0.29219\n",
      "[86]\ttrain-mlogloss:0.21400\teval-mlogloss:0.29201\n",
      "[87]\ttrain-mlogloss:0.21322\teval-mlogloss:0.29178\n",
      "[88]\ttrain-mlogloss:0.21235\teval-mlogloss:0.29163\n",
      "[89]\ttrain-mlogloss:0.21170\teval-mlogloss:0.29160\n",
      "[90]\ttrain-mlogloss:0.21092\teval-mlogloss:0.29131\n",
      "[91]\ttrain-mlogloss:0.21018\teval-mlogloss:0.29123\n",
      "[92]\ttrain-mlogloss:0.20926\teval-mlogloss:0.29114\n",
      "[93]\ttrain-mlogloss:0.20861\teval-mlogloss:0.29134\n",
      "[94]\ttrain-mlogloss:0.20790\teval-mlogloss:0.29132\n",
      "[95]\ttrain-mlogloss:0.20726\teval-mlogloss:0.29148\n",
      "[96]\ttrain-mlogloss:0.20649\teval-mlogloss:0.29140\n",
      "[97]\ttrain-mlogloss:0.20588\teval-mlogloss:0.29145\n",
      "[98]\ttrain-mlogloss:0.20516\teval-mlogloss:0.29156\n",
      "[99]\ttrain-mlogloss:0.20454\teval-mlogloss:0.29141\n",
      "[100]\ttrain-mlogloss:0.20386\teval-mlogloss:0.29123\n",
      "[101]\ttrain-mlogloss:0.20325\teval-mlogloss:0.29126\n",
      "[102]\ttrain-mlogloss:0.20252\teval-mlogloss:0.29112\n",
      "[103]\ttrain-mlogloss:0.20191\teval-mlogloss:0.29128\n",
      "[104]\ttrain-mlogloss:0.20136\teval-mlogloss:0.29119\n",
      "[105]\ttrain-mlogloss:0.20076\teval-mlogloss:0.29138\n",
      "[106]\ttrain-mlogloss:0.20009\teval-mlogloss:0.29130\n",
      "[107]\ttrain-mlogloss:0.19950\teval-mlogloss:0.29127\n",
      "[108]\ttrain-mlogloss:0.19875\teval-mlogloss:0.29112\n",
      "[109]\ttrain-mlogloss:0.19837\teval-mlogloss:0.29110\n",
      "[110]\ttrain-mlogloss:0.19776\teval-mlogloss:0.29159\n",
      "[111]\ttrain-mlogloss:0.19739\teval-mlogloss:0.29172\n",
      "[112]\ttrain-mlogloss:0.19679\teval-mlogloss:0.29182\n",
      "[113]\ttrain-mlogloss:0.19632\teval-mlogloss:0.29200\n",
      "[114]\ttrain-mlogloss:0.19580\teval-mlogloss:0.29195\n",
      "[115]\ttrain-mlogloss:0.19520\teval-mlogloss:0.29223\n",
      "[116]\ttrain-mlogloss:0.19448\teval-mlogloss:0.29227\n",
      "[117]\ttrain-mlogloss:0.19389\teval-mlogloss:0.29243\n",
      "[118]\ttrain-mlogloss:0.19339\teval-mlogloss:0.29247\n",
      "[119]\ttrain-mlogloss:0.19286\teval-mlogloss:0.29269\n",
      "[120]\ttrain-mlogloss:0.19230\teval-mlogloss:0.29292\n",
      "[121]\ttrain-mlogloss:0.19177\teval-mlogloss:0.29288\n",
      "[122]\ttrain-mlogloss:0.19130\teval-mlogloss:0.29277\n",
      "[123]\ttrain-mlogloss:0.19066\teval-mlogloss:0.29295\n",
      "[124]\ttrain-mlogloss:0.19012\teval-mlogloss:0.29317\n",
      "[125]\ttrain-mlogloss:0.18981\teval-mlogloss:0.29346\n",
      "[126]\ttrain-mlogloss:0.18936\teval-mlogloss:0.29371\n",
      "[127]\ttrain-mlogloss:0.18885\teval-mlogloss:0.29386\n",
      "[128]\ttrain-mlogloss:0.18845\teval-mlogloss:0.29376\n",
      "[129]\ttrain-mlogloss:0.18800\teval-mlogloss:0.29412\n",
      "[130]\ttrain-mlogloss:0.18752\teval-mlogloss:0.29409\n",
      "[131]\ttrain-mlogloss:0.18703\teval-mlogloss:0.29396\n",
      "[132]\ttrain-mlogloss:0.18647\teval-mlogloss:0.29386\n",
      "[133]\ttrain-mlogloss:0.18607\teval-mlogloss:0.29383\n",
      "[134]\ttrain-mlogloss:0.18568\teval-mlogloss:0.29374\n",
      "[135]\ttrain-mlogloss:0.18529\teval-mlogloss:0.29403\n",
      "[136]\ttrain-mlogloss:0.18489\teval-mlogloss:0.29413\n",
      "[137]\ttrain-mlogloss:0.18449\teval-mlogloss:0.29423\n",
      "[138]\ttrain-mlogloss:0.18408\teval-mlogloss:0.29451\n",
      "[139]\ttrain-mlogloss:0.18382\teval-mlogloss:0.29459\n",
      "[140]\ttrain-mlogloss:0.18332\teval-mlogloss:0.29442\n",
      "[141]\ttrain-mlogloss:0.18278\teval-mlogloss:0.29433\n",
      "[142]\ttrain-mlogloss:0.18239\teval-mlogloss:0.29436\n",
      "[143]\ttrain-mlogloss:0.18185\teval-mlogloss:0.29403\n",
      "[144]\ttrain-mlogloss:0.18142\teval-mlogloss:0.29403\n",
      "[145]\ttrain-mlogloss:0.18085\teval-mlogloss:0.29398\n",
      "[146]\ttrain-mlogloss:0.18044\teval-mlogloss:0.29398\n",
      "[147]\ttrain-mlogloss:0.18015\teval-mlogloss:0.29409\n",
      "[148]\ttrain-mlogloss:0.17977\teval-mlogloss:0.29419\n",
      "[149]\ttrain-mlogloss:0.17922\teval-mlogloss:0.29420\n",
      "[150]\ttrain-mlogloss:0.17888\teval-mlogloss:0.29444\n",
      "[151]\ttrain-mlogloss:0.17849\teval-mlogloss:0.29432\n",
      "[152]\ttrain-mlogloss:0.17812\teval-mlogloss:0.29397\n",
      "[153]\ttrain-mlogloss:0.17774\teval-mlogloss:0.29393\n",
      "[154]\ttrain-mlogloss:0.17728\teval-mlogloss:0.29413\n",
      "[155]\ttrain-mlogloss:0.17695\teval-mlogloss:0.29427\n",
      "[156]\ttrain-mlogloss:0.17663\teval-mlogloss:0.29433\n",
      "[157]\ttrain-mlogloss:0.17622\teval-mlogloss:0.29416\n",
      "[158]\ttrain-mlogloss:0.17586\teval-mlogloss:0.29407\n",
      "[159]\ttrain-mlogloss:0.17550\teval-mlogloss:0.29410\n",
      "[160]\ttrain-mlogloss:0.17511\teval-mlogloss:0.29406\n",
      "[161]\ttrain-mlogloss:0.17472\teval-mlogloss:0.29402\n",
      "[162]\ttrain-mlogloss:0.17407\teval-mlogloss:0.29422\n",
      "[163]\ttrain-mlogloss:0.17375\teval-mlogloss:0.29435\n",
      "[164]\ttrain-mlogloss:0.17348\teval-mlogloss:0.29436\n",
      "[165]\ttrain-mlogloss:0.17313\teval-mlogloss:0.29415\n",
      "[166]\ttrain-mlogloss:0.17274\teval-mlogloss:0.29402\n",
      "[167]\ttrain-mlogloss:0.17239\teval-mlogloss:0.29402\n",
      "[168]\ttrain-mlogloss:0.17202\teval-mlogloss:0.29412\n",
      "[169]\ttrain-mlogloss:0.17168\teval-mlogloss:0.29403\n",
      "[170]\ttrain-mlogloss:0.17131\teval-mlogloss:0.29405\n",
      "[171]\ttrain-mlogloss:0.17093\teval-mlogloss:0.29416\n",
      "[172]\ttrain-mlogloss:0.17041\teval-mlogloss:0.29431\n",
      "[173]\ttrain-mlogloss:0.17017\teval-mlogloss:0.29439\n",
      "[174]\ttrain-mlogloss:0.16986\teval-mlogloss:0.29462\n",
      "[175]\ttrain-mlogloss:0.16958\teval-mlogloss:0.29482\n",
      "[176]\ttrain-mlogloss:0.16915\teval-mlogloss:0.29493\n",
      "[177]\ttrain-mlogloss:0.16883\teval-mlogloss:0.29506\n",
      "[178]\ttrain-mlogloss:0.16847\teval-mlogloss:0.29496\n",
      "[179]\ttrain-mlogloss:0.16803\teval-mlogloss:0.29482\n",
      "[180]\ttrain-mlogloss:0.16764\teval-mlogloss:0.29470\n",
      "[181]\ttrain-mlogloss:0.16731\teval-mlogloss:0.29467\n",
      "[182]\ttrain-mlogloss:0.16712\teval-mlogloss:0.29474\n",
      "[183]\ttrain-mlogloss:0.16685\teval-mlogloss:0.29483\n",
      "[184]\ttrain-mlogloss:0.16636\teval-mlogloss:0.29528\n",
      "[185]\ttrain-mlogloss:0.16600\teval-mlogloss:0.29543\n",
      "[186]\ttrain-mlogloss:0.16566\teval-mlogloss:0.29544\n",
      "[187]\ttrain-mlogloss:0.16529\teval-mlogloss:0.29536\n",
      "[188]\ttrain-mlogloss:0.16503\teval-mlogloss:0.29544\n",
      "[189]\ttrain-mlogloss:0.16477\teval-mlogloss:0.29560\n",
      "[190]\ttrain-mlogloss:0.16437\teval-mlogloss:0.29562\n",
      "[191]\ttrain-mlogloss:0.16396\teval-mlogloss:0.29552\n",
      "[192]\ttrain-mlogloss:0.16371\teval-mlogloss:0.29547\n",
      "[193]\ttrain-mlogloss:0.16348\teval-mlogloss:0.29521\n",
      "[194]\ttrain-mlogloss:0.16323\teval-mlogloss:0.29503\n",
      "[195]\ttrain-mlogloss:0.16298\teval-mlogloss:0.29538\n",
      "[196]\ttrain-mlogloss:0.16261\teval-mlogloss:0.29542\n",
      "[197]\ttrain-mlogloss:0.16242\teval-mlogloss:0.29536\n",
      "[198]\ttrain-mlogloss:0.16228\teval-mlogloss:0.29546\n",
      "[199]\ttrain-mlogloss:0.16207\teval-mlogloss:0.29569\n",
      "[200]\ttrain-mlogloss:0.16170\teval-mlogloss:0.29567\n",
      "[201]\ttrain-mlogloss:0.16136\teval-mlogloss:0.29582\n",
      "[202]\ttrain-mlogloss:0.16117\teval-mlogloss:0.29584\n",
      "[203]\ttrain-mlogloss:0.16082\teval-mlogloss:0.29578\n",
      "[204]\ttrain-mlogloss:0.16052\teval-mlogloss:0.29604\n",
      "[205]\ttrain-mlogloss:0.16018\teval-mlogloss:0.29618\n",
      "[206]\ttrain-mlogloss:0.15996\teval-mlogloss:0.29624\n",
      "[207]\ttrain-mlogloss:0.15977\teval-mlogloss:0.29613\n",
      "[208]\ttrain-mlogloss:0.15952\teval-mlogloss:0.29604\n",
      "[209]\ttrain-mlogloss:0.15925\teval-mlogloss:0.29624\n",
      "xgb now score is: [2.388448718059808]\n",
      "[0]\ttrain-mlogloss:0.67139\teval-mlogloss:0.67102\n",
      "[1]\ttrain-mlogloss:0.65112\teval-mlogloss:0.65005\n",
      "[2]\ttrain-mlogloss:0.63188\teval-mlogloss:0.63041\n",
      "[3]\ttrain-mlogloss:0.61372\teval-mlogloss:0.61212\n",
      "[4]\ttrain-mlogloss:0.59630\teval-mlogloss:0.59435\n",
      "[5]\ttrain-mlogloss:0.58010\teval-mlogloss:0.57771\n",
      "[6]\ttrain-mlogloss:0.56475\teval-mlogloss:0.56226\n",
      "[7]\ttrain-mlogloss:0.55010\teval-mlogloss:0.54728\n",
      "[8]\ttrain-mlogloss:0.53611\teval-mlogloss:0.53295\n",
      "[9]\ttrain-mlogloss:0.52280\teval-mlogloss:0.51917\n",
      "[10]\ttrain-mlogloss:0.51024\teval-mlogloss:0.50636\n",
      "[11]\ttrain-mlogloss:0.49831\teval-mlogloss:0.49419\n",
      "[12]\ttrain-mlogloss:0.48691\teval-mlogloss:0.48255\n",
      "[13]\ttrain-mlogloss:0.47615\teval-mlogloss:0.47158\n",
      "[14]\ttrain-mlogloss:0.46558\teval-mlogloss:0.46069\n",
      "[15]\ttrain-mlogloss:0.45574\teval-mlogloss:0.45075\n",
      "[16]\ttrain-mlogloss:0.44633\teval-mlogloss:0.44103\n",
      "[17]\ttrain-mlogloss:0.43730\teval-mlogloss:0.43189\n",
      "[18]\ttrain-mlogloss:0.42868\teval-mlogloss:0.42308\n",
      "[19]\ttrain-mlogloss:0.42045\teval-mlogloss:0.41484\n",
      "[20]\ttrain-mlogloss:0.41264\teval-mlogloss:0.40681\n",
      "[21]\ttrain-mlogloss:0.40497\teval-mlogloss:0.39882\n",
      "[22]\ttrain-mlogloss:0.39787\teval-mlogloss:0.39140\n",
      "[23]\ttrain-mlogloss:0.39097\teval-mlogloss:0.38418\n",
      "[24]\ttrain-mlogloss:0.38418\teval-mlogloss:0.37729\n",
      "[25]\ttrain-mlogloss:0.37801\teval-mlogloss:0.37099\n",
      "[26]\ttrain-mlogloss:0.37200\teval-mlogloss:0.36490\n",
      "[27]\ttrain-mlogloss:0.36625\teval-mlogloss:0.35894\n",
      "[28]\ttrain-mlogloss:0.36090\teval-mlogloss:0.35356\n",
      "[29]\ttrain-mlogloss:0.35565\teval-mlogloss:0.34820\n",
      "[30]\ttrain-mlogloss:0.35058\teval-mlogloss:0.34297\n",
      "[31]\ttrain-mlogloss:0.34577\teval-mlogloss:0.33802\n",
      "[32]\ttrain-mlogloss:0.34105\teval-mlogloss:0.33321\n",
      "[33]\ttrain-mlogloss:0.33653\teval-mlogloss:0.32877\n",
      "[34]\ttrain-mlogloss:0.33219\teval-mlogloss:0.32447\n",
      "[35]\ttrain-mlogloss:0.32816\teval-mlogloss:0.32027\n",
      "[36]\ttrain-mlogloss:0.32405\teval-mlogloss:0.31609\n",
      "[37]\ttrain-mlogloss:0.32020\teval-mlogloss:0.31219\n",
      "[38]\ttrain-mlogloss:0.31652\teval-mlogloss:0.30853\n",
      "[39]\ttrain-mlogloss:0.31293\teval-mlogloss:0.30510\n",
      "[40]\ttrain-mlogloss:0.30944\teval-mlogloss:0.30170\n",
      "[41]\ttrain-mlogloss:0.30600\teval-mlogloss:0.29817\n",
      "[42]\ttrain-mlogloss:0.30294\teval-mlogloss:0.29520\n",
      "[43]\ttrain-mlogloss:0.29993\teval-mlogloss:0.29215\n",
      "[44]\ttrain-mlogloss:0.29711\teval-mlogloss:0.28921\n",
      "[45]\ttrain-mlogloss:0.29422\teval-mlogloss:0.28647\n",
      "[46]\ttrain-mlogloss:0.29164\teval-mlogloss:0.28398\n",
      "[47]\ttrain-mlogloss:0.28907\teval-mlogloss:0.28128\n",
      "[48]\ttrain-mlogloss:0.28654\teval-mlogloss:0.27874\n",
      "[49]\ttrain-mlogloss:0.28404\teval-mlogloss:0.27624\n",
      "[50]\ttrain-mlogloss:0.28162\teval-mlogloss:0.27405\n",
      "[51]\ttrain-mlogloss:0.27924\teval-mlogloss:0.27172\n",
      "[52]\ttrain-mlogloss:0.27720\teval-mlogloss:0.26962\n",
      "[53]\ttrain-mlogloss:0.27488\teval-mlogloss:0.26736\n",
      "[54]\ttrain-mlogloss:0.27271\teval-mlogloss:0.26532\n",
      "[55]\ttrain-mlogloss:0.27072\teval-mlogloss:0.26341\n",
      "[56]\ttrain-mlogloss:0.26882\teval-mlogloss:0.26186\n",
      "[57]\ttrain-mlogloss:0.26715\teval-mlogloss:0.26025\n",
      "[58]\ttrain-mlogloss:0.26522\teval-mlogloss:0.25862\n",
      "[59]\ttrain-mlogloss:0.26349\teval-mlogloss:0.25706\n",
      "[60]\ttrain-mlogloss:0.26168\teval-mlogloss:0.25541\n",
      "[61]\ttrain-mlogloss:0.26019\teval-mlogloss:0.25396\n",
      "[62]\ttrain-mlogloss:0.25877\teval-mlogloss:0.25267\n",
      "[63]\ttrain-mlogloss:0.25732\teval-mlogloss:0.25113\n",
      "[64]\ttrain-mlogloss:0.25571\teval-mlogloss:0.24975\n",
      "[65]\ttrain-mlogloss:0.25428\teval-mlogloss:0.24856\n",
      "[66]\ttrain-mlogloss:0.25303\teval-mlogloss:0.24721\n",
      "[67]\ttrain-mlogloss:0.25174\teval-mlogloss:0.24598\n",
      "[68]\ttrain-mlogloss:0.25048\teval-mlogloss:0.24485\n",
      "[69]\ttrain-mlogloss:0.24926\teval-mlogloss:0.24384\n",
      "[70]\ttrain-mlogloss:0.24807\teval-mlogloss:0.24274\n",
      "[71]\ttrain-mlogloss:0.24690\teval-mlogloss:0.24186\n",
      "[72]\ttrain-mlogloss:0.24564\teval-mlogloss:0.24120\n",
      "[73]\ttrain-mlogloss:0.24460\teval-mlogloss:0.24027\n",
      "[74]\ttrain-mlogloss:0.24355\teval-mlogloss:0.23959\n",
      "[75]\ttrain-mlogloss:0.24257\teval-mlogloss:0.23869\n",
      "[76]\ttrain-mlogloss:0.24145\teval-mlogloss:0.23790\n",
      "[77]\ttrain-mlogloss:0.24019\teval-mlogloss:0.23704\n",
      "[78]\ttrain-mlogloss:0.23929\teval-mlogloss:0.23617\n",
      "[79]\ttrain-mlogloss:0.23840\teval-mlogloss:0.23548\n",
      "[80]\ttrain-mlogloss:0.23734\teval-mlogloss:0.23480\n",
      "[81]\ttrain-mlogloss:0.23651\teval-mlogloss:0.23412\n",
      "[82]\ttrain-mlogloss:0.23552\teval-mlogloss:0.23354\n",
      "[83]\ttrain-mlogloss:0.23447\teval-mlogloss:0.23284\n",
      "[84]\ttrain-mlogloss:0.23367\teval-mlogloss:0.23226\n",
      "[85]\ttrain-mlogloss:0.23260\teval-mlogloss:0.23169\n",
      "[86]\ttrain-mlogloss:0.23180\teval-mlogloss:0.23118\n",
      "[87]\ttrain-mlogloss:0.23096\teval-mlogloss:0.23058\n",
      "[88]\ttrain-mlogloss:0.23031\teval-mlogloss:0.23002\n",
      "[89]\ttrain-mlogloss:0.22941\teval-mlogloss:0.22960\n",
      "[90]\ttrain-mlogloss:0.22870\teval-mlogloss:0.22908\n",
      "[91]\ttrain-mlogloss:0.22789\teval-mlogloss:0.22860\n",
      "[92]\ttrain-mlogloss:0.22695\teval-mlogloss:0.22833\n",
      "[93]\ttrain-mlogloss:0.22623\teval-mlogloss:0.22795\n",
      "[94]\ttrain-mlogloss:0.22559\teval-mlogloss:0.22763\n",
      "[95]\ttrain-mlogloss:0.22480\teval-mlogloss:0.22722\n",
      "[96]\ttrain-mlogloss:0.22418\teval-mlogloss:0.22690\n",
      "[97]\ttrain-mlogloss:0.22355\teval-mlogloss:0.22650\n",
      "[98]\ttrain-mlogloss:0.22287\teval-mlogloss:0.22629\n",
      "[99]\ttrain-mlogloss:0.22216\teval-mlogloss:0.22612\n",
      "[100]\ttrain-mlogloss:0.22157\teval-mlogloss:0.22584\n",
      "[101]\ttrain-mlogloss:0.22117\teval-mlogloss:0.22555\n",
      "[102]\ttrain-mlogloss:0.22037\teval-mlogloss:0.22547\n",
      "[103]\ttrain-mlogloss:0.21969\teval-mlogloss:0.22526\n",
      "[104]\ttrain-mlogloss:0.21887\teval-mlogloss:0.22519\n",
      "[105]\ttrain-mlogloss:0.21821\teval-mlogloss:0.22499\n",
      "[106]\ttrain-mlogloss:0.21735\teval-mlogloss:0.22487\n",
      "[107]\ttrain-mlogloss:0.21669\teval-mlogloss:0.22452\n",
      "[108]\ttrain-mlogloss:0.21595\teval-mlogloss:0.22438\n",
      "[109]\ttrain-mlogloss:0.21546\teval-mlogloss:0.22422\n",
      "[110]\ttrain-mlogloss:0.21473\teval-mlogloss:0.22391\n",
      "[111]\ttrain-mlogloss:0.21420\teval-mlogloss:0.22369\n",
      "[112]\ttrain-mlogloss:0.21361\teval-mlogloss:0.22372\n",
      "[113]\ttrain-mlogloss:0.21298\teval-mlogloss:0.22381\n",
      "[114]\ttrain-mlogloss:0.21237\teval-mlogloss:0.22386\n",
      "[115]\ttrain-mlogloss:0.21186\teval-mlogloss:0.22372\n",
      "[116]\ttrain-mlogloss:0.21120\teval-mlogloss:0.22360\n",
      "[117]\ttrain-mlogloss:0.21054\teval-mlogloss:0.22343\n",
      "[118]\ttrain-mlogloss:0.20994\teval-mlogloss:0.22339\n",
      "[119]\ttrain-mlogloss:0.20938\teval-mlogloss:0.22336\n",
      "[120]\ttrain-mlogloss:0.20892\teval-mlogloss:0.22337\n",
      "[121]\ttrain-mlogloss:0.20838\teval-mlogloss:0.22324\n",
      "[122]\ttrain-mlogloss:0.20769\teval-mlogloss:0.22332\n",
      "[123]\ttrain-mlogloss:0.20725\teval-mlogloss:0.22326\n",
      "[124]\ttrain-mlogloss:0.20673\teval-mlogloss:0.22310\n",
      "[125]\ttrain-mlogloss:0.20611\teval-mlogloss:0.22291\n",
      "[126]\ttrain-mlogloss:0.20553\teval-mlogloss:0.22260\n",
      "[127]\ttrain-mlogloss:0.20488\teval-mlogloss:0.22256\n",
      "[128]\ttrain-mlogloss:0.20429\teval-mlogloss:0.22226\n",
      "[129]\ttrain-mlogloss:0.20364\teval-mlogloss:0.22214\n",
      "[130]\ttrain-mlogloss:0.20312\teval-mlogloss:0.22213\n",
      "[131]\ttrain-mlogloss:0.20262\teval-mlogloss:0.22202\n",
      "[132]\ttrain-mlogloss:0.20223\teval-mlogloss:0.22184\n",
      "[133]\ttrain-mlogloss:0.20181\teval-mlogloss:0.22179\n",
      "[134]\ttrain-mlogloss:0.20130\teval-mlogloss:0.22199\n",
      "[135]\ttrain-mlogloss:0.20087\teval-mlogloss:0.22200\n",
      "[136]\ttrain-mlogloss:0.20043\teval-mlogloss:0.22218\n",
      "[137]\ttrain-mlogloss:0.20001\teval-mlogloss:0.22240\n",
      "[138]\ttrain-mlogloss:0.19965\teval-mlogloss:0.22255\n",
      "[139]\ttrain-mlogloss:0.19906\teval-mlogloss:0.22291\n",
      "[140]\ttrain-mlogloss:0.19848\teval-mlogloss:0.22289\n",
      "[141]\ttrain-mlogloss:0.19810\teval-mlogloss:0.22285\n",
      "[142]\ttrain-mlogloss:0.19761\teval-mlogloss:0.22286\n",
      "[143]\ttrain-mlogloss:0.19716\teval-mlogloss:0.22268\n",
      "[144]\ttrain-mlogloss:0.19671\teval-mlogloss:0.22255\n",
      "[145]\ttrain-mlogloss:0.19631\teval-mlogloss:0.22264\n",
      "[146]\ttrain-mlogloss:0.19574\teval-mlogloss:0.22292\n",
      "[147]\ttrain-mlogloss:0.19538\teval-mlogloss:0.22284\n",
      "[148]\ttrain-mlogloss:0.19495\teval-mlogloss:0.22285\n",
      "[149]\ttrain-mlogloss:0.19448\teval-mlogloss:0.22313\n",
      "[150]\ttrain-mlogloss:0.19404\teval-mlogloss:0.22312\n",
      "[151]\ttrain-mlogloss:0.19359\teval-mlogloss:0.22321\n",
      "[152]\ttrain-mlogloss:0.19311\teval-mlogloss:0.22307\n",
      "[153]\ttrain-mlogloss:0.19286\teval-mlogloss:0.22308\n",
      "[154]\ttrain-mlogloss:0.19262\teval-mlogloss:0.22310\n",
      "[155]\ttrain-mlogloss:0.19221\teval-mlogloss:0.22319\n",
      "[156]\ttrain-mlogloss:0.19173\teval-mlogloss:0.22322\n",
      "[157]\ttrain-mlogloss:0.19139\teval-mlogloss:0.22339\n",
      "[158]\ttrain-mlogloss:0.19085\teval-mlogloss:0.22333\n",
      "[159]\ttrain-mlogloss:0.19039\teval-mlogloss:0.22336\n",
      "[160]\ttrain-mlogloss:0.19006\teval-mlogloss:0.22318\n",
      "[161]\ttrain-mlogloss:0.18965\teval-mlogloss:0.22316\n",
      "[162]\ttrain-mlogloss:0.18930\teval-mlogloss:0.22322\n",
      "[163]\ttrain-mlogloss:0.18882\teval-mlogloss:0.22320\n",
      "[164]\ttrain-mlogloss:0.18833\teval-mlogloss:0.22322\n",
      "[165]\ttrain-mlogloss:0.18787\teval-mlogloss:0.22296\n",
      "[166]\ttrain-mlogloss:0.18759\teval-mlogloss:0.22320\n",
      "[167]\ttrain-mlogloss:0.18712\teval-mlogloss:0.22315\n",
      "[168]\ttrain-mlogloss:0.18685\teval-mlogloss:0.22306\n",
      "[169]\ttrain-mlogloss:0.18648\teval-mlogloss:0.22333\n",
      "[170]\ttrain-mlogloss:0.18617\teval-mlogloss:0.22314\n",
      "[171]\ttrain-mlogloss:0.18573\teval-mlogloss:0.22325\n",
      "[172]\ttrain-mlogloss:0.18526\teval-mlogloss:0.22313\n",
      "[173]\ttrain-mlogloss:0.18489\teval-mlogloss:0.22325\n",
      "[174]\ttrain-mlogloss:0.18442\teval-mlogloss:0.22352\n",
      "[175]\ttrain-mlogloss:0.18399\teval-mlogloss:0.22335\n",
      "[176]\ttrain-mlogloss:0.18352\teval-mlogloss:0.22308\n",
      "[177]\ttrain-mlogloss:0.18308\teval-mlogloss:0.22314\n",
      "[178]\ttrain-mlogloss:0.18271\teval-mlogloss:0.22292\n",
      "[179]\ttrain-mlogloss:0.18235\teval-mlogloss:0.22330\n",
      "[180]\ttrain-mlogloss:0.18197\teval-mlogloss:0.22343\n",
      "[181]\ttrain-mlogloss:0.18158\teval-mlogloss:0.22324\n",
      "[182]\ttrain-mlogloss:0.18130\teval-mlogloss:0.22308\n",
      "[183]\ttrain-mlogloss:0.18096\teval-mlogloss:0.22312\n",
      "[184]\ttrain-mlogloss:0.18058\teval-mlogloss:0.22305\n",
      "[185]\ttrain-mlogloss:0.18008\teval-mlogloss:0.22298\n",
      "[186]\ttrain-mlogloss:0.17973\teval-mlogloss:0.22281\n",
      "[187]\ttrain-mlogloss:0.17938\teval-mlogloss:0.22283\n",
      "[188]\ttrain-mlogloss:0.17902\teval-mlogloss:0.22307\n",
      "[189]\ttrain-mlogloss:0.17876\teval-mlogloss:0.22302\n",
      "[190]\ttrain-mlogloss:0.17842\teval-mlogloss:0.22291\n",
      "[191]\ttrain-mlogloss:0.17798\teval-mlogloss:0.22302\n",
      "[192]\ttrain-mlogloss:0.17770\teval-mlogloss:0.22304\n",
      "[193]\ttrain-mlogloss:0.17740\teval-mlogloss:0.22301\n",
      "[194]\ttrain-mlogloss:0.17726\teval-mlogloss:0.22279\n",
      "[195]\ttrain-mlogloss:0.17694\teval-mlogloss:0.22309\n",
      "[196]\ttrain-mlogloss:0.17660\teval-mlogloss:0.22321\n",
      "[197]\ttrain-mlogloss:0.17630\teval-mlogloss:0.22328\n",
      "[198]\ttrain-mlogloss:0.17606\teval-mlogloss:0.22327\n",
      "[199]\ttrain-mlogloss:0.17575\teval-mlogloss:0.22313\n",
      "[200]\ttrain-mlogloss:0.17558\teval-mlogloss:0.22326\n",
      "[201]\ttrain-mlogloss:0.17532\teval-mlogloss:0.22336\n",
      "[202]\ttrain-mlogloss:0.17501\teval-mlogloss:0.22320\n",
      "[203]\ttrain-mlogloss:0.17470\teval-mlogloss:0.22319\n",
      "[204]\ttrain-mlogloss:0.17443\teval-mlogloss:0.22337\n",
      "[205]\ttrain-mlogloss:0.17417\teval-mlogloss:0.22354\n",
      "[206]\ttrain-mlogloss:0.17384\teval-mlogloss:0.22355\n",
      "[207]\ttrain-mlogloss:0.17359\teval-mlogloss:0.22357\n",
      "[208]\ttrain-mlogloss:0.17329\teval-mlogloss:0.22356\n",
      "[209]\ttrain-mlogloss:0.17295\teval-mlogloss:0.22346\n",
      "[210]\ttrain-mlogloss:0.17261\teval-mlogloss:0.22356\n",
      "[211]\ttrain-mlogloss:0.17238\teval-mlogloss:0.22357\n",
      "[212]\ttrain-mlogloss:0.17215\teval-mlogloss:0.22370\n",
      "[213]\ttrain-mlogloss:0.17178\teval-mlogloss:0.22374\n",
      "[214]\ttrain-mlogloss:0.17160\teval-mlogloss:0.22359\n",
      "[215]\ttrain-mlogloss:0.17133\teval-mlogloss:0.22360\n",
      "[216]\ttrain-mlogloss:0.17101\teval-mlogloss:0.22361\n",
      "[217]\ttrain-mlogloss:0.17077\teval-mlogloss:0.22359\n",
      "[218]\ttrain-mlogloss:0.17044\teval-mlogloss:0.22362\n",
      "[219]\ttrain-mlogloss:0.17018\teval-mlogloss:0.22360\n",
      "[220]\ttrain-mlogloss:0.16996\teval-mlogloss:0.22365\n",
      "[221]\ttrain-mlogloss:0.16958\teval-mlogloss:0.22359\n",
      "[222]\ttrain-mlogloss:0.16937\teval-mlogloss:0.22353\n",
      "[223]\ttrain-mlogloss:0.16910\teval-mlogloss:0.22379\n",
      "[224]\ttrain-mlogloss:0.16879\teval-mlogloss:0.22365\n",
      "[225]\ttrain-mlogloss:0.16849\teval-mlogloss:0.22360\n",
      "[226]\ttrain-mlogloss:0.16818\teval-mlogloss:0.22352\n",
      "[227]\ttrain-mlogloss:0.16795\teval-mlogloss:0.22360\n",
      "[228]\ttrain-mlogloss:0.16768\teval-mlogloss:0.22372\n",
      "[229]\ttrain-mlogloss:0.16731\teval-mlogloss:0.22373\n",
      "[230]\ttrain-mlogloss:0.16713\teval-mlogloss:0.22368\n",
      "[231]\ttrain-mlogloss:0.16689\teval-mlogloss:0.22367\n",
      "[232]\ttrain-mlogloss:0.16652\teval-mlogloss:0.22368\n",
      "[233]\ttrain-mlogloss:0.16626\teval-mlogloss:0.22368\n",
      "xgb now score is: [2.388448718059808, 2.5234747524932026]\n",
      "[0]\ttrain-mlogloss:0.67116\teval-mlogloss:0.67106\n",
      "[1]\ttrain-mlogloss:0.65073\teval-mlogloss:0.65023\n",
      "[2]\ttrain-mlogloss:0.63130\teval-mlogloss:0.63045\n",
      "[3]\ttrain-mlogloss:0.61306\teval-mlogloss:0.61196\n",
      "[4]\ttrain-mlogloss:0.59568\teval-mlogloss:0.59445\n",
      "[5]\ttrain-mlogloss:0.57949\teval-mlogloss:0.57802\n",
      "[6]\ttrain-mlogloss:0.56378\teval-mlogloss:0.56207\n",
      "[7]\ttrain-mlogloss:0.54902\teval-mlogloss:0.54733\n",
      "[8]\ttrain-mlogloss:0.53514\teval-mlogloss:0.53317\n",
      "[9]\ttrain-mlogloss:0.52199\teval-mlogloss:0.51982\n",
      "[10]\ttrain-mlogloss:0.50936\teval-mlogloss:0.50694\n",
      "[11]\ttrain-mlogloss:0.49728\teval-mlogloss:0.49447\n",
      "[12]\ttrain-mlogloss:0.48574\teval-mlogloss:0.48275\n",
      "[13]\ttrain-mlogloss:0.47466\teval-mlogloss:0.47152\n",
      "[14]\ttrain-mlogloss:0.46419\teval-mlogloss:0.46093\n",
      "[15]\ttrain-mlogloss:0.45431\teval-mlogloss:0.45103\n",
      "[16]\ttrain-mlogloss:0.44479\teval-mlogloss:0.44134\n",
      "[17]\ttrain-mlogloss:0.43574\teval-mlogloss:0.43219\n",
      "[18]\ttrain-mlogloss:0.42713\teval-mlogloss:0.42356\n",
      "[19]\ttrain-mlogloss:0.41891\teval-mlogloss:0.41525\n",
      "[20]\ttrain-mlogloss:0.41098\teval-mlogloss:0.40734\n",
      "[21]\ttrain-mlogloss:0.40349\teval-mlogloss:0.39976\n",
      "[22]\ttrain-mlogloss:0.39622\teval-mlogloss:0.39235\n",
      "[23]\ttrain-mlogloss:0.38913\teval-mlogloss:0.38523\n",
      "[24]\ttrain-mlogloss:0.38250\teval-mlogloss:0.37852\n",
      "[25]\ttrain-mlogloss:0.37609\teval-mlogloss:0.37215\n",
      "[26]\ttrain-mlogloss:0.36996\teval-mlogloss:0.36602\n",
      "[27]\ttrain-mlogloss:0.36410\teval-mlogloss:0.36033\n",
      "[28]\ttrain-mlogloss:0.35842\teval-mlogloss:0.35463\n",
      "[29]\ttrain-mlogloss:0.35302\teval-mlogloss:0.34918\n",
      "[30]\ttrain-mlogloss:0.34785\teval-mlogloss:0.34399\n",
      "[31]\ttrain-mlogloss:0.34286\teval-mlogloss:0.33903\n",
      "[32]\ttrain-mlogloss:0.33815\teval-mlogloss:0.33451\n",
      "[33]\ttrain-mlogloss:0.33360\teval-mlogloss:0.32996\n",
      "[34]\ttrain-mlogloss:0.32921\teval-mlogloss:0.32561\n",
      "[35]\ttrain-mlogloss:0.32516\teval-mlogloss:0.32135\n",
      "[36]\ttrain-mlogloss:0.32093\teval-mlogloss:0.31739\n",
      "[37]\ttrain-mlogloss:0.31712\teval-mlogloss:0.31354\n",
      "[38]\ttrain-mlogloss:0.31343\teval-mlogloss:0.31008\n",
      "[39]\ttrain-mlogloss:0.30988\teval-mlogloss:0.30651\n",
      "[40]\ttrain-mlogloss:0.30630\teval-mlogloss:0.30306\n",
      "[41]\ttrain-mlogloss:0.30290\teval-mlogloss:0.29967\n",
      "[42]\ttrain-mlogloss:0.29981\teval-mlogloss:0.29656\n",
      "[43]\ttrain-mlogloss:0.29665\teval-mlogloss:0.29339\n",
      "[44]\ttrain-mlogloss:0.29373\teval-mlogloss:0.29073\n",
      "[45]\ttrain-mlogloss:0.29074\teval-mlogloss:0.28797\n",
      "[46]\ttrain-mlogloss:0.28787\teval-mlogloss:0.28547\n",
      "[47]\ttrain-mlogloss:0.28535\teval-mlogloss:0.28296\n",
      "[48]\ttrain-mlogloss:0.28277\teval-mlogloss:0.28077\n",
      "[49]\ttrain-mlogloss:0.28023\teval-mlogloss:0.27841\n",
      "[50]\ttrain-mlogloss:0.27788\teval-mlogloss:0.27596\n",
      "[51]\ttrain-mlogloss:0.27558\teval-mlogloss:0.27389\n",
      "[52]\ttrain-mlogloss:0.27335\teval-mlogloss:0.27197\n",
      "[53]\ttrain-mlogloss:0.27097\teval-mlogloss:0.26989\n",
      "[54]\ttrain-mlogloss:0.26882\teval-mlogloss:0.26834\n",
      "[55]\ttrain-mlogloss:0.26672\teval-mlogloss:0.26648\n",
      "[56]\ttrain-mlogloss:0.26460\teval-mlogloss:0.26494\n",
      "[57]\ttrain-mlogloss:0.26278\teval-mlogloss:0.26341\n",
      "[58]\ttrain-mlogloss:0.26104\teval-mlogloss:0.26195\n",
      "[59]\ttrain-mlogloss:0.25927\teval-mlogloss:0.26063\n",
      "[60]\ttrain-mlogloss:0.25736\teval-mlogloss:0.25942\n",
      "[61]\ttrain-mlogloss:0.25577\teval-mlogloss:0.25812\n",
      "[62]\ttrain-mlogloss:0.25423\teval-mlogloss:0.25694\n",
      "[63]\ttrain-mlogloss:0.25267\teval-mlogloss:0.25601\n",
      "[64]\ttrain-mlogloss:0.25100\teval-mlogloss:0.25488\n",
      "[65]\ttrain-mlogloss:0.24943\teval-mlogloss:0.25383\n",
      "[66]\ttrain-mlogloss:0.24794\teval-mlogloss:0.25284\n",
      "[67]\ttrain-mlogloss:0.24662\teval-mlogloss:0.25160\n",
      "[68]\ttrain-mlogloss:0.24523\teval-mlogloss:0.25079\n",
      "[69]\ttrain-mlogloss:0.24416\teval-mlogloss:0.24973\n",
      "[70]\ttrain-mlogloss:0.24270\teval-mlogloss:0.24910\n",
      "[71]\ttrain-mlogloss:0.24144\teval-mlogloss:0.24848\n",
      "[72]\ttrain-mlogloss:0.24025\teval-mlogloss:0.24777\n",
      "[73]\ttrain-mlogloss:0.23900\teval-mlogloss:0.24695\n",
      "[74]\ttrain-mlogloss:0.23769\teval-mlogloss:0.24626\n",
      "[75]\ttrain-mlogloss:0.23654\teval-mlogloss:0.24569\n",
      "[76]\ttrain-mlogloss:0.23542\teval-mlogloss:0.24502\n",
      "[77]\ttrain-mlogloss:0.23439\teval-mlogloss:0.24445\n",
      "[78]\ttrain-mlogloss:0.23330\teval-mlogloss:0.24385\n",
      "[79]\ttrain-mlogloss:0.23215\teval-mlogloss:0.24322\n",
      "[80]\ttrain-mlogloss:0.23122\teval-mlogloss:0.24263\n",
      "[81]\ttrain-mlogloss:0.23035\teval-mlogloss:0.24209\n",
      "[82]\ttrain-mlogloss:0.22904\teval-mlogloss:0.24184\n",
      "[83]\ttrain-mlogloss:0.22797\teval-mlogloss:0.24138\n",
      "[84]\ttrain-mlogloss:0.22696\teval-mlogloss:0.24108\n",
      "[85]\ttrain-mlogloss:0.22596\teval-mlogloss:0.24063\n",
      "[86]\ttrain-mlogloss:0.22490\teval-mlogloss:0.24055\n",
      "[87]\ttrain-mlogloss:0.22411\teval-mlogloss:0.24017\n",
      "[88]\ttrain-mlogloss:0.22332\teval-mlogloss:0.23976\n",
      "[89]\ttrain-mlogloss:0.22258\teval-mlogloss:0.23933\n",
      "[90]\ttrain-mlogloss:0.22174\teval-mlogloss:0.23920\n",
      "[91]\ttrain-mlogloss:0.22090\teval-mlogloss:0.23894\n",
      "[92]\ttrain-mlogloss:0.22011\teval-mlogloss:0.23876\n",
      "[93]\ttrain-mlogloss:0.21941\teval-mlogloss:0.23850\n",
      "[94]\ttrain-mlogloss:0.21852\teval-mlogloss:0.23813\n",
      "[95]\ttrain-mlogloss:0.21800\teval-mlogloss:0.23783\n",
      "[96]\ttrain-mlogloss:0.21720\teval-mlogloss:0.23786\n",
      "[97]\ttrain-mlogloss:0.21640\teval-mlogloss:0.23744\n",
      "[98]\ttrain-mlogloss:0.21581\teval-mlogloss:0.23735\n",
      "[99]\ttrain-mlogloss:0.21497\teval-mlogloss:0.23729\n",
      "[100]\ttrain-mlogloss:0.21415\teval-mlogloss:0.23730\n",
      "[101]\ttrain-mlogloss:0.21358\teval-mlogloss:0.23727\n",
      "[102]\ttrain-mlogloss:0.21300\teval-mlogloss:0.23700\n",
      "[103]\ttrain-mlogloss:0.21231\teval-mlogloss:0.23691\n",
      "[104]\ttrain-mlogloss:0.21161\teval-mlogloss:0.23686\n",
      "[105]\ttrain-mlogloss:0.21065\teval-mlogloss:0.23664\n",
      "[106]\ttrain-mlogloss:0.21013\teval-mlogloss:0.23655\n",
      "[107]\ttrain-mlogloss:0.20951\teval-mlogloss:0.23641\n",
      "[108]\ttrain-mlogloss:0.20887\teval-mlogloss:0.23642\n",
      "[109]\ttrain-mlogloss:0.20837\teval-mlogloss:0.23634\n",
      "[110]\ttrain-mlogloss:0.20765\teval-mlogloss:0.23649\n",
      "[111]\ttrain-mlogloss:0.20716\teval-mlogloss:0.23664\n",
      "[112]\ttrain-mlogloss:0.20665\teval-mlogloss:0.23663\n",
      "[113]\ttrain-mlogloss:0.20608\teval-mlogloss:0.23658\n",
      "[114]\ttrain-mlogloss:0.20549\teval-mlogloss:0.23654\n",
      "[115]\ttrain-mlogloss:0.20479\teval-mlogloss:0.23648\n",
      "[116]\ttrain-mlogloss:0.20418\teval-mlogloss:0.23643\n",
      "[117]\ttrain-mlogloss:0.20348\teval-mlogloss:0.23659\n",
      "[118]\ttrain-mlogloss:0.20297\teval-mlogloss:0.23646\n",
      "[119]\ttrain-mlogloss:0.20231\teval-mlogloss:0.23661\n",
      "[120]\ttrain-mlogloss:0.20170\teval-mlogloss:0.23654\n",
      "[121]\ttrain-mlogloss:0.20114\teval-mlogloss:0.23634\n",
      "[122]\ttrain-mlogloss:0.20035\teval-mlogloss:0.23623\n",
      "[123]\ttrain-mlogloss:0.19978\teval-mlogloss:0.23637\n",
      "[124]\ttrain-mlogloss:0.19927\teval-mlogloss:0.23631\n",
      "[125]\ttrain-mlogloss:0.19877\teval-mlogloss:0.23620\n",
      "[126]\ttrain-mlogloss:0.19809\teval-mlogloss:0.23611\n",
      "[127]\ttrain-mlogloss:0.19771\teval-mlogloss:0.23611\n",
      "[128]\ttrain-mlogloss:0.19712\teval-mlogloss:0.23612\n",
      "[129]\ttrain-mlogloss:0.19642\teval-mlogloss:0.23624\n",
      "[130]\ttrain-mlogloss:0.19588\teval-mlogloss:0.23630\n",
      "[131]\ttrain-mlogloss:0.19534\teval-mlogloss:0.23652\n",
      "[132]\ttrain-mlogloss:0.19466\teval-mlogloss:0.23654\n",
      "[133]\ttrain-mlogloss:0.19413\teval-mlogloss:0.23656\n",
      "[134]\ttrain-mlogloss:0.19363\teval-mlogloss:0.23665\n",
      "[135]\ttrain-mlogloss:0.19319\teval-mlogloss:0.23670\n",
      "[136]\ttrain-mlogloss:0.19268\teval-mlogloss:0.23674\n",
      "[137]\ttrain-mlogloss:0.19226\teval-mlogloss:0.23669\n",
      "[138]\ttrain-mlogloss:0.19171\teval-mlogloss:0.23673\n",
      "[139]\ttrain-mlogloss:0.19126\teval-mlogloss:0.23672\n",
      "[140]\ttrain-mlogloss:0.19075\teval-mlogloss:0.23674\n",
      "[141]\ttrain-mlogloss:0.19040\teval-mlogloss:0.23680\n",
      "[142]\ttrain-mlogloss:0.18981\teval-mlogloss:0.23695\n",
      "[143]\ttrain-mlogloss:0.18929\teval-mlogloss:0.23716\n",
      "[144]\ttrain-mlogloss:0.18885\teval-mlogloss:0.23712\n",
      "[145]\ttrain-mlogloss:0.18831\teval-mlogloss:0.23717\n",
      "[146]\ttrain-mlogloss:0.18796\teval-mlogloss:0.23726\n",
      "[147]\ttrain-mlogloss:0.18761\teval-mlogloss:0.23746\n",
      "[148]\ttrain-mlogloss:0.18723\teval-mlogloss:0.23756\n",
      "[149]\ttrain-mlogloss:0.18671\teval-mlogloss:0.23774\n",
      "[150]\ttrain-mlogloss:0.18630\teval-mlogloss:0.23776\n",
      "[151]\ttrain-mlogloss:0.18589\teval-mlogloss:0.23797\n",
      "[152]\ttrain-mlogloss:0.18527\teval-mlogloss:0.23794\n",
      "[153]\ttrain-mlogloss:0.18483\teval-mlogloss:0.23830\n",
      "[154]\ttrain-mlogloss:0.18439\teval-mlogloss:0.23848\n",
      "[155]\ttrain-mlogloss:0.18389\teval-mlogloss:0.23850\n",
      "[156]\ttrain-mlogloss:0.18351\teval-mlogloss:0.23851\n",
      "[157]\ttrain-mlogloss:0.18305\teval-mlogloss:0.23850\n",
      "[158]\ttrain-mlogloss:0.18258\teval-mlogloss:0.23851\n",
      "[159]\ttrain-mlogloss:0.18223\teval-mlogloss:0.23864\n",
      "[160]\ttrain-mlogloss:0.18185\teval-mlogloss:0.23871\n",
      "[161]\ttrain-mlogloss:0.18147\teval-mlogloss:0.23899\n",
      "[162]\ttrain-mlogloss:0.18121\teval-mlogloss:0.23911\n",
      "[163]\ttrain-mlogloss:0.18076\teval-mlogloss:0.23925\n",
      "[164]\ttrain-mlogloss:0.18045\teval-mlogloss:0.23916\n",
      "[165]\ttrain-mlogloss:0.18008\teval-mlogloss:0.23921\n",
      "[166]\ttrain-mlogloss:0.17969\teval-mlogloss:0.23927\n",
      "[167]\ttrain-mlogloss:0.17917\teval-mlogloss:0.23925\n",
      "[168]\ttrain-mlogloss:0.17890\teval-mlogloss:0.23923\n",
      "[169]\ttrain-mlogloss:0.17853\teval-mlogloss:0.23938\n",
      "[170]\ttrain-mlogloss:0.17809\teval-mlogloss:0.23942\n",
      "[171]\ttrain-mlogloss:0.17770\teval-mlogloss:0.23962\n",
      "[172]\ttrain-mlogloss:0.17739\teval-mlogloss:0.23986\n",
      "[173]\ttrain-mlogloss:0.17704\teval-mlogloss:0.23997\n",
      "[174]\ttrain-mlogloss:0.17679\teval-mlogloss:0.24020\n",
      "[175]\ttrain-mlogloss:0.17640\teval-mlogloss:0.24041\n",
      "[176]\ttrain-mlogloss:0.17614\teval-mlogloss:0.24045\n",
      "[177]\ttrain-mlogloss:0.17578\teval-mlogloss:0.24058\n",
      "[178]\ttrain-mlogloss:0.17535\teval-mlogloss:0.24062\n",
      "[179]\ttrain-mlogloss:0.17499\teval-mlogloss:0.24062\n",
      "[180]\ttrain-mlogloss:0.17465\teval-mlogloss:0.24083\n",
      "[181]\ttrain-mlogloss:0.17428\teval-mlogloss:0.24083\n",
      "[182]\ttrain-mlogloss:0.17393\teval-mlogloss:0.24095\n",
      "[183]\ttrain-mlogloss:0.17360\teval-mlogloss:0.24103\n",
      "[184]\ttrain-mlogloss:0.17331\teval-mlogloss:0.24103\n",
      "[185]\ttrain-mlogloss:0.17305\teval-mlogloss:0.24099\n",
      "[186]\ttrain-mlogloss:0.17269\teval-mlogloss:0.24111\n",
      "[187]\ttrain-mlogloss:0.17243\teval-mlogloss:0.24107\n",
      "[188]\ttrain-mlogloss:0.17205\teval-mlogloss:0.24118\n",
      "[189]\ttrain-mlogloss:0.17151\teval-mlogloss:0.24123\n",
      "[190]\ttrain-mlogloss:0.17108\teval-mlogloss:0.24130\n",
      "[191]\ttrain-mlogloss:0.17068\teval-mlogloss:0.24144\n",
      "[192]\ttrain-mlogloss:0.17056\teval-mlogloss:0.24129\n",
      "[193]\ttrain-mlogloss:0.17022\teval-mlogloss:0.24144\n",
      "[194]\ttrain-mlogloss:0.17000\teval-mlogloss:0.24155\n",
      "[195]\ttrain-mlogloss:0.16988\teval-mlogloss:0.24173\n",
      "[196]\ttrain-mlogloss:0.16955\teval-mlogloss:0.24178\n",
      "[197]\ttrain-mlogloss:0.16924\teval-mlogloss:0.24193\n",
      "[198]\ttrain-mlogloss:0.16890\teval-mlogloss:0.24202\n",
      "[199]\ttrain-mlogloss:0.16863\teval-mlogloss:0.24204\n",
      "[200]\ttrain-mlogloss:0.16832\teval-mlogloss:0.24214\n",
      "[201]\ttrain-mlogloss:0.16799\teval-mlogloss:0.24212\n",
      "[202]\ttrain-mlogloss:0.16778\teval-mlogloss:0.24215\n",
      "[203]\ttrain-mlogloss:0.16747\teval-mlogloss:0.24232\n",
      "[204]\ttrain-mlogloss:0.16720\teval-mlogloss:0.24235\n",
      "[205]\ttrain-mlogloss:0.16691\teval-mlogloss:0.24263\n",
      "[206]\ttrain-mlogloss:0.16659\teval-mlogloss:0.24269\n",
      "[207]\ttrain-mlogloss:0.16629\teval-mlogloss:0.24272\n",
      "[208]\ttrain-mlogloss:0.16598\teval-mlogloss:0.24293\n",
      "[209]\ttrain-mlogloss:0.16573\teval-mlogloss:0.24285\n",
      "[210]\ttrain-mlogloss:0.16548\teval-mlogloss:0.24294\n",
      "[211]\ttrain-mlogloss:0.16515\teval-mlogloss:0.24307\n",
      "[212]\ttrain-mlogloss:0.16476\teval-mlogloss:0.24311\n",
      "[213]\ttrain-mlogloss:0.16444\teval-mlogloss:0.24308\n",
      "[214]\ttrain-mlogloss:0.16416\teval-mlogloss:0.24312\n",
      "[215]\ttrain-mlogloss:0.16400\teval-mlogloss:0.24311\n",
      "[216]\ttrain-mlogloss:0.16376\teval-mlogloss:0.24316\n",
      "[217]\ttrain-mlogloss:0.16345\teval-mlogloss:0.24355\n",
      "[218]\ttrain-mlogloss:0.16321\teval-mlogloss:0.24380\n",
      "[219]\ttrain-mlogloss:0.16306\teval-mlogloss:0.24381\n",
      "[220]\ttrain-mlogloss:0.16279\teval-mlogloss:0.24389\n",
      "[221]\ttrain-mlogloss:0.16259\teval-mlogloss:0.24414\n",
      "[222]\ttrain-mlogloss:0.16226\teval-mlogloss:0.24422\n",
      "[223]\ttrain-mlogloss:0.16203\teval-mlogloss:0.24438\n",
      "[224]\ttrain-mlogloss:0.16171\teval-mlogloss:0.24458\n",
      "[225]\ttrain-mlogloss:0.16144\teval-mlogloss:0.24477\n",
      "[226]\ttrain-mlogloss:0.16121\teval-mlogloss:0.24483\n",
      "xgb now score is: [2.388448718059808, 2.5234747524932026, 2.529490461815149]\n",
      "[0]\ttrain-mlogloss:0.67068\teval-mlogloss:0.67223\n",
      "[1]\ttrain-mlogloss:0.64965\teval-mlogloss:0.65267\n",
      "[2]\ttrain-mlogloss:0.62978\teval-mlogloss:0.63405\n",
      "[3]\ttrain-mlogloss:0.61114\teval-mlogloss:0.61669\n",
      "[4]\ttrain-mlogloss:0.59349\teval-mlogloss:0.60024\n",
      "[5]\ttrain-mlogloss:0.57697\teval-mlogloss:0.58482\n",
      "[6]\ttrain-mlogloss:0.56110\teval-mlogloss:0.57016\n",
      "[7]\ttrain-mlogloss:0.54594\teval-mlogloss:0.55618\n",
      "[8]\ttrain-mlogloss:0.53148\teval-mlogloss:0.54266\n",
      "[9]\ttrain-mlogloss:0.51785\teval-mlogloss:0.53011\n",
      "[10]\ttrain-mlogloss:0.50496\teval-mlogloss:0.51833\n",
      "[11]\ttrain-mlogloss:0.49269\teval-mlogloss:0.50683\n",
      "[12]\ttrain-mlogloss:0.48073\teval-mlogloss:0.49604\n",
      "[13]\ttrain-mlogloss:0.46932\teval-mlogloss:0.48579\n",
      "[14]\ttrain-mlogloss:0.45867\teval-mlogloss:0.47616\n",
      "[15]\ttrain-mlogloss:0.44851\teval-mlogloss:0.46700\n",
      "[16]\ttrain-mlogloss:0.43875\teval-mlogloss:0.45832\n",
      "[17]\ttrain-mlogloss:0.42939\teval-mlogloss:0.44976\n",
      "[18]\ttrain-mlogloss:0.42038\teval-mlogloss:0.44171\n",
      "[19]\ttrain-mlogloss:0.41179\teval-mlogloss:0.43400\n",
      "[20]\ttrain-mlogloss:0.40366\teval-mlogloss:0.42664\n",
      "[21]\ttrain-mlogloss:0.39598\teval-mlogloss:0.41984\n",
      "[22]\ttrain-mlogloss:0.38855\teval-mlogloss:0.41334\n",
      "[23]\ttrain-mlogloss:0.38148\teval-mlogloss:0.40735\n",
      "[24]\ttrain-mlogloss:0.37462\teval-mlogloss:0.40157\n",
      "[25]\ttrain-mlogloss:0.36815\teval-mlogloss:0.39606\n",
      "[26]\ttrain-mlogloss:0.36193\teval-mlogloss:0.39059\n",
      "[27]\ttrain-mlogloss:0.35597\teval-mlogloss:0.38550\n",
      "[28]\ttrain-mlogloss:0.35020\teval-mlogloss:0.38074\n",
      "[29]\ttrain-mlogloss:0.34464\teval-mlogloss:0.37594\n",
      "[30]\ttrain-mlogloss:0.33945\teval-mlogloss:0.37168\n",
      "[31]\ttrain-mlogloss:0.33439\teval-mlogloss:0.36742\n",
      "[32]\ttrain-mlogloss:0.32962\teval-mlogloss:0.36355\n",
      "[33]\ttrain-mlogloss:0.32481\teval-mlogloss:0.35969\n",
      "[34]\ttrain-mlogloss:0.32027\teval-mlogloss:0.35605\n",
      "[35]\ttrain-mlogloss:0.31591\teval-mlogloss:0.35248\n",
      "[36]\ttrain-mlogloss:0.31161\teval-mlogloss:0.34939\n",
      "[37]\ttrain-mlogloss:0.30765\teval-mlogloss:0.34635\n",
      "[38]\ttrain-mlogloss:0.30381\teval-mlogloss:0.34341\n",
      "[39]\ttrain-mlogloss:0.30009\teval-mlogloss:0.34053\n",
      "[40]\ttrain-mlogloss:0.29658\teval-mlogloss:0.33794\n",
      "[41]\ttrain-mlogloss:0.29304\teval-mlogloss:0.33538\n",
      "[42]\ttrain-mlogloss:0.28971\teval-mlogloss:0.33275\n",
      "[43]\ttrain-mlogloss:0.28658\teval-mlogloss:0.33059\n",
      "[44]\ttrain-mlogloss:0.28345\teval-mlogloss:0.32823\n",
      "[45]\ttrain-mlogloss:0.28049\teval-mlogloss:0.32620\n",
      "[46]\ttrain-mlogloss:0.27774\teval-mlogloss:0.32424\n",
      "[47]\ttrain-mlogloss:0.27496\teval-mlogloss:0.32240\n",
      "[48]\ttrain-mlogloss:0.27231\teval-mlogloss:0.32057\n",
      "[49]\ttrain-mlogloss:0.26975\teval-mlogloss:0.31889\n",
      "[50]\ttrain-mlogloss:0.26738\teval-mlogloss:0.31736\n",
      "[51]\ttrain-mlogloss:0.26498\teval-mlogloss:0.31591\n",
      "[52]\ttrain-mlogloss:0.26274\teval-mlogloss:0.31435\n",
      "[53]\ttrain-mlogloss:0.26059\teval-mlogloss:0.31296\n",
      "[54]\ttrain-mlogloss:0.25849\teval-mlogloss:0.31166\n",
      "[55]\ttrain-mlogloss:0.25650\teval-mlogloss:0.31039\n",
      "[56]\ttrain-mlogloss:0.25441\teval-mlogloss:0.30908\n",
      "[57]\ttrain-mlogloss:0.25256\teval-mlogloss:0.30819\n",
      "[58]\ttrain-mlogloss:0.25061\teval-mlogloss:0.30693\n",
      "[59]\ttrain-mlogloss:0.24891\teval-mlogloss:0.30584\n",
      "[60]\ttrain-mlogloss:0.24727\teval-mlogloss:0.30488\n",
      "[61]\ttrain-mlogloss:0.24557\teval-mlogloss:0.30377\n",
      "[62]\ttrain-mlogloss:0.24405\teval-mlogloss:0.30296\n",
      "[63]\ttrain-mlogloss:0.24257\teval-mlogloss:0.30222\n",
      "[64]\ttrain-mlogloss:0.24106\teval-mlogloss:0.30141\n",
      "[65]\ttrain-mlogloss:0.23967\teval-mlogloss:0.30064\n",
      "[66]\ttrain-mlogloss:0.23818\teval-mlogloss:0.30001\n",
      "[67]\ttrain-mlogloss:0.23683\teval-mlogloss:0.29937\n",
      "[68]\ttrain-mlogloss:0.23552\teval-mlogloss:0.29863\n",
      "[69]\ttrain-mlogloss:0.23429\teval-mlogloss:0.29802\n",
      "[70]\ttrain-mlogloss:0.23320\teval-mlogloss:0.29749\n",
      "[71]\ttrain-mlogloss:0.23202\teval-mlogloss:0.29691\n",
      "[72]\ttrain-mlogloss:0.23084\teval-mlogloss:0.29656\n",
      "[73]\ttrain-mlogloss:0.22975\teval-mlogloss:0.29613\n",
      "[74]\ttrain-mlogloss:0.22868\teval-mlogloss:0.29575\n",
      "[75]\ttrain-mlogloss:0.22769\teval-mlogloss:0.29545\n",
      "[76]\ttrain-mlogloss:0.22670\teval-mlogloss:0.29478\n",
      "[77]\ttrain-mlogloss:0.22580\teval-mlogloss:0.29444\n",
      "[78]\ttrain-mlogloss:0.22491\teval-mlogloss:0.29410\n",
      "[79]\ttrain-mlogloss:0.22380\teval-mlogloss:0.29355\n",
      "[80]\ttrain-mlogloss:0.22304\teval-mlogloss:0.29331\n",
      "[81]\ttrain-mlogloss:0.22228\teval-mlogloss:0.29324\n",
      "[82]\ttrain-mlogloss:0.22150\teval-mlogloss:0.29300\n",
      "[83]\ttrain-mlogloss:0.22062\teval-mlogloss:0.29268\n",
      "[84]\ttrain-mlogloss:0.21990\teval-mlogloss:0.29246\n",
      "[85]\ttrain-mlogloss:0.21900\teval-mlogloss:0.29251\n",
      "[86]\ttrain-mlogloss:0.21816\teval-mlogloss:0.29211\n",
      "[87]\ttrain-mlogloss:0.21743\teval-mlogloss:0.29212\n",
      "[88]\ttrain-mlogloss:0.21663\teval-mlogloss:0.29212\n",
      "[89]\ttrain-mlogloss:0.21610\teval-mlogloss:0.29190\n",
      "[90]\ttrain-mlogloss:0.21518\teval-mlogloss:0.29165\n",
      "[91]\ttrain-mlogloss:0.21444\teval-mlogloss:0.29138\n",
      "[92]\ttrain-mlogloss:0.21369\teval-mlogloss:0.29139\n",
      "[93]\ttrain-mlogloss:0.21292\teval-mlogloss:0.29112\n",
      "[94]\ttrain-mlogloss:0.21225\teval-mlogloss:0.29108\n",
      "[95]\ttrain-mlogloss:0.21177\teval-mlogloss:0.29113\n",
      "[96]\ttrain-mlogloss:0.21107\teval-mlogloss:0.29108\n",
      "[97]\ttrain-mlogloss:0.21053\teval-mlogloss:0.29125\n",
      "[98]\ttrain-mlogloss:0.20984\teval-mlogloss:0.29112\n",
      "[99]\ttrain-mlogloss:0.20914\teval-mlogloss:0.29076\n",
      "[100]\ttrain-mlogloss:0.20858\teval-mlogloss:0.29063\n",
      "[101]\ttrain-mlogloss:0.20806\teval-mlogloss:0.29063\n",
      "[102]\ttrain-mlogloss:0.20726\teval-mlogloss:0.29032\n",
      "[103]\ttrain-mlogloss:0.20661\teval-mlogloss:0.29045\n",
      "[104]\ttrain-mlogloss:0.20594\teval-mlogloss:0.29048\n",
      "[105]\ttrain-mlogloss:0.20537\teval-mlogloss:0.29020\n",
      "[106]\ttrain-mlogloss:0.20479\teval-mlogloss:0.29024\n",
      "[107]\ttrain-mlogloss:0.20418\teval-mlogloss:0.28983\n",
      "[108]\ttrain-mlogloss:0.20384\teval-mlogloss:0.28983\n",
      "[109]\ttrain-mlogloss:0.20320\teval-mlogloss:0.28965\n",
      "[110]\ttrain-mlogloss:0.20267\teval-mlogloss:0.28977\n",
      "[111]\ttrain-mlogloss:0.20202\teval-mlogloss:0.28987\n",
      "[112]\ttrain-mlogloss:0.20148\teval-mlogloss:0.28992\n",
      "[113]\ttrain-mlogloss:0.20088\teval-mlogloss:0.28984\n",
      "[114]\ttrain-mlogloss:0.20043\teval-mlogloss:0.28994\n",
      "[115]\ttrain-mlogloss:0.19994\teval-mlogloss:0.28984\n",
      "[116]\ttrain-mlogloss:0.19939\teval-mlogloss:0.28958\n",
      "[117]\ttrain-mlogloss:0.19888\teval-mlogloss:0.28952\n",
      "[118]\ttrain-mlogloss:0.19833\teval-mlogloss:0.28943\n",
      "[119]\ttrain-mlogloss:0.19785\teval-mlogloss:0.28951\n",
      "[120]\ttrain-mlogloss:0.19748\teval-mlogloss:0.28954\n",
      "[121]\ttrain-mlogloss:0.19696\teval-mlogloss:0.28968\n",
      "[122]\ttrain-mlogloss:0.19648\teval-mlogloss:0.28977\n",
      "[123]\ttrain-mlogloss:0.19593\teval-mlogloss:0.28958\n",
      "[124]\ttrain-mlogloss:0.19551\teval-mlogloss:0.28954\n",
      "[125]\ttrain-mlogloss:0.19501\teval-mlogloss:0.28979\n",
      "[126]\ttrain-mlogloss:0.19454\teval-mlogloss:0.29003\n",
      "[127]\ttrain-mlogloss:0.19399\teval-mlogloss:0.28984\n",
      "[128]\ttrain-mlogloss:0.19351\teval-mlogloss:0.28999\n",
      "[129]\ttrain-mlogloss:0.19292\teval-mlogloss:0.29003\n",
      "[130]\ttrain-mlogloss:0.19225\teval-mlogloss:0.29018\n",
      "[131]\ttrain-mlogloss:0.19177\teval-mlogloss:0.29005\n",
      "[132]\ttrain-mlogloss:0.19126\teval-mlogloss:0.28974\n",
      "[133]\ttrain-mlogloss:0.19092\teval-mlogloss:0.28997\n",
      "[134]\ttrain-mlogloss:0.19041\teval-mlogloss:0.29018\n",
      "[135]\ttrain-mlogloss:0.19004\teval-mlogloss:0.29011\n",
      "[136]\ttrain-mlogloss:0.18966\teval-mlogloss:0.28992\n",
      "[137]\ttrain-mlogloss:0.18909\teval-mlogloss:0.29006\n",
      "[138]\ttrain-mlogloss:0.18866\teval-mlogloss:0.29005\n",
      "[139]\ttrain-mlogloss:0.18825\teval-mlogloss:0.29009\n",
      "[140]\ttrain-mlogloss:0.18786\teval-mlogloss:0.29013\n",
      "[141]\ttrain-mlogloss:0.18753\teval-mlogloss:0.29016\n",
      "[142]\ttrain-mlogloss:0.18708\teval-mlogloss:0.29036\n",
      "[143]\ttrain-mlogloss:0.18686\teval-mlogloss:0.29032\n",
      "[144]\ttrain-mlogloss:0.18645\teval-mlogloss:0.29029\n",
      "[145]\ttrain-mlogloss:0.18615\teval-mlogloss:0.29038\n",
      "[146]\ttrain-mlogloss:0.18566\teval-mlogloss:0.29045\n",
      "[147]\ttrain-mlogloss:0.18516\teval-mlogloss:0.29048\n",
      "[148]\ttrain-mlogloss:0.18481\teval-mlogloss:0.29043\n",
      "[149]\ttrain-mlogloss:0.18443\teval-mlogloss:0.29052\n",
      "[150]\ttrain-mlogloss:0.18409\teval-mlogloss:0.29048\n",
      "[151]\ttrain-mlogloss:0.18375\teval-mlogloss:0.29056\n",
      "[152]\ttrain-mlogloss:0.18316\teval-mlogloss:0.29038\n",
      "[153]\ttrain-mlogloss:0.18279\teval-mlogloss:0.29046\n",
      "[154]\ttrain-mlogloss:0.18250\teval-mlogloss:0.29035\n",
      "[155]\ttrain-mlogloss:0.18216\teval-mlogloss:0.29016\n",
      "[156]\ttrain-mlogloss:0.18175\teval-mlogloss:0.29002\n",
      "[157]\ttrain-mlogloss:0.18128\teval-mlogloss:0.28993\n",
      "[158]\ttrain-mlogloss:0.18096\teval-mlogloss:0.29008\n",
      "[159]\ttrain-mlogloss:0.18066\teval-mlogloss:0.29012\n",
      "[160]\ttrain-mlogloss:0.18036\teval-mlogloss:0.29012\n",
      "[161]\ttrain-mlogloss:0.17997\teval-mlogloss:0.29030\n",
      "[162]\ttrain-mlogloss:0.17965\teval-mlogloss:0.29035\n",
      "[163]\ttrain-mlogloss:0.17926\teval-mlogloss:0.29043\n",
      "[164]\ttrain-mlogloss:0.17902\teval-mlogloss:0.29069\n",
      "[165]\ttrain-mlogloss:0.17879\teval-mlogloss:0.29081\n",
      "[166]\ttrain-mlogloss:0.17841\teval-mlogloss:0.29104\n",
      "[167]\ttrain-mlogloss:0.17803\teval-mlogloss:0.29100\n",
      "[168]\ttrain-mlogloss:0.17776\teval-mlogloss:0.29110\n",
      "[169]\ttrain-mlogloss:0.17739\teval-mlogloss:0.29134\n",
      "[170]\ttrain-mlogloss:0.17703\teval-mlogloss:0.29112\n",
      "[171]\ttrain-mlogloss:0.17665\teval-mlogloss:0.29105\n",
      "[172]\ttrain-mlogloss:0.17637\teval-mlogloss:0.29117\n",
      "[173]\ttrain-mlogloss:0.17606\teval-mlogloss:0.29103\n",
      "[174]\ttrain-mlogloss:0.17565\teval-mlogloss:0.29102\n",
      "[175]\ttrain-mlogloss:0.17539\teval-mlogloss:0.29098\n",
      "[176]\ttrain-mlogloss:0.17504\teval-mlogloss:0.29097\n",
      "[177]\ttrain-mlogloss:0.17476\teval-mlogloss:0.29093\n",
      "[178]\ttrain-mlogloss:0.17460\teval-mlogloss:0.29092\n",
      "[179]\ttrain-mlogloss:0.17418\teval-mlogloss:0.29096\n",
      "[180]\ttrain-mlogloss:0.17393\teval-mlogloss:0.29088\n",
      "[181]\ttrain-mlogloss:0.17359\teval-mlogloss:0.29100\n",
      "[182]\ttrain-mlogloss:0.17329\teval-mlogloss:0.29102\n",
      "[183]\ttrain-mlogloss:0.17305\teval-mlogloss:0.29097\n",
      "[184]\ttrain-mlogloss:0.17266\teval-mlogloss:0.29087\n",
      "[185]\ttrain-mlogloss:0.17248\teval-mlogloss:0.29087\n",
      "[186]\ttrain-mlogloss:0.17212\teval-mlogloss:0.29071\n",
      "[187]\ttrain-mlogloss:0.17178\teval-mlogloss:0.29069\n",
      "[188]\ttrain-mlogloss:0.17157\teval-mlogloss:0.29078\n",
      "[189]\ttrain-mlogloss:0.17123\teval-mlogloss:0.29090\n",
      "[190]\ttrain-mlogloss:0.17080\teval-mlogloss:0.29088\n",
      "[191]\ttrain-mlogloss:0.17032\teval-mlogloss:0.29070\n",
      "[192]\ttrain-mlogloss:0.17015\teval-mlogloss:0.29067\n",
      "[193]\ttrain-mlogloss:0.16985\teval-mlogloss:0.29069\n",
      "[194]\ttrain-mlogloss:0.16966\teval-mlogloss:0.29077\n",
      "[195]\ttrain-mlogloss:0.16938\teval-mlogloss:0.29087\n",
      "[196]\ttrain-mlogloss:0.16913\teval-mlogloss:0.29094\n",
      "[197]\ttrain-mlogloss:0.16889\teval-mlogloss:0.29106\n",
      "[198]\ttrain-mlogloss:0.16859\teval-mlogloss:0.29115\n",
      "[199]\ttrain-mlogloss:0.16825\teval-mlogloss:0.29135\n",
      "[200]\ttrain-mlogloss:0.16788\teval-mlogloss:0.29144\n",
      "[201]\ttrain-mlogloss:0.16761\teval-mlogloss:0.29139\n",
      "[202]\ttrain-mlogloss:0.16743\teval-mlogloss:0.29124\n",
      "[203]\ttrain-mlogloss:0.16717\teval-mlogloss:0.29132\n",
      "[204]\ttrain-mlogloss:0.16687\teval-mlogloss:0.29158\n",
      "[205]\ttrain-mlogloss:0.16661\teval-mlogloss:0.29176\n",
      "[206]\ttrain-mlogloss:0.16638\teval-mlogloss:0.29169\n",
      "[207]\ttrain-mlogloss:0.16616\teval-mlogloss:0.29171\n",
      "[208]\ttrain-mlogloss:0.16596\teval-mlogloss:0.29165\n",
      "[209]\ttrain-mlogloss:0.16570\teval-mlogloss:0.29161\n",
      "[210]\ttrain-mlogloss:0.16533\teval-mlogloss:0.29138\n",
      "[211]\ttrain-mlogloss:0.16512\teval-mlogloss:0.29138\n",
      "[212]\ttrain-mlogloss:0.16492\teval-mlogloss:0.29146\n",
      "[213]\ttrain-mlogloss:0.16458\teval-mlogloss:0.29143\n",
      "[214]\ttrain-mlogloss:0.16438\teval-mlogloss:0.29148\n",
      "[215]\ttrain-mlogloss:0.16423\teval-mlogloss:0.29131\n",
      "[216]\ttrain-mlogloss:0.16394\teval-mlogloss:0.29120\n",
      "[217]\ttrain-mlogloss:0.16364\teval-mlogloss:0.29119\n",
      "[218]\ttrain-mlogloss:0.16330\teval-mlogloss:0.29120\n",
      "xgb now score is: [2.388448718059808, 2.5234747524932026, 2.529490461815149, 2.4168856119271367]\n",
      "[0]\ttrain-mlogloss:0.67138\teval-mlogloss:0.67048\n",
      "[1]\ttrain-mlogloss:0.65113\teval-mlogloss:0.64952\n",
      "[2]\ttrain-mlogloss:0.63197\teval-mlogloss:0.63000\n",
      "[3]\ttrain-mlogloss:0.61384\teval-mlogloss:0.61151\n",
      "[4]\ttrain-mlogloss:0.59666\teval-mlogloss:0.59404\n",
      "[5]\ttrain-mlogloss:0.58066\teval-mlogloss:0.57744\n",
      "[6]\ttrain-mlogloss:0.56513\teval-mlogloss:0.56147\n",
      "[7]\ttrain-mlogloss:0.55069\teval-mlogloss:0.54648\n",
      "[8]\ttrain-mlogloss:0.53689\teval-mlogloss:0.53223\n",
      "[9]\ttrain-mlogloss:0.52363\teval-mlogloss:0.51864\n",
      "[10]\ttrain-mlogloss:0.51095\teval-mlogloss:0.50593\n",
      "[11]\ttrain-mlogloss:0.49910\teval-mlogloss:0.49384\n",
      "[12]\ttrain-mlogloss:0.48781\teval-mlogloss:0.48212\n",
      "[13]\ttrain-mlogloss:0.47687\teval-mlogloss:0.47096\n",
      "[14]\ttrain-mlogloss:0.46641\teval-mlogloss:0.46017\n",
      "[15]\ttrain-mlogloss:0.45662\teval-mlogloss:0.45023\n",
      "[16]\ttrain-mlogloss:0.44688\teval-mlogloss:0.44018\n",
      "[17]\ttrain-mlogloss:0.43796\teval-mlogloss:0.43090\n",
      "[18]\ttrain-mlogloss:0.42931\teval-mlogloss:0.42180\n",
      "[19]\ttrain-mlogloss:0.42110\teval-mlogloss:0.41336\n",
      "[20]\ttrain-mlogloss:0.41331\teval-mlogloss:0.40523\n",
      "[21]\ttrain-mlogloss:0.40578\teval-mlogloss:0.39737\n",
      "[22]\ttrain-mlogloss:0.39840\teval-mlogloss:0.38964\n",
      "[23]\ttrain-mlogloss:0.39138\teval-mlogloss:0.38235\n",
      "[24]\ttrain-mlogloss:0.38496\teval-mlogloss:0.37599\n",
      "[25]\ttrain-mlogloss:0.37871\teval-mlogloss:0.36965\n",
      "[26]\ttrain-mlogloss:0.37274\teval-mlogloss:0.36356\n",
      "[27]\ttrain-mlogloss:0.36700\teval-mlogloss:0.35770\n",
      "[28]\ttrain-mlogloss:0.36142\teval-mlogloss:0.35174\n",
      "[29]\ttrain-mlogloss:0.35606\teval-mlogloss:0.34630\n",
      "[30]\ttrain-mlogloss:0.35079\teval-mlogloss:0.34105\n",
      "[31]\ttrain-mlogloss:0.34590\teval-mlogloss:0.33620\n",
      "[32]\ttrain-mlogloss:0.34128\teval-mlogloss:0.33141\n",
      "[33]\ttrain-mlogloss:0.33680\teval-mlogloss:0.32680\n",
      "[34]\ttrain-mlogloss:0.33241\teval-mlogloss:0.32219\n",
      "[35]\ttrain-mlogloss:0.32825\teval-mlogloss:0.31800\n",
      "[36]\ttrain-mlogloss:0.32437\teval-mlogloss:0.31407\n",
      "[37]\ttrain-mlogloss:0.32052\teval-mlogloss:0.31008\n",
      "[38]\ttrain-mlogloss:0.31666\teval-mlogloss:0.30629\n",
      "[39]\ttrain-mlogloss:0.31302\teval-mlogloss:0.30266\n",
      "[40]\ttrain-mlogloss:0.30963\teval-mlogloss:0.29904\n",
      "[41]\ttrain-mlogloss:0.30628\teval-mlogloss:0.29598\n",
      "[42]\ttrain-mlogloss:0.30307\teval-mlogloss:0.29289\n",
      "[43]\ttrain-mlogloss:0.29983\teval-mlogloss:0.28995\n",
      "[44]\ttrain-mlogloss:0.29687\teval-mlogloss:0.28689\n",
      "[45]\ttrain-mlogloss:0.29399\teval-mlogloss:0.28407\n",
      "[46]\ttrain-mlogloss:0.29114\teval-mlogloss:0.28114\n",
      "[47]\ttrain-mlogloss:0.28834\teval-mlogloss:0.27866\n",
      "[48]\ttrain-mlogloss:0.28602\teval-mlogloss:0.27634\n",
      "[49]\ttrain-mlogloss:0.28339\teval-mlogloss:0.27376\n",
      "[50]\ttrain-mlogloss:0.28114\teval-mlogloss:0.27145\n",
      "[51]\ttrain-mlogloss:0.27866\teval-mlogloss:0.26934\n",
      "[52]\ttrain-mlogloss:0.27646\teval-mlogloss:0.26717\n",
      "[53]\ttrain-mlogloss:0.27421\teval-mlogloss:0.26508\n",
      "[54]\ttrain-mlogloss:0.27208\teval-mlogloss:0.26309\n",
      "[55]\ttrain-mlogloss:0.26996\teval-mlogloss:0.26119\n",
      "[56]\ttrain-mlogloss:0.26793\teval-mlogloss:0.25934\n",
      "[57]\ttrain-mlogloss:0.26586\teval-mlogloss:0.25755\n",
      "[58]\ttrain-mlogloss:0.26402\teval-mlogloss:0.25577\n",
      "[59]\ttrain-mlogloss:0.26224\teval-mlogloss:0.25428\n",
      "[60]\ttrain-mlogloss:0.26046\teval-mlogloss:0.25292\n",
      "[61]\ttrain-mlogloss:0.25892\teval-mlogloss:0.25152\n",
      "[62]\ttrain-mlogloss:0.25733\teval-mlogloss:0.25002\n",
      "[63]\ttrain-mlogloss:0.25566\teval-mlogloss:0.24857\n",
      "[64]\ttrain-mlogloss:0.25407\teval-mlogloss:0.24732\n",
      "[65]\ttrain-mlogloss:0.25261\teval-mlogloss:0.24591\n",
      "[66]\ttrain-mlogloss:0.25128\teval-mlogloss:0.24481\n",
      "[67]\ttrain-mlogloss:0.24981\teval-mlogloss:0.24369\n",
      "[68]\ttrain-mlogloss:0.24861\teval-mlogloss:0.24259\n",
      "[69]\ttrain-mlogloss:0.24726\teval-mlogloss:0.24136\n",
      "[70]\ttrain-mlogloss:0.24594\teval-mlogloss:0.24011\n",
      "[71]\ttrain-mlogloss:0.24478\teval-mlogloss:0.23912\n",
      "[72]\ttrain-mlogloss:0.24353\teval-mlogloss:0.23813\n",
      "[73]\ttrain-mlogloss:0.24248\teval-mlogloss:0.23723\n",
      "[74]\ttrain-mlogloss:0.24118\teval-mlogloss:0.23639\n",
      "[75]\ttrain-mlogloss:0.24002\teval-mlogloss:0.23560\n",
      "[76]\ttrain-mlogloss:0.23897\teval-mlogloss:0.23488\n",
      "[77]\ttrain-mlogloss:0.23785\teval-mlogloss:0.23421\n",
      "[78]\ttrain-mlogloss:0.23666\teval-mlogloss:0.23349\n",
      "[79]\ttrain-mlogloss:0.23582\teval-mlogloss:0.23283\n",
      "[80]\ttrain-mlogloss:0.23481\teval-mlogloss:0.23245\n",
      "[81]\ttrain-mlogloss:0.23398\teval-mlogloss:0.23187\n",
      "[82]\ttrain-mlogloss:0.23306\teval-mlogloss:0.23136\n",
      "[83]\ttrain-mlogloss:0.23227\teval-mlogloss:0.23071\n",
      "[84]\ttrain-mlogloss:0.23160\teval-mlogloss:0.23013\n",
      "[85]\ttrain-mlogloss:0.23088\teval-mlogloss:0.22952\n",
      "[86]\ttrain-mlogloss:0.23003\teval-mlogloss:0.22905\n",
      "[87]\ttrain-mlogloss:0.22918\teval-mlogloss:0.22864\n",
      "[88]\ttrain-mlogloss:0.22838\teval-mlogloss:0.22810\n",
      "[89]\ttrain-mlogloss:0.22764\teval-mlogloss:0.22772\n",
      "[90]\ttrain-mlogloss:0.22686\teval-mlogloss:0.22726\n",
      "[91]\ttrain-mlogloss:0.22620\teval-mlogloss:0.22690\n",
      "[92]\ttrain-mlogloss:0.22522\teval-mlogloss:0.22656\n",
      "[93]\ttrain-mlogloss:0.22444\teval-mlogloss:0.22620\n",
      "[94]\ttrain-mlogloss:0.22373\teval-mlogloss:0.22579\n",
      "[95]\ttrain-mlogloss:0.22307\teval-mlogloss:0.22548\n",
      "[96]\ttrain-mlogloss:0.22234\teval-mlogloss:0.22513\n",
      "[97]\ttrain-mlogloss:0.22150\teval-mlogloss:0.22489\n",
      "[98]\ttrain-mlogloss:0.22079\teval-mlogloss:0.22449\n",
      "[99]\ttrain-mlogloss:0.22011\teval-mlogloss:0.22420\n",
      "[100]\ttrain-mlogloss:0.21945\teval-mlogloss:0.22391\n",
      "[101]\ttrain-mlogloss:0.21885\teval-mlogloss:0.22374\n",
      "[102]\ttrain-mlogloss:0.21824\teval-mlogloss:0.22350\n",
      "[103]\ttrain-mlogloss:0.21744\teval-mlogloss:0.22335\n",
      "[104]\ttrain-mlogloss:0.21663\teval-mlogloss:0.22302\n",
      "[105]\ttrain-mlogloss:0.21604\teval-mlogloss:0.22284\n",
      "[106]\ttrain-mlogloss:0.21539\teval-mlogloss:0.22262\n",
      "[107]\ttrain-mlogloss:0.21463\teval-mlogloss:0.22250\n",
      "[108]\ttrain-mlogloss:0.21407\teval-mlogloss:0.22231\n",
      "[109]\ttrain-mlogloss:0.21339\teval-mlogloss:0.22220\n",
      "[110]\ttrain-mlogloss:0.21289\teval-mlogloss:0.22202\n",
      "[111]\ttrain-mlogloss:0.21212\teval-mlogloss:0.22189\n",
      "[112]\ttrain-mlogloss:0.21165\teval-mlogloss:0.22166\n",
      "[113]\ttrain-mlogloss:0.21119\teval-mlogloss:0.22158\n",
      "[114]\ttrain-mlogloss:0.21057\teval-mlogloss:0.22141\n",
      "[115]\ttrain-mlogloss:0.20985\teval-mlogloss:0.22123\n",
      "[116]\ttrain-mlogloss:0.20926\teval-mlogloss:0.22115\n",
      "[117]\ttrain-mlogloss:0.20873\teval-mlogloss:0.22099\n",
      "[118]\ttrain-mlogloss:0.20819\teval-mlogloss:0.22088\n",
      "[119]\ttrain-mlogloss:0.20779\teval-mlogloss:0.22069\n",
      "[120]\ttrain-mlogloss:0.20723\teval-mlogloss:0.22074\n",
      "[121]\ttrain-mlogloss:0.20670\teval-mlogloss:0.22068\n",
      "[122]\ttrain-mlogloss:0.20637\teval-mlogloss:0.22049\n",
      "[123]\ttrain-mlogloss:0.20584\teval-mlogloss:0.22055\n",
      "[124]\ttrain-mlogloss:0.20541\teval-mlogloss:0.22054\n",
      "[125]\ttrain-mlogloss:0.20488\teval-mlogloss:0.22059\n",
      "[126]\ttrain-mlogloss:0.20440\teval-mlogloss:0.22055\n",
      "[127]\ttrain-mlogloss:0.20390\teval-mlogloss:0.22065\n",
      "[128]\ttrain-mlogloss:0.20343\teval-mlogloss:0.22060\n",
      "[129]\ttrain-mlogloss:0.20281\teval-mlogloss:0.22064\n",
      "[130]\ttrain-mlogloss:0.20242\teval-mlogloss:0.22078\n",
      "[131]\ttrain-mlogloss:0.20192\teval-mlogloss:0.22065\n",
      "[132]\ttrain-mlogloss:0.20144\teval-mlogloss:0.22061\n",
      "[133]\ttrain-mlogloss:0.20100\teval-mlogloss:0.22057\n",
      "[134]\ttrain-mlogloss:0.20058\teval-mlogloss:0.22062\n",
      "[135]\ttrain-mlogloss:0.20011\teval-mlogloss:0.22066\n",
      "[136]\ttrain-mlogloss:0.19957\teval-mlogloss:0.22062\n",
      "[137]\ttrain-mlogloss:0.19907\teval-mlogloss:0.22052\n",
      "[138]\ttrain-mlogloss:0.19857\teval-mlogloss:0.22054\n",
      "[139]\ttrain-mlogloss:0.19822\teval-mlogloss:0.22051\n",
      "[140]\ttrain-mlogloss:0.19766\teval-mlogloss:0.22049\n",
      "[141]\ttrain-mlogloss:0.19710\teval-mlogloss:0.22056\n",
      "[142]\ttrain-mlogloss:0.19669\teval-mlogloss:0.22061\n",
      "[143]\ttrain-mlogloss:0.19631\teval-mlogloss:0.22059\n",
      "[144]\ttrain-mlogloss:0.19590\teval-mlogloss:0.22067\n",
      "[145]\ttrain-mlogloss:0.19540\teval-mlogloss:0.22078\n",
      "[146]\ttrain-mlogloss:0.19501\teval-mlogloss:0.22070\n",
      "[147]\ttrain-mlogloss:0.19455\teval-mlogloss:0.22080\n",
      "[148]\ttrain-mlogloss:0.19417\teval-mlogloss:0.22096\n",
      "[149]\ttrain-mlogloss:0.19377\teval-mlogloss:0.22084\n",
      "[150]\ttrain-mlogloss:0.19333\teval-mlogloss:0.22092\n",
      "[151]\ttrain-mlogloss:0.19284\teval-mlogloss:0.22098\n",
      "[152]\ttrain-mlogloss:0.19242\teval-mlogloss:0.22114\n",
      "[153]\ttrain-mlogloss:0.19202\teval-mlogloss:0.22113\n",
      "[154]\ttrain-mlogloss:0.19159\teval-mlogloss:0.22110\n",
      "[155]\ttrain-mlogloss:0.19116\teval-mlogloss:0.22092\n",
      "[156]\ttrain-mlogloss:0.19072\teval-mlogloss:0.22094\n",
      "[157]\ttrain-mlogloss:0.19039\teval-mlogloss:0.22105\n",
      "[158]\ttrain-mlogloss:0.18994\teval-mlogloss:0.22111\n",
      "[159]\ttrain-mlogloss:0.18957\teval-mlogloss:0.22096\n",
      "[160]\ttrain-mlogloss:0.18915\teval-mlogloss:0.22093\n",
      "[161]\ttrain-mlogloss:0.18875\teval-mlogloss:0.22104\n",
      "[162]\ttrain-mlogloss:0.18853\teval-mlogloss:0.22122\n",
      "[163]\ttrain-mlogloss:0.18820\teval-mlogloss:0.22100\n",
      "[164]\ttrain-mlogloss:0.18791\teval-mlogloss:0.22117\n",
      "[165]\ttrain-mlogloss:0.18754\teval-mlogloss:0.22115\n",
      "[166]\ttrain-mlogloss:0.18705\teval-mlogloss:0.22126\n",
      "[167]\ttrain-mlogloss:0.18667\teval-mlogloss:0.22133\n",
      "[168]\ttrain-mlogloss:0.18641\teval-mlogloss:0.22147\n",
      "[169]\ttrain-mlogloss:0.18597\teval-mlogloss:0.22155\n",
      "[170]\ttrain-mlogloss:0.18565\teval-mlogloss:0.22151\n",
      "[171]\ttrain-mlogloss:0.18529\teval-mlogloss:0.22155\n",
      "[172]\ttrain-mlogloss:0.18496\teval-mlogloss:0.22165\n",
      "[173]\ttrain-mlogloss:0.18459\teval-mlogloss:0.22140\n",
      "[174]\ttrain-mlogloss:0.18419\teval-mlogloss:0.22153\n",
      "[175]\ttrain-mlogloss:0.18398\teval-mlogloss:0.22167\n",
      "[176]\ttrain-mlogloss:0.18361\teval-mlogloss:0.22164\n",
      "[177]\ttrain-mlogloss:0.18328\teval-mlogloss:0.22185\n",
      "[178]\ttrain-mlogloss:0.18292\teval-mlogloss:0.22197\n",
      "[179]\ttrain-mlogloss:0.18263\teval-mlogloss:0.22206\n",
      "[180]\ttrain-mlogloss:0.18237\teval-mlogloss:0.22213\n",
      "[181]\ttrain-mlogloss:0.18208\teval-mlogloss:0.22207\n",
      "[182]\ttrain-mlogloss:0.18172\teval-mlogloss:0.22205\n",
      "[183]\ttrain-mlogloss:0.18147\teval-mlogloss:0.22198\n",
      "[184]\ttrain-mlogloss:0.18115\teval-mlogloss:0.22215\n",
      "[185]\ttrain-mlogloss:0.18096\teval-mlogloss:0.22208\n",
      "[186]\ttrain-mlogloss:0.18065\teval-mlogloss:0.22198\n",
      "[187]\ttrain-mlogloss:0.18031\teval-mlogloss:0.22216\n",
      "[188]\ttrain-mlogloss:0.18001\teval-mlogloss:0.22239\n",
      "[189]\ttrain-mlogloss:0.17977\teval-mlogloss:0.22246\n",
      "[190]\ttrain-mlogloss:0.17941\teval-mlogloss:0.22256\n",
      "[191]\ttrain-mlogloss:0.17900\teval-mlogloss:0.22260\n",
      "[192]\ttrain-mlogloss:0.17868\teval-mlogloss:0.22250\n",
      "[193]\ttrain-mlogloss:0.17844\teval-mlogloss:0.22258\n",
      "[194]\ttrain-mlogloss:0.17818\teval-mlogloss:0.22265\n",
      "[195]\ttrain-mlogloss:0.17799\teval-mlogloss:0.22265\n",
      "[196]\ttrain-mlogloss:0.17766\teval-mlogloss:0.22239\n",
      "[197]\ttrain-mlogloss:0.17729\teval-mlogloss:0.22221\n",
      "[198]\ttrain-mlogloss:0.17706\teval-mlogloss:0.22206\n",
      "[199]\ttrain-mlogloss:0.17690\teval-mlogloss:0.22215\n",
      "[200]\ttrain-mlogloss:0.17656\teval-mlogloss:0.22218\n",
      "[201]\ttrain-mlogloss:0.17625\teval-mlogloss:0.22233\n",
      "[202]\ttrain-mlogloss:0.17608\teval-mlogloss:0.22236\n",
      "[203]\ttrain-mlogloss:0.17582\teval-mlogloss:0.22234\n",
      "[204]\ttrain-mlogloss:0.17555\teval-mlogloss:0.22229\n",
      "[205]\ttrain-mlogloss:0.17535\teval-mlogloss:0.22245\n",
      "[206]\ttrain-mlogloss:0.17501\teval-mlogloss:0.22236\n",
      "[207]\ttrain-mlogloss:0.17477\teval-mlogloss:0.22258\n",
      "[208]\ttrain-mlogloss:0.17455\teval-mlogloss:0.22268\n",
      "[209]\ttrain-mlogloss:0.17432\teval-mlogloss:0.22271\n",
      "[210]\ttrain-mlogloss:0.17404\teval-mlogloss:0.22270\n",
      "[211]\ttrain-mlogloss:0.17371\teval-mlogloss:0.22293\n",
      "[212]\ttrain-mlogloss:0.17341\teval-mlogloss:0.22295\n",
      "[213]\ttrain-mlogloss:0.17317\teval-mlogloss:0.22300\n",
      "[214]\ttrain-mlogloss:0.17280\teval-mlogloss:0.22316\n",
      "[215]\ttrain-mlogloss:0.17255\teval-mlogloss:0.22315\n",
      "[216]\ttrain-mlogloss:0.17224\teval-mlogloss:0.22316\n",
      "[217]\ttrain-mlogloss:0.17203\teval-mlogloss:0.22324\n",
      "[218]\ttrain-mlogloss:0.17177\teval-mlogloss:0.22319\n",
      "[219]\ttrain-mlogloss:0.17153\teval-mlogloss:0.22331\n",
      "[220]\ttrain-mlogloss:0.17127\teval-mlogloss:0.22341\n",
      "[221]\ttrain-mlogloss:0.17109\teval-mlogloss:0.22341\n",
      "[222]\ttrain-mlogloss:0.17094\teval-mlogloss:0.22341\n",
      "xgb now score is: [2.388448718059808, 2.5234747524932026, 2.529490461815149, 2.4168856119271367, 2.502331032752991]\n",
      "xgb_score_list: [2.388448718059808, 2.5234747524932026, 2.529490461815149, 2.4168856119271367, 2.502331032752991]\n",
      "xgb_score_mean: 2.472126115409657\n"
     ]
    }
   ],
   "source": [
    "clf_list = clf_list\n",
    "column_list = []\n",
    "train_data_list=[]\n",
    "test_data_list=[]\n",
    "for clf in clf_list:\n",
    "    train_data,test_data,clf_name=clf(x_train, y_train, x_valid, kf, label_split=None)\n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data)\n",
    "train_stacking = np.concatenate(train_data_list, axis=1)\n",
    "test_stacking = np.concatenate(test_data_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原始特征和stacking特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 合并所有特征\n",
    "train = pd.DataFrame(np.concatenate([x_train, train_stacking], axis=1))\n",
    "test = np.concatenate([x_valid, test_stacking], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征重命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all = pd.DataFrame(train)\n",
    "df_train_all.columns = features_columns + clf_list_col\n",
    "df_test_all = pd.DataFrame(test)\n",
    "df_test_all.columns = features_columns + clf_list_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取数据ID以及特征标签LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all['label'] = all_data_test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练数据和测试数据保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all.to_csv('./data/train_all.csv',header=True,index=False)\n",
    "df_test_all.to_csv('./data/test_all.csv',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
